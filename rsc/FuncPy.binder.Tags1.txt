vim:shiftwidth=2:syntax=python:
set shiftwidth=2 | set syntax=python | colors molokai_dark
[TODO: CLEAN UP ALL CODE
  = IN NEWLY INCLUDED FILES[!!]]

___zzzz
 ___zzzz2

__abcode_pyfiltering
__JupyterCb
----
__funcpy
__pyfunct2
__pyfunc3
  ----
___MPyCb=Funct
___PyStdLib3_ByEx=Funct
  ___PyCb3=Funct  =pp 4, 17, 28, [92]
___FluPy=Funct
----
  __pyrecursion

    ????=PyStdLibRef


__funcpy

(Avoiding) Flow Control ----
Encapsulation
Comprehensions
Recursion
Eliminating Loops

Callables ----
Named Functions and Lambdas
Closures and Callable Instances
Methods of Classes
Multiple Dispatch

Lazy Evaluation ----
The Iterator Protocol
Module: itertools

Higher-Order Functions ----
Utility Higher-Order Functions
The operator Module
The functools Module
Decorators


    evince -p v ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 7]

Preface

What Is Functional Programming?


We'd better start with the hardest
question: "What is functional
programming (FP), anyway?" '

One answer would be to say that
functional programming is what you
do when you program in languages
like Lisp, Scheme, Clojure, Scala,
Haskell, ML, OCAML, Erlang, or a
few others. n@That is a safe answer,
but not one that clarifies very
much. Unfortunately, it is hard to
get a consistent opinion on just
what functional programming is,
even from functional programmers
themselves. A story about elephants
and blind men seems apropos here.
It is also safe to contrast
functional programming with
"imperative programming" (what you
do in languages like C, Pascal,
C++, Java, Perl, Awk, TCL, and most
others, at least for the most
part). Functional programming is
also not object-oriented
programming (OOP), although some
languages are both. And it is not
Logic Programming (e.g., Prolog),
but again some languages are
multiparadigm.

n@N@Personally, I would roughly
characterize functional programming
as having at least several of the
following characteristics.
Languages that get called
functional make these things easy,
and make other things either hard
or impossible:

- Functions are first class
  (objects). That is, everything
  you can do with "data" can be
  done with functions themselves
  (such as passing a function to
  another function).
- Recursion is used as a primary
  control structure. In some
  languages, no other "loop"
  construct exists.

    evince -p vi ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 8]

- There is a focus on list
  processing (for example, it is
  the source of the name Lisp).
  Lists are often used with
  recursion on sublists as a
  substitute for loops.
- n@"Pure" functional languages
  eschew side effects. This
  excludes the almost ubiquitous
  pattern in imperative languages
  of assigning first one, then
  another value to the same
  variable to track the program
  state.
- Functional programming either
  discourages or outright disallows
  statements, and instead works
  with n@N@the evaluation of
  expressions (in other words,
  functions plus arguments). N@N@In the
  pure case, one program is one
  expression (plus supporting
  definitions).
- n@N@Functional programming worries
  about what is to be computed
  rather than how it is to be
  computed.
- Much functional programming
  utilizes "higher order" functions
  (in other words, functions that
  operate on functions that operate
  on functions).

Advocates of functional programming
argue that all these
characteristics make for more
rapidly developed, shorter, and
less bug-prone code. Moreover, high
theorists of computer science,
logic, and math find it a lot
easier to prove formal properties
of functional languages and
programs than of imperative
languages and programs. One crucial
concept in functional programming
is that of a "pure function" - one
that always returns the same result
given the same arguments - which is
more closely akin to the meaning of
"function" in mathematics than that
in imperative programming.

Python is most definitely not a
"pure functional programming
language"; side effects are
widespread in most Python programs.
That is, variables are frequently
rebound, mutable data collections
often change contents, and I/O is
freely interleaved with
computation. It is also not even a
"functional programming language"
more generally. However, Python is
a multiparadigm language that makes
functional programming easy to do
when desired, and easy to mix with
other programming styles.


Beyond the Standard Library

T@While they will not be discussed
withing the limited space of this
report, a large number of useful
third-party Python libraries for
functional programming are
available.

    evince -p vii ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 9]

The one exception here is that I
will discuss Matthew Rocklin's
N@multipledispatch as the best
current implementation of the
concept it implements. '

N@Most third-party libraries around
functional programming are
collections of higher-order
functions, and N@sometimes
enhancements to the tools for
working lazily with iterators
contained in itertools. Some
notable examples include the
following, but this list should not
be taken as exhaustive:

- n@pyrsistent contains a number of
  immutable collections. All
  methods on a data structure that
  would normally mutate it instead
  return a new copy of the
  structure containing the
  requested updates. The original
  structure is left untouched.
- toolz provides a set of utility
  functions for iterators,
  functions, and dictionaries.
  N@These functions interoperate well
  and form the building blocks of
  common data analytic operations.
  They extend the standard
  libraries itertools and functools
  and borrow heavily from the
  standard libraries of
  contemporary functional
  languages.
- n@N@N@hypothesis is a library for
  creating unit tests for finding
  edge cases in your code you
  wouldn't have thought to look
  for. It works by generating
  random data matching your
  specification and checking that
  your guarantee still holds in
  that case. This is often called
  property-based testing, and was
  popularized by the Haskell
  library QuickCheck. '
- N@more_itertools tries to collect
  useful compositions of iterators
  that neither itertools nor the
  recipes included in its docs
  address. These compositions are
  deceptively tricky to get right
  and this well-crafted library
  helps users avoid pitfalls of
  rolling them themselves.


Resources

There are a large number of other
papers, articles, and books written
about functional programming, in
Python and otherwise. The Python
standard documentation itself
contains an excellent introduction
called N@N@"Functional Programming
HOWTO," by Andrew Kuchling, that
discusses some of the motivation
for functional programming styles,
as well as particular capabilities
in Python.

    evince -p viii ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 10]

Mentioned in Kuchling's
introduction are several very old
public domain articles this author
wrote in the 2000s, on which
portions of this report are based.
These include:

- N@The first chapter of my book Text
  Processing in Python, which
  discusses functional programming
  for text processing, in the
  section titled "Utilizing
  Higher-Order Functions in Text
  Processing."

I also wrote several articles,
mentioned by Kuchling, for IBM's
n@developerWorks site that discussed
using functional programming in an
early version of Python 2.x:

- N@Charming Python: Functional
  programming in Python, Part 1:
  Making more out of your favorite
  scripting language
- Charming Python: Functional
  programming in Python, Part 2:
  Wading into functional
  programming?
- Charming Python: Functional
  programming in Python, Part 3:
  Currying and other higher-order
  functions

Not mentioned by Kuchling, and also
for an older version of Python, I
discussed multiple dispatch in
another article for the same
column. The implementation I
created there has no advantages
over the more recent
N@multipledispatch library, but it
provides a longer conceptual
explanation than this report can:

- Charming Python: Multiple
  dispatch: Generalizing
  polymorphism with n@multimethods


A Stylistic Note

As in most programming texts, a
fixed font will be used both for
inline and block samples of code,
including simple command or
function names. Within code blocks,
a notional segment of pseudocode is
indicated with a word surrounded by
angle brackets (i.e., not valid
Python), such as <code-block>. In
other cases, syntactically valid
but undefined functions are used
with descriptive names, such as
get_the_data().


___start2
    evince -p 1 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 11]

(Avoiding) Flow Control


In typical imperative Python
programs - including those that
make use of classes and methods to
hold their imperative code - a
block of code generally consists of
some outside loops (for or while),
assignment of state variables
within those loops, modification of
data structures like dicts, lists,
and sets (or various other
structures, either from the
standard library or from
third-party packages), and some
branch statements (if/elif/else or
try/except/finally). All of this is
both natural and seems at first
easy to reason about. The problems
often arise, however, precisely
with those side effects that come
with state variables and mutable
data structures; they often model
our concepts from the physical
world of containers fairly well,
but it is also difficult to reason
accurately about what state data is
in at a given point in a program.

One solution is to focus not on
constructing a data collection N@but
rather on describing "what" that
data collection consists of.
N@N@When one simply thinks, "Here's
some data, what do I need to do with
it?" rather than the mechanism of
constructing the data, more direct
reasoning is often possible. N@The
imperative flow-control described in
the last paragraph is much more
about the "how" than the "what" and
we can often shift the question.


Encapsulation

N@One obvious way of focusing more on
"what" than "how" is simply to
refactor code, and to put the data
construction in a more isolated
place - i.e., in a function or
method. For example, consider an
existing snippet of imperative code
that looks like this:

    evince -p 2 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 12]

# configure the data to start with
collection = get_initial_state()
N@state_var = None
for datum in data_set:
  if condition(state_var):
    state_var = calculate_from(datum)
    new = modify(datum, state_var)
    collection.add_to(new)
  else:
    new = modify_differently(datum)
    collection.add_to(new)

# Now actually work with the data
for thing in collection:
  process(thing)

We might simply remove the "how" of
the data construction from the
current scope, and tuck it away in
a function that we can think about
in isolation (or not think about at
all once it is sufficiently
abstracted). For example:

# tuck away construction of data
def make_collection(data_set):
  collection = get_initial_state()
  state_var = None
  for datum in data_set:
    if condition(state_var):
      state_var = calculate_from(datum, state_var)
      new = modify(datum, state_var)
      collection.add_to(new)
    else:
      new = modify_differently(datum)
      collection.add_to(new)
  return collection

# Now actually work with the data
for thing in make_collection(data_set):
  process(thing)

N@N@We haven't changed the
programming logic, nor even the
lines of code, at all, but we have
still shifted the focus from "How do
we construct collection?" to "What
does make_collection() create?" '


Comprehensions

N@Using comprehensions is often a
way both to make code more compact
and to shift our focus from the
"how" to the "what." A comprehension
is an expression that uses the same
keywords as loop and conditional
blocks, but inverts their order to
focus on the data rather than on the
procedure.

    evince -p 3 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 13]

Simply changing the form of
expression can often make a
surprisingly large difference in
how we reason about code and how
easy it is to understand. T@The
ternary operator also performs a
similar restructuring of our focus,
using the same keywords in a
different order. For example, if
our original code was:

collection = list()
for datum in data_set:
  if condition(datum):
    collection.append(datum)
  else:
    new = modify(datum)
    collection.append(new)

Somewhat more compactly we could
write this as:

collection = [d if condition(d) else modify(d)
             for d in data_set]

Far more important than simply
saving a few characters and lines is
the mental shift enacted by thinking
of what collection is, and by
avoiding needing to think about or
debug "What is the state of
collection at this point in the
loop?"

n@List comprehensions have been in
Python the longest, and are in some
ways the simplest. N@We now also
have generator comprehensions, set
comprehensions, and dict
comprehensions available in Python
syntax. As a caveat though, while
you can nest comprehensions to
arbitrary depth, past a fairly
simple level they tend to stop
clarifying and start obscuring.
N@For genuinely complex construction
of a data collection, refactoring
into functions remains more
readable.


Generators

Generator comprehensions have the
same syntax as list comprehensions
- other than that there are no
square brackets around them (but
parentheses are needed syntactically
in some contexts, in place of
brackets) - N@but they are also
lazy. T@T@That is to say that they
are merely a description of "how to
get the data" that is not realized
until one explicitly asks for it,
either by calling .next() on the
object, or by looping over it.
N@This often saves memory for large
sequences and defers computation
until it is actually needed. For
example:

log_lines = (line for line in read_line(huge_log_file)
                  if complex_condition(line))

    evince -p 4 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 14]

For typical uses, the behavior is
the same as if you had constructed
a list, but runtime behavior is
nicer. Obviously, this N@generator
comprehension also has imperative
versions, for example:

N@N@def get_log_lines(log_file):
  line = read_line(log_file)
  while True:
    try:
      if complex_condition(line):
        T@yield line
      line = read_line(log_file)
    except StopIteration:
      raise

log_lines = get_log_lines(huge_log_file)

Yes, the imperative version could be
simplified too, but the version
shown is meant to illustrate the
behind-the-scenes "how" of a for
loop over an iteratable - more
details we also want to abstract
from in our thinking. N@In fact,
even using yield is somewhat of an
abstraction from the underlying
"iterator protocol." We could do
this with a class that had
.__next__() and .__iter__() methods.
For example:

N@class GetLogLines(object):
  def __init__(self, log_file):
    self.log_file = log_file
    self.line = None
  def __iter__(self):
    return self
  def __next__(self):
    if self.line is None:
      self.line = read_line(log_file)
    while not complex_condition(self.line):
      self.line = read_line(self.log_file)
    return self.line

log_lines = GetLogLines(huge_log_file)

Aside from the digression into the
iterator protocol and laziness more
generally, the reader should see
that the comprehension focuses
attention much better on the
"what," whereas the imperative
version - although successful as
refactorings perhaps - retains the
focus on the "how."


Dicts and Sets

In the same fashion that lists can
be created in comprehensions rather
than by creating an empty list,
looping, and repeatedly calling
N@.append(), dictionaries and sets
N@can be created "all at once" rather
than by repeatedly calling
.update() or .add() in a loop.

    evince -p 5 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 15]

n@N@N@For example:

>>> {i:chr(65+i) for i in range(6)}
{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'}
>>> {chr(65+i) for i in range(6)}
{'A', 'B', 'C', 'D', 'E', 'F'}

The imperative versions of these
comprehensions would look very
similar to the examples shown
earlier for other built-in
datatypes.


Recursion

n@Functional programmers often put
weight in expressing flow-control
through recursion rather than
through loops. N@Done this way, we
can avoid altering the state of any
variables or data structures within
an algorithm, and more importantly
get more at the "what" than the
"how" of a computation. n@However,
in considering using recursive
styles we should distinguish between
the cases where recursion is just
"iteration by another name" and
those where a problem can readily be
partitioned into smaller problems,
each approached in a similar way.

There are two reasons why we should
make the distinction mentioned. On
the one hand, using recursion
effectively as a way of marching
through a sequence of elements is,
while possible, really not
"Pythonic." It matches the style of
other languages like Lisp,
definitely, but it often feels
contrived in Python. On the other
hand, Python is simply
comparatively slow at recursion,
and has a limited stack depth
limit. n@Yes, you can change this
with sys.setrecursion limit() to
more than the default 1000; but if
you find yourself doing so it is
probably a mistake. N@Python lacks an
internal feature called tail call
elimination that makes deep
recursion computationally efficient
in some languages. Let us find a
trivial example where recursion is
really just a kind of iteration:

def running_sum(numbers, start=0):
  if len(numbers) == 0:
    print()
    return
  total = numbers[0] + start
  print(total, end=" ")
  running_sum(numbers[1:], total)

    evince -p 6 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 16]

There is little to recommend this
approach, however; T@an iteration
that simply repeatedly modified the
total state variable would be more
readable, and moreover this
function is perfectly reasonable to
want to call against sequences of
much larger length than 1000.
However, in other cases, recursive
style, even over sequential
operations, still expresses
algorithms more intuitively and in
a way that is easier to reason
about. A slightly less trivial
example, factorial in recursive and
iterative style:

def factorialR(N):
  N@"Recursive factorial function"
  assert isinstance(N, int) and N >= 1
  return 1 if N <= 1 else N * factorialR(N-1)

def factorialI(N):
  "Iterative factorial function"
  assert isinstance(N, int) and N >= 1
  product = 1
  while N >= 1:
    product *= N
    N -= 1
  return product

Although this algorithm can also be
expressed easily enough with a
running product variable, the
recursive expression still comes
closer to the "what" than the "how"
of the algorithm. The details of
repeatedly changing the values of
product and N in the iterative
version feels like it's just
bookkeeping, n@not the nature of the
computation itself (but the
iterative version is probably
faster, and it is easy to reach the
recursion limit if it is not
adjusted). '

As a footnote, the fastest version
I know of for factorial() in Python
is in a functional programming
style, and also expresses the
"what" of the algorithm well once
some higher-order functions are
familiar:

N@N@from functools import reduce
from operator import mul
def factorialHOF(n):
  return reduce(mul, range(1, n+1), 1)

Where recursion is compelling, and
sometimes even the only really
obvious way to express a solution,
is when a problem offers itself to
a "divide and conquer" approach.

That is, if we can do a similar
computation on two halves (or
anyway, several similarly sized
chunks) of a larger collection. T@In
that case, the recursion depth is
only O(log N) of the size of the
collection, which is unlikely to be
overly deep.

    evince -p 7 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 17]

n@N@N@T@For example, the quicksort
algorithm is very elegantly
expressed without any state
variables or loops, but wholly
through recursion:

def quicksort(lst):
  "Quicksort over a list-like sequence"
  if len(lst) == 0:
    return lst
  pivot = lst[0]
  pivots = [x for x in lst if x == pivot]
  small = quicksort([x for x in lst if x < pivot])
  large = quicksort([x for x in lst if x > pivot])
  return small + pivots + large

Some names are used in the function
body to hold convenient values,
N@but they are never mutated. It
would not be as readable, but the
definition could be written as a
single expression if we wanted to do
so. In fact, it is somewhat
difficult, and certainly less
intuitive, to transform this into a
stateful iterative version.

As general advice, it is good
practice to look for possibilities
of recursive expression - and
especially for versions that avoid
the need for state variables or
mutable data collections - whenever
a problem looks partitionable into
smaller problems. It is not a good
idea in Python - most of the time -
to use recursion merely for
"iteration by other means."


Eliminating Loops

Just for fun, let us take a quick
look at how we could take out all
loops from any Python program. Most
of the time this is a bad idea,
N@both for readability and
performance, but it is worth
looking at how simple it is to do
in a systematic fashion as
background to contemplate those
cases where it is actually a good
idea.

If we simply call a function inside
a for loop, the built-in
higher-order function map() comes to
our aid:

# statement-based loop
for e in it:
  func(e)

N@N@HM@The following code is
entirely equivalent to the
functional version, except there is
no repeated rebinding of the
variable e involved, and hence no
state:

# map()-based "loop"
map(func, it)

    evince -p 8 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 18]

A similar technique is available for
a functional approach to sequential
program flow. Most imperative
programming consists of statements
that amount to "do this, then do
that, then do the other thing."
N@N@If those individual actions are
wrapped in functions, map() lets us
do just this:

n@N@N@# let f1, f2, f3 (etc) be functions
# that perform actions

# an execution utility function
do_it = lambda f, *args: f(*args)
# map()-based action sequence
map(do_it, [f1, f2, f3])

We can combine the sequencing of
function calls with passing
arguments from iterables:

>>> hello = lambda first, last: print("Hello", first, last)
>>> bye = lambda first, last: print("Bye", first, last)
>>> N@N@_ = list(map(do_it, [hello, bye],
>>>                     ['David','Jane'], ['Mertz','Doe']))
Hello David Mertz
Bye Jane Doe

Of course, looking at the example,
one suspects the result one really
wants is actually to pass all the
arguments to each of the functions
rather than one argument from each
list to each function. Expressing
that is difficult without using a
list comprehension, but easy enough
using one:

>>> do_all_funcs = lambda fns, *args: [
                          list(map(fn, *args)) for fn in fns]
>>> _ = do_all_funcs([hello, bye],
                     ['David','Jane'], ['Mertz','Doe'])
Hello David Mertz
Hello Jane Doe
Bye David Mertz
Bye Jane Doe

In general, the whole of our main
program could, in principle, be a
map() expression with an iterable
of functions to execute to complete
the program.

Translating while is slightly more
complicated, but is possible to do
directly using recursion:

# statement-based while loop
  while <cond>:
    <pre-suite>
    if <break_condition>:
      break
    else:
      <suite>

T@# FP-style recursive while loop
def while_block():
  <pre-suite>
  if <break_condition>:
    return 1
  else:
    <suite>
  return 0

while_FP = lambda: (<cond> and while_block()) or while_FP()
while_FP()

    evince -p 9 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 19]

Our translation of while still
requires a while_block() function
that may itself contain statements
rather than just expressions. We
could go further in turning suites
into function sequences, using
map() as above. If we did this, we
could, moreover, also return a
single ternary expression. n@The
details of this further purely
functional refactoring is left to
readers as an exercise (hint: it
will be ugly; fun to play with, but
not good production code).

It is hard for <cond> to be useful
with the usual tests, such as while
myvar==7, since the loop body (by
design) cannot change any variable
values. One way to add a more
useful condition is to let
while_block() return a more
interesting value, and compare that
return value for a termination
condition. Here is a concrete
example of eliminating statements:

# imperative version of "echo()"
def echo_IMP():
  while 1:
    x = input("IMP -- ")
    if x == 'quit':
      break
    else:
      print(x)
echo_IMP()

Now let's remove the while loop for
the functional version: '

# FP version of "echo()"
def identity_print(x):
    # "identity with side-effect"
  print(x)
  return x
echo_FP = lambda: identity_print(input("FP -- ")) == 'quit' or echo_FP()
echo_FP()

    evince -p 10 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 20]

N@What we have accomplished is that
we have managed to express a little
program that involves I/O, looping,
and conditional statements as a
pure expression with recursion (in
fact, as a function object that can
be passed elsewhere if desired). We
do still utilize the utility
function identity_print(), but this
function is completely general, and
can be reused in every functional
program expression we might create
later (it's a one-time cost).
Notice that any expression
containing identity_print(x)
evaluates to the same thing as if
it had simply contained x; it is
only called for its I/O side
effect. '


Eliminating Recursion

N@N@As with the simple factorial
example given above, sometimes we
can perform "recursion without
recursion" by using
functools.reduce() or other folding
operations (N@other "folds" are not
in the Python standard library, but
can easily be constructed and/or
occur in third-party libraries).
Te@A recursion is often simply a way
of combining something simpler with
an accumulated intermediate result,
and that is exactly what reduce()
does at heart. A slightly longer
discussion of functools.reduce()
occurs in the chapter on
higher-order functions.

    evince -p 11 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 21]

Callables

The emphasis in functional
programming is, somewhat
tautologously, on calling
functions. Python actually gives us
several different ways to create
functions, n@or at least something
very function-like (i.e., that can
be called). They are:

- Regular functions created with
  def and given a name at
  definition time
- Anonymous functions created with
  lambda
- n@Instances of classes that define
  a __call()__ method
- N@Closures returned by function
  factories
- n@Static methods of instances,
  either via the @staticmethod
  decorator or via the class
  __dict__
- n@Generator functions

This list is probably not
exhaustive, but it gives a sense of
the numerous slightly different
ways one can create something
callable. Of course, a plain method
of a class instance is also a
callable, n@but one generally uses
those where the emphasis is on
accessing and modifying mutable
state.

Python is a multiple paradigm
language, but it has an emphasis on
object-oriented styles. When one
defines a class, it is generally to
generate instances meant as
containers for data that change as
one calls methods of the class.
This style is in some ways opposite
to a functional programming
approach, which emphasizes
immutability and pure functions.

    evince -p 12 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 22]

Any method that accesses the state
of an instance (in any degree) to
determine what result to return is
not a pure function. Of course, all
the other types of callables we
discuss also allow reliance on
state in various ways. N@The author
of this report has long pondered
whether he could use some dark
magic within Python explicitly to
declare a function as pure - say by
decorating it with a hypothetical
@purefunction decorator that would
raise an exception if the function
can have side effects - but
consensus seems to be that it would
be impossible to guard against
every edge case in Python's
internal machinery. '

N@The advantage of a pure function
and side-effect-free code is that
it is generally easier to debug and
test. N@Callables that freely
intersperse statefulness with their
returned results cannot be examined
independently of their running
context to see how they behave, at
least not entirely so. For example,
a unit test (N@using doctest or
unittest, or some third-party
testing framework such as N@py.test
or nose) might succeed in one
context but fail when identical
calls are made within a running,
stateful program. Of course, at the
very least, any program that does
anything must have some kind of
output (whether to console, a file,
a database, over the network, or
whatever) in it to do anything
useful, so side effects cannot be
entirely eliminated, only isolated
to a degree when thinking in
functional programming terms.


Named Functions and Lambdas

The most obvious ways to create
callables in Python are, in definite
order of obviousness, named
functions and lambdas. N@The only
in-principle difference between them
is simply whether they have a
.__qualname__ attribute, since both
can very well be bound to one or
more names. n@N@In most cases,
lambda expressions are used within
Python only for callbacks and other
uses where a simple action is
inlined into a function call. N@But
as we have shown in this report,
flow control in general can be
incorporated into single-expression
lambdas if we really want. Let's
define a simple example to
illustrate: '

>>> def hello1(name):
...   print("Hello", name)
...
>>> hello2 = lambda name: print("Hello", name)
>>> hello1('David')
Hello David

    evince -p 13 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 23]

>>> hello2('David')
Hello David
>>> hello1.__qualname__
'hello1'
>>> hello2.__qualname__
'<lambda>'
# can bind func to other names
>>> hello3 = hello2
>>> N@hello3.__qualname__
'<lambda>'
>>> hello3.__qualname__ = 'hello3'
>>> hello3.__qualname__
'hello3'

One of the reasons that functions
are useful is that they isolate
state lexically, and avoid
contamination of enclosing
namespaces. This is a limited form
of non-mutability in that (by
default) nothing you do within a
function will bind state variables
outside the function. Of course,
this guarantee is very limited in
that both the global and nonlocal
statements explicitly allow state
to "leak out" of a function.
Moreover, many data types are
themselves mutable, so if they are
passed into a function that
function might change their
contents. T@Furthermore, doing I/O
can also change the "state of the
world" and hence alter results of
functions (e.g., by changing the
contents of a file or a database
that is itself read elsewhere).

Notwithstanding all the caveats and
limits mentioned above, a programmer
who wants to focus on a functional
programming style can intentionally
decide to write many functions as
pure functions to allow mathematical
and formal reasoning about them. In
most cases, one only leaks state
intentionally, and creating a
certain subset of all your
functionality as pure functions
allows for cleaner code. Hm@They
might perhaps be broken up by "pure"
modules, or annotated in the
function names or docstrings.


Closures and Callable Instances

n@N@N@There is a saying in computer
science that a class is "data with
operations attached" while a
closure is "operations with data
attached." In some sense they
accomplish much the same thing of
putting logic and data in the same
object. But there is definitely a
philosophical difference in the
approaches, with classes
emphasizing mutable or rebindable
state, and closures emphasizing
immutability and pure functions.
Neither side of this divide is
absolute - at least in Python
- but different attitudes motivate
the use of each.

    evince -p 14 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 24]

Let us construct a toy example that
shows this, something just past a
"hello world" of the different
styles:

# A class that creates callable adder instances
class Adder(object):
  def __init__(self, n):
    self.n = n
  def __call__(self, m):
    return self.n + m
add5_i = Adder(5)
  # "instance" or "imperative"

We have constructed something
callable that adds five to an
argument passed in. Seems simple
and mathematical enough. Let us
also try it as a closure:

def make_adder(n):
  def adder(m):
    return m + n
  return adder
add5_f = make_adder(5)
  # "functional"

So far these seem to amount to
pretty much the same thing, but the
mutable state in the instance
provides an attractive nuisance:

>>> add5_i(10)
15
# only argument affects result
>>> add5_f(10)
15
# state is readily changeable
>>> add5_i.n = 10
# result is dependent on prior flow
>>> add5_i(10)
20

The behavior of an "adder" created
by either Adder() or make_adder()
is, of course, not determined until
runtime in general. But once the
object exists, the closure behaves
in a pure functional way, while the
class instance remains state
dependent. One might simply settle
for "don't change that state" - and
indeed that is possible (if no one
else with poorer understanding
imports and uses your code) - but
one is accustomed to changing the
state of instances, and a style
that prevents abuse
programmatically encourages better
habits.

There is a little "gotcha" about how
Python binds variables in closures.
N@It does so by name rather than
value, and that can cause confusion,
but also has an easy solution. For
example, what if we want to
manufacture several related closures
encapsulating different data:

    evince -p 15 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 25]

# almost surely not the behavior we intended!
>>> adders = []
>>> for n in range(5):
      adders.append(lambda m: m+n)
>>> [adder(10) for adder in adders]
[14, 14, 14, 14, 14]
>>> n = 10
>>> [adder(10) for adder in adders]
[20, 20, 20, 20, 20]

Fortunately, a small change brings
behavior that probably better meets
our goal:

>>> adders = []
>>> for n in range(5):
...   adders.append(lambda m, n=n: m+n)
...
>>> [adder(10) for adder in adders]
[10, 11, 12, 13, 14]
>>> n = 10
>>> [adder(10) for adder in adders]
[10, 11, 12, 13, 14]
>>> add4 = adders[4]
# Can override the bound value
>>> add4(10, 100)
110

n@Notice that using the keyword
argument scope-binding trick allows
you to change the closed-over
value; but this poses much less of
a danger for confusion than in the
class instance. The overriding
value for the named variable must
be passed explictly in the call
itself, not rebound somewhere
remote in the program flow. Yes,
the name add4 is no longer
accurately descriptive for "add any
two numbers," but at least the
change in result is syntactically
local.


Methods of Classes

All methods of classes are
callables. For the most part,
however, calling a method of an
instance goes against the grain of
functional programming styles.
N@N@Usually we use methods because
we want to reference mutable data
that is bundled in the attributes of
the instance, and hence each call to
a method may produce a different
result that varies independently of
the arguments passed to it.


Accessors and Operators

Even accessors, whether created
with the @property decorator or
otherwise, are technically
callables, albeit accessors are
callables with Methods of Classes a
limited use (from a functional
programming perspective) in that
they take no arguments as getters,
and return no value as setters:

    evince -p 16 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 26]

T@T@class Car(object):
  def __init__(self):
    self._speed = 100

  @property
  def speed(self):
    print("Speed is", self._speed)
    return self._speed

  @speed.setter
  def speed(self, value):
    print("Setting to", value)
    self._speed = value

# >> car = Car()
# >>> car.speed = 80
  # Odd syntax to pass one argument
# Setting to 80
# >>> x = car.speed
# Speed is 80

___zzzz
In an accessor, we co-opt the
Python syntax of assignment to pass
an argument instead. That in itself
is fairly easy for much Python
syntax though, for example:

>>> class TalkativeInt(int):
      def __lshift__(self, other):
        print("Shift", self, "by", other)
        return int.__lshift__(self, other)
...
>>> t = TalkativeInt(8)
>>> t << 3
Shift 8 by 3
64

Every operator in Python is
basically a method call "under the
hood." But while occasionally
producing a more readable "domain
specific language" (DSL), defining
special callable meanings for
operators adds no improvement to
the underlying capabilities of
function calls.


Static Methods of Instances

n@N@N@One use of classes and their
methods that is more closely
aligned with a functional style of
programming is to use them simply
as namespaces to hold a variety of
related functions:

    evince -p 17 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 27]

import math

class RightTriangle(object):
  "Class used solely as namespace for related functions"
  TT@@staticmethod
  def hypotenuse(a, b):
    return math.sqrt(a**2 + b**2)

  @staticmethod
  def sin(a, b):
    return a / RightTriangle.hypotenuse(a, b)

  @staticmethod
  def cos(a, b):
    return b / RightTriangle.hypotenuse(a, b)

Keeping this functionality in a
class avoids polluting the global
(or module, etc.) namespace, and
lets us name either the class or an
instance of it when we make calls
to pure functions. For example:

>>> RightTriangle.hypotenuse(3,4)
5.0
>>> rt = RightTriangle()
>>> rt.sin(3,4)
0.6
>>> rt.cos(3,4)
0.8

By far the most straightforward way
to define static methods is with
the decorator named in the obvious
way. However, in Python 3.x, you
can pull out functions that have
not been so decorated too - i.e.,
the concept of an "unbound method"
is no longer needed in modern
Python versions:

>>> import functools, operator
>>> class Math(object):
...   def product(*nums):
...     return functools.reduce(operator.mul, nums)
...   def power_chain(*nums):
...     return functools.reduce(operator.pow, nums)
...
>>> Math.product(3,4,5)
60
>>> Math.power_chain(3,4,5)
3486784401

Without @staticmethod, however,
this will not work on the instances
since they still expect to be
passed self:

    evince -p 18 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 28]

>>> m = Math()
>>> m.product(3,4,5)
-----------------------------------------------------------------
TypeError
Traceback (most recent call last)
<ipython-input-5-e1de62cf88af> in <module>()
----> 1 m.product(3,4,5)

<ipython-input-2-535194f57a64> in product(*nums)
      2 class Math(object):
      3   def product(*nums):
----> 4     return functools.reduce(operator.mul, nums)
      5   def power_chain(*nums):
      6     return functools.reduce(operator.pow, nums)

TypeError: unsupported operand type(s) for *: 'Math' and 'int'

If your namespace is entirely a bag
for pure functions, there is no
reason not to call via the class
rather than the instance. But if
you wish to mix some pure functions
with some other stateful methods
that rely on instance mutable
state, you should use the @staticme
thod decorator.


Generator Functions

A special sort of function in
Python is one that contains a yield
statement, which turns it into a
generator. What is returned from
calling such a function is not a
regular value, but rather an
iterator that produces a sequence
of values as you call the next()
function on it or loop over it.
This is discussed in more detail in
the chapter entitled "Lazy
Evaluation."

While like any Python object, there
are many ways to introduce
statefulness into a generator, in
principle a generator can be "pure"
in the sense of a pure function. It
is merely a pure function that
produces a (potentially infinite)
sequence of values rather than a
single value, but still based only
on the arguments passed into it.
Notice, however, that generator
functions typically have a great
deal of internal state; it is at
the boundaries of call signature
and return value that they act like
a side-effect-free "black box." A
simple example:

>>> def get_primes():
...   "Simple lazy Sieve of Eratosthenes"
...   candidate = 2
...   found = []
...   while True:
...     if all(candidate % prime != 0 for prime in found):
...       yield candidate
...       found.append(candidate)
...     candidate += 1
...
>>> primes = get_primes()
>>> next(primes), next(primes), next(primes)
(2, 3, 5)
>>> for _, prime in zip(range(10), primes):
...   print(prime, end=" ")
...
7 11 13 17 19 23 29 31 37 41

    evince -p 19 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 29]

Every time you create a new object
with get_primes() the iterator is
the same infinite lazy sequence -
another example might pass in some
initializing values that affected
the result - but the object itself
is stateful as it is consumed
incrementally.


Multiple Dispatch

A very interesting approach to
programming multiple paths of
execution is a technique called
"multiple dispatch" or sometimes
"multimethods." The idea here is to
declare multiple signatures for a
single function and call the actual
computation that matches the types
or properties of the calling
arguments. This technique often
allows one to avoid or reduce the
use of explicitly conditional
branching, and instead substitute
the use of more intuitive pattern
descriptions of arguments.

A long time ago, this author wrote
a module called n@multimethods that
was quite flexible in its options
for resolving "dispatch
linearization" but is also so old
as only to work with Python 2.x,
and was even written before Python
had decorators for more elegant
expression of the concept. Matthew
Rocklin's more recent multipledis
patch is a modern approach for
recent Python versions, albeit it
lacks some of the theoretical
arcana I explored in my ancient
module. Ideally, in this author's
opinion, a future Python version
would include a standardized syntax
or API for multiple dispatch (but
more likely the task will always be
the domain of third-party
libraries).

To explain how multiple dispatch
can make more readable and less
bug-prone code, let us implement
the game of rock/paper/scissors in
three styles. Let us create the
classes to play the game for all
the versions:

class Thing(object): pass
class Rock(Thing): pass
class Paper(Thing): pass
class Scissors(Thing): pass

    evince -p 20 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 30]

Many Branches

First a purely imperative version.
This is going to have a lot of
repetitive, nested, conditional
blocks that are easy to get wrong:

def beats(x, y):
  if isinstance(x, Rock):
    if isinstance(y, Rock):
      return None # No winner
    elif isinstance(y, Paper):
      return y
    elif isinstance(y, Scissors):
      return x
    else:
      raise TypeError("Unknown second thing")
  elif isinstance(x, Paper):
    if isinstance(y, Rock):
      return x
    elif isinstance(y, Paper):
      return None # No winner
    elif isinstance(y, Scissors):
      return y
    else:
      raise TypeError("Unknown second thing")
  elif isinstance(x, Scissors):
    if isinstance(y, Rock):
      return y
    elif isinstance(y, Paper):
      return x
    elif isinstance(y, Scissors):
      return None # No winner
    else:
      raise TypeError("Unknown second thing")
  else:
    raise TypeError("Unknown first thing")

rock, paper, scissors = Rock(), Paper(), Scissors()
# >>> beats(paper, rock)
# <__main__.Paper at 0x103b96b00>
# >>> beats(paper, 3)
# TypeError: Unknown second thing


Delegating to the Object

As a second try we might try to
eliminate some of the fragile
repitition with Python's "duck
typing" - that is, maybe we can
have different things share a
common method that is called as
needed: '

    evince -p 21 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 31]

class DuckRock(Rock):
  def beats(self, other):
    if N@N@isinstance(other, Rock):
      return None # No winner
    elif isinstance(other, Paper):
      return other
    elif isinstance(other, Scissors):
      return self
    else:
      raise TypeError("Unknown second thing")

class DuckPaper(Paper):
  def beats(self, other):
    if isinstance(other, Rock):
      return self
    elif isinstance(other, Paper):
      return None # No winner
    elif isinstance(other, Scissors):
      return other
    else:
      raise TypeError("Unknown second thing")

class DuckScissors(Scissors):
  def beats(self, other):
    if isinstance(other, Rock):
      return other
    elif isinstance(other, Paper):
      return self
    elif isinstance(other, Scissors):
      return None # No winner
    else:
      raise TypeError("Unknown second thing")

def beats2(x, y):
  if hasattr(x, 'beats'):
    return x.beats(y)
  else:
    raise TypeError("Unknown first thing")

rock, paper, scissors = DuckRock(), DuckPaper(), DuckScissors()
# >>> beats2(rock, paper)
# <__main__.DuckPaper at 0x103b894a8>
# >>> beats2(3, rock)
# TypeError: Unknown first thing

We haven't actually reduced the
amount of code, but this version
somewhat reduces the complexity
within each individual callable,
and reduces the level of nested
conditionals by one. Most of the
logic is pushed into separate
classes rather than deep branching.
In object-oriented programming we
can "delegate dispatch to the
object" (but only to the one
controlling object). '

    evince -p 22 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 32]

Pattern Matching

As a final try, we can express all
the logic more directly using
multiple dispatch. This should be
more readable, albeit there are
still a number of cases to define:

from multipledispatch import dispatch

N@N@@dispatch(Rock, Rock)
def beats3(x, y): return None

@dispatch(Rock, Paper)
def beats3(x, y): return y

@dispatch(Rock, Scissors)
def beats3(x, y): return x

@dispatch(Paper, Rock)
def beats3(x, y): return x

@dispatch(Paper, Paper)
def beats3(x, y): return None

@dispatch(Paper, Scissors)
def beats3(x, y): return x

@dispatch(Scissors, Rock)
def beats3(x, y): return y

@dispatch(Scissors, Paper)
def beats3(x, y): return x

@dispatch(Scissors, Scissors)
def beats3(x, y): return None

@dispatch(object, object)
def beats3(x, y):
  if not isinstance(x, (Rock, Paper, Scissors)):
    raise TypeError("Unknown first thing")
  else:
    raise TypeError("Unknown second thing")

# >>> beats3(rock, paper)
# <__main__.DuckPaper at 0x103b894a8>
# >>> beats3(rock, 3)
# TypeError: Unknown second thing

    evince -p 23 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 33]

Predicate-Based Dispatch

N@A really exotic approach to
expressing conditionals as dispatch
decisions is to include predicates
directly within the function
signatures (or perhaps within
decorators on them, as with
multipledispatch). I do not know of
any well-maintained Python library
that does this, but let us simply
stipulate a hypothetical library
briefly to illustrate the concept.
This imaginary library might be
aptly named predicative_dispatch:

from predicative_dispatch import predicate

@predicate(lambda x: x < 0, lambda y: True)
def sign(x, y):
  print("x is negative; y is", y)

@predicate(lambda x: x == 0, lambda y: True)
def sign(x, y):
  print("x is zero; y is", y)

@predicate(lambda x: x > 0, lambda y: True)
def sign(x, y):
  print("x is positive; y is", y)

While this small example is
obviously not a full specification,
the reader can see how we might
move much or all of the conditional
branching into the function call
signatures themselves, and this
might result in smaller, more
easily understood and debugged
functions.

    evince -p 25 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 35]

Lazy Evaluation


A powerful feature of Python is its
iterator protocol (which we will
get to shortly). This capability is
only loosely connected to
functional programming per se,
since Python does not quite offer
lazy data structures in the sense
of a language like Haskell.
However, use of the iterator
protocol - and Python's many
built-in or standard library
iteratables - accomplish much the
same effect as an actual lazy data
structure. '

Let us explain the contrast here in
slightly more detail. In a language
like Haskell, which is inherently
lazily evaluated, we might define a
list of all the prime numbers in a
manner like the following:

-- Define a list of ALL the prime numbers
primes = sieve [2 ..]
  where sieve (p:xs) = p : sieve [x | x <- xs, (x `rem` p)/=0]

This report is not the place to try
to teach Haskell, but you can see a
comprehension in there, which is in
fact the model that Python used in
introducing its own comprehensions.
There is also deep recursion
involved, which is not going to
work in Python.

Apart from syntactic differences,
or even the ability to recurse to
indefinite depth, the significant
difference here is that the Haskell
version of primes is an actual
(infinite) sequence, not just an
object capable of sequentially
producing elements (as was the
primes object we demonstrated in
the chapter entitled "Callables").
In particular, you can index into
an arbitrary element of the
infinite list of primes in Haskell,
and the intermediate values will be
produced internally as needed based
on the syntactic construction of
the list itself.

    evince -p 26 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 36]

Mind you, one can replicate this in
Python too, it just isn't in the
inherent syntax of the language and
takes more manual construction.
Given the get_primes() generator
function discussed earlier, we
might write our own container to
simulate the same thing, for
example: '

from collections.abc import Sequence
class ExpandingSequence(Sequence):
  def __init__(self, it):
    self.it = it
    self._cache = []
  def __getitem__(self, index):
    while len(self._cache) <= index:
      self._cache.append(next(self.it))
    return self._cache[index]
  def __len__(self):
    return len(self._cache)

This new container can be both lazy
and also indexible:

>>> primes = ExpandingSequence(get_primes())
>>> for _, p in zip(range(10), primes):
...   print(p, end=" ")
...
2 3 5 7 11 13 17 19 23 29
>>> primes[10]
31
>>> primes[5]
13
>>> len(primes)
11
>>> primes[100]
547
>>> len(primes)
101

Of course, there are other custom
capabilities we might want to
engineer in, since lazy data
structures are not inherently
intertwined into Python. Maybe we'd
like to be able to slice this
special sequence. Maybe we'd like a
prettier representation of the
object when printed. Maybe we
should report the length as inf if
we somehow signaled it was meant to
be infinite. All of this is
possible, but it takes a little bit
of code to add each behavior rather
than simply being the default
assumption of Python data
structures.

    evince -p 27 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 37]

The Iterator Protocol

n@N@N@The easiest way to create an
iterator - that is to say, a lazy
sequence - in Python is to define a
generator function, as was
discussed in the chapter entitled
"Callables." Simply use the yield
statement within the body of a
function to define the places
(usually in a loop) where values
are produced.

Or, technically, the easiest way is
to use one of the many iterable
objects already produced by
built-ins or the standard library
rather than programming a custom one
at all. N@Generator functions are
syntax sugar for defining a function
that returns an iterator.

N@Many objects have a method named
.__iter__(), which will return an
iterator when it is called,
generally via the iter() built-in
function, or even more often simply
by looping over the object (e.g.,
for item in collection: ...).

n@N@What an iterator is is the
object returned by a call to
iter(some thing), which itself has a
method named .__iter__() that simply
returns the object itself, and
another method named .__next__().
The reason the iterable itself still
has an .__iter__() method is to make
iter() idempotent. That is, this
identity should always hold (or
raise TypeError("object is not
iterable")):

iter_seq = iter(sequence)
iter(iter_seq) == iter_seq

The above remarks are a bit
abstract, so let us look at a few
concrete examples:

>>> lazy = open('06-laziness.md')
  # iterate over lines of file
>>> '__iter__' in dir(lazy) and '__next__' in dir(lazy)
True
>>> plus1 = map(lambda x: x+1, range(10))
>>> plus1
  # iterate over deferred computations
<map at 0x103b002b0>
>>> '__iter__' in dir(plus1) and '__next__' in dir(plus1)
True
>>> def to10():
...   for i in range(10):
...     yield i
...
>>> '__iter__' in dir(to10)
False
>>> '__iter__' in dir(to10()) and '__next__' in dir(to10())
True
>>> l = [1,2,3]
>>> '__iter__' in dir(l)
True
>>> '__next__' in dir(l)
False
>>> li = iter(l)
  # iterate over concrete collection
>>> li
<list_iterator at 0x103b11278>
>>> li == iter(li)
True

    evince -p 28 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 38]

T@In a functional programming style -
or even just generally for
readability - writing custom
iterators as generator functions is
most natural. However, we can also
create custom classes that obey the
protocol; often these will have
other behaviors (i.e., methods) as
well, but most such behaviors
necessarily rely on statefulness
and side effects to be meaningful.
For example:

from collections.abc import Iterable
class Fibonacci(Iterable):
  def __init__(self):
    self.a, self.b = 0, 1
    self.total = 0
  def __iter__(self):
    return self
  def __next__(self):
    self.a, self.b = self.b, self.a + self.b
    self.total += self.a
    return self.a
  def running_sum(self):
    return self.total

# >>> fib = Fibonacci()
# >>> fib.running_sum()
# 0
# >>> for _, i in zip(range(10), fib):
# ...   print(i, end=" ")
# ...
# 1 1 2 3 5 8 13 21 34 55
# >>> fib.running_sum()
# 143
# >>> next(fib)
# 89

This example is trivial, of course,
but it shows a class that both
implements the iterator protocol
and also provides an additional
method to return something stateful
about its instance. Since
statefulness is for object-oriented
programmers, in a functional
programming style we will generally
avoid classes like this.

    evince -p 29 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 39]

n@ N@N@Module: itertools

The module itertools is a
collection of very powerful - and
carefully designed - functions for
performing iterator algebra. That
is, these allow you to combine
iterators in sophisticated ways
without having to concretely
instantiate anything more than is
currently required. As well as the
basic functions in the module
itself, the module documentation
provides a number of short, but
easy to get subtly wrong, recipes
for additional functions that each
utilize two or three of the basic
functions in combination. The
third-party module N@more_itertools
mentioned in the Preface provides
additional functions that are
likewise designed to avoid common
pitfalls and edge cases.

The basic goal of using the
building blocks inside itertools is
to avoid performing computations
before they are required, to avoid
the memory requirements of a large
instantiated collection, to avoid
potentially slow I/O until it is
stricly required, and so on.
Iterators are lazy sequences rather
than realized collections, and when
combined with functions or recipes
in itertools they retain this
property.

Here is a quick example of
combining a few things. Rather than
the stateful Fibonacci class to let
us keep a running sum, we might
simply create a single lazy
iterator to generate both the
current number and this sum:

>>> def fibonacci():
...   a, b = 1, 1
...   while True:
...     yield a
...     a, b = b, a+b
...
>>> N@N@from itertools import tee, accumulate
>>> s, t = tee(fibonacci())
>>> pairs = zip(t, accumulate(s))
>>> for _, (fib, total) in zip(range(7), pairs):
...   print(fib, total)
...
1 1
1 2
2 4
3 7
5 12
8 20
13 33

    evince -p 30 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 40]

Figuring out exactly how to use
functions in itertools correctly
and optimally often requires
careful thought, but once combined,
remarkable power is obtained for
dealing with large, or even
infinite, iterators that could not
be done with concrete collections.

The documentation for the itertools
module contain details on its
combinatorial functions as well as
a number of short recipes for
combining them. This paper does not
have space to repeat those
descriptions, so just exhibiting a
few of them above will suffice.

N@N@Note that for practical purposes,
zip(), map(), filter(), and range()
(which is, in a sense, just a
terminating itertools.count())
could well live in itertools if
they were not built-ins. That is,
all of those functions lazily
generate sequential items (mostly
based on existing iterables)
without creating a concrete
sequence. N@N@Built-ins like all(),
any(), sum(), min(), max(), and
functools.reduce() also act on
iterables, but all of them, in the
general case, need to exhaust the
iterator rather than remain lazy.

The function itertools.product()
might be out of place in its module
since it also creates concrete
cached sequences, and cannot
operate on infinite iterators.


Chaining Iterables

N@N@The itertools.chain() and
itertools.chain.from_iterable()
functions combine multiple
iterables. Built-in zip() and iter
tools.zip_longest() also do this,
of course, but in manners that
allow incremental advancement
through the iterables. A
consequence of this is that while
chaining infinite iterables is
valid syntactically and
semantically, no actual program
will exhaust the earlier iterable.
For example:

from itertools import chain, count
thrice_to_inf = chain(count(), count(), count())

n@T@Conceptually, thrice_to_inf will
count to infinity three times, but
in practice once would always be
enough. However, for merely large
iterables - not for infinite ones -
chaining can be very useful and
parsimonious:

def from_logs(fnames):
  yield from (open(file) for file in fnames)
lines = chain.from_iterable(from_logs(
              ['huge.log', 'gigantic.log']))

    evince -p 31 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 41]

Notice that in the example given,
we didn't even need to pass in a
concrete list of files - that
sequence of filenames itself could
be a lazy iterable per the API
given. '

N@N@Besides the chaining with
itertools, we should mention
collections.ChainMap() in the same
breath. Dictionaries (or generally
any collections.abc.Mapping) are
iterable (over their keys). Just as
we might want to chain multiple
sequence-like iterables, we
sometimes want to chain together
multiple mappings without needing to
create a single larger concrete one.
ChainMap() is handy, and does not
alter the underlying mappings used
to construct it.

    evince -p 33 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 43]

Higher-Order Functions


In the last chapter we saw an
iterator algebra that builds on the
iter tools module. In some ways,
higher-order functions (often
abbreviated as "HOFs") provide
similar building blocks to express
complex concepts by combining
simpler functions into new
functions. In general, a
higher-order function is simply a
function that takes one or more
functions as arguments and/or
produces a function as a result.
Many interesting abstractions are
available here. They allow chaining
and combining higher-order
functions in a manner analogous to
how we can combine functions in
itertools to produce new iterables.

N@N@A few useful higher-order
functions are contained in the
functools module, and a few others
are built-ins. It is common the
think of map(), filter(), and
functools.reduce() as the most basic
building blocks of higher-order
functions, and most functional
programming languages use these
functions as their primitives
(occasionally under other names).
Almost as basic as map/filter/reduce
as a building block is currying.
n@N@In Python, currying is spelled
as partial(), and is contained in
the functools module - this is a
function that will take another
function, along with zero or more
arguments to pre-fill, and return a
function of fewer arguments that
operates as the input function would
when those arguments are passed to
it.

N@The built-in functions map() and
filter() are equivalent to
comprehensions - especially now
that generator comprehensions are
available - and most Python
programmers find the comprehension
versions more readable. For
example, here are some (almost)
equivalent pairs:

    evince -p 34 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 44]

n@N@# Classic "FP-style"
transformed = map(tranformation, iterator)
# Comprehension
transformed = (transformation(x) for x in iterator)

# Classic "FP-style"
filtered = filter(predicate, iterator)
# Comprehension
filtered = (x for x in iterator if predicate(x))

N@N@The function functools.reduce()
is very general, very powerful, and
very subtle to use to its full
power. It takes successive items of
an iterable, and combines them in
some way. N@The most common use case
for reduce() is probably covered by
the built-in sum(), which is a more
compact spelling of:

from functools import reduce
total = reduce(operator.add, it, 0)
# total = sum(it)

It may or may not be obvious that
map() and filter() are also a
special cases of reduce(). That is:

>>> add5 = lambda n: n+5
>>> reduce(lambda l, x: l+[add5(x)], range(10), [])
[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
>>> # simpler: map(add5, range(10))
>>> isOdd = lambda n: n%2
>>> reduce(lambda l, x: l+[x] if isOdd(x) else l, range(10),
[])
[1, 3, 5, 7, 9]
>>> # simpler: filter(isOdd, range(10))

n@These reduce() expressions are
awkward, but they also illustrate
how powerful the function is in its
generality: anything that can be
computed from a sequence of
successive elements can (if
awkwardly) be expressed as a
reduction.

N@There are a few common higher-order
functions that are not among the
"batteries included" with Python,
but that are very easy to create as
utilities (and are included with
many third-party collections of
functional programming tools).
Different libraries - and other
programming languages - may use
different names for the utility
functions I describe, but analogous
capabilities are widespread (as are
the names I choose).

    evince -p 35 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 45]

Utility Higher-Order Functions

N@n@N@A handy utility is compose().
This is a function that takes a
sequence of functions and returns a
function that represents the
application of each of these
argument functions to a data
argument:

def compose(*funcs):
  """Return a new function s.t.
     compose(f,g,...)(x) == f(g(...(x)))"""
  def inner(data, funcs=funcs):
    result = data
    for f in reversed(funcs):
      result = f(result)
    return result
  return inner

# >>> times2 = lambda x: x*2
# >>> minus3 = lambda x: x-3
# >>> mod6 = lambda x: x%6
# >>> f = compose(mod6, times2, minus3)
# >>> all(f(i)==((i-3)*2)%6 for i in range(1000000))
# True

For these one-line math operations
(times2, minus3, etc.), we could
have simply written the underlying
math expression at least as easily;
but if the composite calculations
each involved branching,
flow-control, complex logic, etc.,
this would not be true.

T@The built-in functions all() and
any() are useful for asking whether
a predicate holds of elements of an
iterable. But it is also nice to be
able to ask whether any/all of a
collection of predicates hold for a
particular data item in a
composable way. We might implement
these as:

all_pred = lambda item, *tests: all(p(item) for p in tests)
any_pred = lambda item, *tests: any(p(item) for p in tests)

To show the use, let us make a few
predicates:

>>> is_lt100 = partial(operator.ge, 100)
  # less than 100?
>>> is_gt10 = partial(operator.le, 10)
  # greater than 10?
>>> from nums import is_prime
  # implemented elsewhere
>>> all_pred(71, is_lt100, is_gt10, is_prime)
True
>>> predicates = (is_lt100, is_gt10, is_prime)
>>> all_pred(107, *predicates)
False

n@T@T@The library toolz has what
might be a more general version of
this called juxt() that creates a
function that calls several
functions with the same arguments
and returns a tuple of results. We
could use that, for example, to do:

    evince -p 36 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 46]

>>> from toolz.functoolz import juxt
>>> juxt([is_lt100, is_gt10, is_prime])(71)
(True, True, True)
>>> all(juxt([is_lt100, is_gt10, is_prime])(71))
True
>>> juxt([is_lt100, is_gt10, is_prime])(107)
(False, True, True)

The utility higher-order functions
shown here are just a small
selection to illustrate
composability. T@Look at a longer
text on functional programming -
or, for example, read the Haskell
prelude - for many other ideas on
useful utility
higher-order-functions.


The operator Module

As has been shown in a few of the
examples, N@every operation that can
be done with Python's infix and
prefix operators corresponds to a
named function in the operator
module. For places where you want
to be able to pass a function
performing the equivalent of some
syntactic operation to some
higher-order function, HM@using the
name from operator is faster and
looks nicer than a corresponding
lambda. For example: '

# Compare ad hoc lambda with `operator` function
sum1 = reduce(lambda a, b: a+b, iterable, 0)
sum2 = reduce(operator.add, iterable, 0)
sum3 = sum(iterable)
  # The actual Pythonic way


The functools Module

The obvious place for Python to
include higher-order functions is
in the functools module, and indeed
a few are in there. However, there
are surprisingly few utility
higher-order functions in that
module. It has gained a few
interesting ones over Python
versions, but core developers have
a resistence to going in the
direction of a full functional
programming language. On the other
hand, as we have seen in a few
example above, many of the most
useful higher-order functions only
take a few lines (sometimes a
single line) to write yourself.

n@N@Apart from reduce(), which is
discussed at the start of this
chapter, the main facility in the
module is partial(), which has also
been mentioned.

    evince -p 37 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 47]

This operation is called "currying"
(after Haskell Curry) in many
languages. There are also some
examples of using partial()
discussed above.

n@N@The remainder of the functools
module is generally devoted to
useful decorators, which is the
topic of the next section.


Decorators

T@T@Although it is - by design - easy
to forget it, probably the most
common use of higher-order
functions in Python is as
decorators. N@T@A decorator is just
syntax sugar that takes a function
as an argument, and if it is
programmed correctly, returns a new
function that is in some way an
enhancement of the original
function (or method, or class).
Just to remind readers, these two
snippets of code defining some_func
and other_func are equivalent:

@enhanced
def some_func(*args):
  pass

def other_func(*args):
  pass
other_func = enhanced(other_func)

Used with the decorator syntax, of
course, the higher-order function
is necessarily used at definition
time for a function. For their
intended purpose, this is usually
when they are best applied. But the
same decorator function can always,
in principle, be used elsewhere in
a program, for example in a more
dynamic way (e.g., mapping a
decorator function across a
runtime-generated collection of
other functions). That would be an
unusual use case, however.

t@T@Decorators are used in many
places in the standard library and
in common third-party libraries. In
some ways they tie in with an idea
that used to be called
"aspect-oriented programming." For
example, the decorator function
T@asyncio.coroutine is used to mark
a function as a coroutine.
N@N@Within functools the three
important decorator functions are
functools.lru_cache,
functools.total_ordering, and
functools.wraps. The first
"memoizes" a function (i.e., it
caches the arguments passed and
returns stored values rather than
performing new computation or I/O).
The second makes it easier to write
custom classes that want to use
inequality operators. The last
makes it easier to write new
decorators.

    evince -p 38 ~/Empire/Doks/Comp/lang/py/funct/funcpy.pdf &
      [-p 48]

All of these are important and
worthwhile purposes, but they are
also more in the spirit of making
the plumbing of Python programming
easier in a general - almost
syntactic - way rather than the
composable higher-order functions
this chapter focuses on.

n@T@Decorators in general are more
useful when you want to poke into
the guts of a function than when
you want to treat it as a pluggable
component in a flow or composition
of functions, often done to mark
the purpose or capabilities of a
particular function.

This report has given only a
glimpse into some techniques for
programming Python in a more
functional style, and only some
suggestions as to the advantages
one often finds in aspiring in that
direction. Programs that use
functional programming are usually
shorter than more traditional
imperative ones, but much more
importantly, they are also usually
both more composable and more
provably correct. A large class of
difficult to debug errors in
program logic are avoided by
writing functions without side
effects, and even more errors are
avoided by writing small units of
functionality whose operation can
be understood and tested more
reliably.

A rich literature on functional
programming as a general technique
- often in particular languages
which are not Python - is available
and well respected. Studying one of
many such classic books, some
published by O'Reilly (including
very nice video training on
functional programming in Python),
can give readers further insight
into the nitty-gritty of functional
programming techniques. Almost
everything one might do in a more
purely functional language can be
done with very little adjustment in
Python as well. '



__pyfunct2

    evince -p i ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 1]
 
___zzzz2
1. Introduction
1.1 Formal provability
1.2 Modularity
1.3 Ease of debugging and testing
1.4 Composability

2. Iterators
2.1 Data Types That Support Iterators

3. Generator expressions and list comprehensions

4. Generators
4.1 Passing values into a generator

5. Built-in functions

6. The itertools module
6.1 Creating new iterators
6.2 Calling functions on elements
6.3 Selecting elements
6.4 Grouping elements

7. The functools module
7.1 The operator module

8. Small functions and the lambda expression

9. Revision History and Acknowledgements

10 References
10.1 General
10.2 Python-specific
10.3 Python documentation


    evince -p ii ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 2]

Indexxix

Author A. M. Kuchling
Release 0.31

In this document, we'll take a tour
of Python's features suitable for
implementing programs in a
functional style.

After an introduction to the
concepts of functional programming,
we'll look at language features such
as iterators and generators and
relevant library modules such as
itertools and functools.


1 Introduction

This section explains the basic
concept of functional programming;
if you're just interested in
learning about Python language
features, skip to the next section.

Programming languages support
decomposing problems in several
different ways:

- Most programming languages are
  procedural: programs are lists of
  instructions that tell the
  computer what to do with the
  program's input. C, Pascal, and
  even Unix shells are procedural
  languages.
- In declarative languages, you
  write a specification that
  describes the problem to be
  solved, and the language
  implementation figures out how to
  perform the computation
  efficiently. SQL is the
  declarative language you're most
  likely to be familiar with; a SQL
  query describes the data set you
  want to retrieve, and the SQL
  engine decides whether to scan
  tables or use indexes, which
  subclauses should be performed
  first, etc.
- Object-oriented programs
  manipulate collections of
  objects. Objects have internal
  state and support methods that
  query or modify this internal
  state in some way. Smalltalk and
  Java are object-oriented
  languages. C++ and Python are
  languages that support
  object-oriented programming, but
  don't force the use of
  object-oriented features.
- Functional programming decomposes
  a problem into a set of
  functions. Ideally, functions
  only take inputs and produce
  outputs, and don't have any
  internal state that affects the
  output produced for a given
  input. Well-known functional
  languages include the ML family
  (Standard ML, OCaml, and other
  variants) and Haskell.

The designers of some computer
languages choose to emphasize one
particular approach to programming.
This often makes it difficult to
write programs that use a different
approach. Other languages are
multi-paradigm languages that
support several different
approaches. Lisp, C++, and Python
are multi-paradigm; you can write
programs or libraries that are
largely procedural,
object-oriented, or functional in
all of these languages. In a large
program, different sections might
be written using different
approaches; the GUI might be
object-oriented while the
processing logic is procedural or
functional, for example.

In a functional program, input
flows through a set of functions.
Each function operates on its input
and produces some output.
Functional style discourages
functions with side effects that
modify internal state or make other
changes that aren't visible in the
function's return value. Functions
that have no side effects at all
are called purely functional.

Avoiding side effects means not
using data structures that get
updated as a program runs; every
function's output must only depend
on its input.

Some languages are very strict
about purity and don't even have
assignment statements such as a=3
or c = a + b, but it's difficult to
avoid all side effects. Printing to
the screen or writing to a disk
file are side effects, for example.
For example, in Python a call to
the print() or time.sleep()
function both return no useful
value; they're only called for
their side effects of sending some
text to the screen or pausing
execution for a second.

Python programs written in
functional style usually won't go
to the extreme of avoiding all I/O
or all assignments; instead,
they'll provide a
functional-appearing interface but
will use non-functional features
internally. For example, the
implementation of a function will
still use assignments to local
variables, but won't modify global
variables or have other side
effects.

    evince -p iii ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 3]

Functional programming can be
considered the opposite of
object-oriented programming.
Objects are little capsules
containing some internal state
along with a collection of method
calls that let you modify this
state, and programs consist of
making the right set of state
changes. Functional programming
wants to avoid state changes as
much as possible and works with
data flowing between functions. In
Python you might combine the two
approaches by writing functions
that take and return instances
representing objects in your
application (e-mail messages,
transactions, etc.).

Functional design may seem like an
odd constraint to work under. Why
should you avoid objects and side
effects? There are theoretical and
practical advantages to the
functional style:

- Formal provability.
- Modularity.
- Composability.
- Ease of debugging and testing.


1.1 Formal provability

A theoretical benefit is that it's
easier to construct a mathematical
proof that a functional program is
correct.

For a long time researchers have
been interested in finding ways to
mathematically prove programs
correct. This is different from
testing a program on numerous
inputs and concluding that its
output is usually correct, or
reading a program's source code and
concluding that the code looks
right; the goal is instead a
rigorous proof that a program
produces the right result for all
possible inputs.

The technique used to prove
programs correct is to write down
invariants, properties of the input
data and of the program's variables
that are always true. For each line
of code, you then show that if
invariants X and Y are true before
the line is executed, the slightly
different invariants X' and Y' are
true after the line is executed.
This continues until you reach the
end of the program, at which point
the invariants should match the
desired conditions on the program's
output.

Functional programming's avoidance
of assignments arose because
assignments are difficult to handle
with this technique; assignments
can break invariants that were true
before the assignment without
producing any new invariants that
can be propagated onward.

Unfortunately, proving programs
correct is largely impractical and
not relevant to Python software.
Even trivial programs require
proofs that are several pages long;
the proof of correctness for a
moderately complicated program
would be enormous, and few or none
of the programs you use daily (the
Python interpreter, your XML
parser, your web browser) could be
proven correct. Even if you wrote
down or generated a proof, there
would then be the question of
verifying the proof; maybe there's
an error in it, and you wrongly
believe you've proved the program
correct.


1.2 Modularity

A more practical benefit of
functional programming is that it
forces you to break apart your
problem into small pieces.

Programs are more modular as a
result. It's easier to specify and
write a small function that does
one thing than a large function
that performs a complicated
transformation. Small functions are
also easier to read and to check
for errors.


1.3 Ease of debugging and testing

Testing and debugging a
functional-style program is easier.

Debugging is simplified because
functions are generally small and
clearly specified. When a program
doesn't work, each function is an
interface point where you can check
that the data are correct. You can
look at the intermediate inputs and
outputs to quickly isolate the
function that's responsible for a
bug.

    evince -p iv ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 4]

Testing is easier because each
function is a potential subject for
a unit test. Functions don't depend
on system state that needs to be
replicated before running a test;
instead you only have to synthesize
the right input and then check that
the output matches expectations.


1.4 Composability

As you work on a functional-style
program, you'll write a number of
functions with varying inputs and
outputs. Some of these functions
will be unavoidably specialized to
a particular application, but
others will be useful in a wide
variety of programs. For example, a
function that takes a directory
path and returns all the XML files
in the directory, or a function
that takes a filename and returns
its contents, can be applied to
many different situations.

Over time you'll form a personal
library of utilities. Often you'll
assemble new programs by arranging
existing functions in a new
configuration and writing a few
functions specialized for the
current task.


2 Iterators

I'll start by looking at a Python
language feature that's an
important foundation for writing
functional-style programs:
iterators.

An iterator is an object
representing a stream of data; this
object returns the data one element
at a time. A Python iterator must
support a method called __next__()
that takes no arguments and always
returns the next element of the
stream. If there are no more
elements in the stream, __next__()
must raise the StopIteration
exception. Iterators don't have to
be finite, though; it's perfectly
reasonable to write an iterator
that produces an infinite stream of
data.

The built-in iter() function takes
an arbitrary object and tries to
return an iterator that will return
the object's contents or elements,
raising TypeError if the object
doesn't support iteration. Several
of Python's built-in data types
support iteration, the most common
being lists and dictionaries. An
object is called iterable if you
can get an iterator for it.

You can experiment with the
iteration interface manually:

>>> L = [1,2,3]
>>> it = iter(L)
>>> it
<...iterator object at ...>
>>> it.__next__()
  # same as next(it)
1
>>> next(it)
2
>>> next(it)
3
>>> next(it)
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
StopIteration
>>>

Python expects iterable objects in
several different contexts, the
most important being the for
statement. In the statement for X
in Y, Y must be an iterator or some
object for which iter() can create
an iterator. These two statements
are equivalent:

for i in iter(obj):
  print(i)

    evince -p v ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 5]

for i in obj:
  print(i)

Iterators can be materialized as
lists or tuples by using the list()
or tuple() constructor functions:

>>> L = [1,2,3]
>>> iterator = iter(L)
>>> t = tuple(iterator)
>>> t
(1, 2, 3)

Sequence unpacking also supports
iterators: if you know an iterator
will return N elements, you can
unpack them into an N-tuple:

>>> L = [1,2,3]
>>> iterator = iter(L)
>>> a,b,c = iterator
>>> a,b,c
(1, 2, 3)

Built-in functions such as max()
and min() can take a single
iterator argument and will return
the largest or smallest element.
The "in" and "not in" operators
also support iterators: X in
iterator is true if X is found in
the stream returned by the
iterator. You'll run into obvious
problems if the iterator is
infinite; max(), min() will never
return, and if the element X never
appears in the stream, the "in" and
"not in" operators won't return
either.

Note that you can only go forward
in an iterator; there's no way to
get the previous element, reset the
iterator, or make a copy of it.
Iterator objects can optionally
provide these additional
capabilities, but the iterator
protocol only specifies the
__next__() method. Functions may
therefore consume all of the
iterator's output, and if you need
to do something different with the
same stream, you'll have to create
a new iterator.


2.1 Data Types That Support Iterators

We've already seen how lists and
tuples support iterators. In fact,
any Python sequence type, such as
strings, will automatically support
creation of an iterator.

Calling iter() on a dictionary
returns an iterator that will loop
over the dictionary's keys:

>>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3,
...      'Apr': 4, 'May': 5, 'Jun': 6,
...      'Jul': 7, 'Aug': 8, 'Sep': 9,
...      'Oct': 10, 'Nov': 11, 'Dec': 12}
>>> for key in m:
...   print(key, m[key])

Mar 3
Feb 2
Aug 8
Sep 9
Apr 4
Jun 6
Jul 7
Jan 1
May 5
Nov 11
Dec 12
Oct 10

Note that the order is essentially
random, because it's based on the
hash ordering of the objects in the
dictionary.

Applying iter() to a dictionary
always loops over the keys, but
dictionaries have methods that
return other iterators. If you want
to iterate over values or key/value
pairs, you can explicitly call the
values() or items() methods to get
an appropriate iterator.

    evince -p vi ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 6]

The dict() constructor can accept
an iterator that returns a finite
stream of (key, value) tuples:

>>> L = [('Italy', 'Rome'), ('France', 'Paris'), ('US', 'Washington DC')]
>>> dict(iter(L))
{'Italy': 'Rome', 'US': 'Washington DC', 'France': 'Paris'}

Files also support iteration by
calling the readline() method until
there are no more lines in the
file. This means you can read each
line of a file like this:

for line in file:
  # do something for each line
  ...

Sets can take their contents from
an iterable and let you iterate
over the set's elements:

S = {2, 3, 5, 7, 11, 13}
for i in S:
  print(i)


3 Generator expressions and list comprehensions

Two common operations on an
iterator's output are 1) performing
some operation for every element,
2) selecting a subset of elements
that meet some condition. For
example, given a list of strings,
you might want to strip off
trailing whitespace from each line
or extract all the strings
containing a given substring.

List comprehensions and generator
expressions (short form:
"listcomps" and "genexps") are a
concise notation for such
operations, borrowed from the
functional programming language
Haskell (http://www.haskell.org/).
You can strip all the whitespace
from a stream of strings with the
following code:

line_list = ['  line 1\n', 'line 2  \n', ...]

# Generator expression -- returns iterator
stripped_iter = (line.strip() for line in line_list)

# List comprehension -- returns list
stripped_list = [line.strip() for line in line_list]

You can select only certain
elements by adding an "if"
condition:

stripped_list = [line.strip() for line in line_list
                 if line != ""]

With a list comprehension, you get
back a Python list; stripped_list
is a list containing the resulting
lines, not an iterator. Generator
expressions return an iterator that
computes the values as necessary,
not needing to materialize all the
values at once. This means that
list comprehensions aren't useful
if you're working with iterators
that return an infinite stream or a
very large amount of data.
Generator expressions are
preferable in these situations.

Generator expressions are
surrounded by parentheses ("()")
and list comprehensions are
surrounded by square brackets
("[]"). Generator expressions have
the form:

(expression for expr in sequence1
            if condition1
            for expr2 in sequence2
            if condition2
            for expr3 in sequence3 ...
            if condition3
            for exprN in sequenceN
            if conditionN)

    evince -p vii ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 7]

Again, for a list comprehension
only the outside brackets are
different (square brackets instead
of parentheses).

The elements of the generated
output will be the successive
values of expression. The if
clauses are all optional; if
present, expression is only
evaluated and added to the result
when condition is true.

Generator expressions always have
to be written inside parentheses,
but the parentheses signalling a
function call also count. If you
want to create an iterator that
will be immediately passed to a
function you can write:

obj_total = sum(obj.count for obj in list_all_objects())

The for...in clauses contain the
sequences to be iterated over. The
sequences do not have to be the
same length, because they are
iterated over from left to right,
not in parallel. For each element
in sequence1, sequence2 is looped
over from the beginning. sequence3
is then looped over for each
resulting pair of elements from
sequence1 and sequence2.

To put it another way, a list
comprehension or generator
expression is equivalent to the
following Python code:

for expr1 in sequence1:
  if not (condition1):
    continue  # Skip this element
  for expr2 in sequence2:
    if not (condition2):
      continue  # Skip this element
    ...
    for exprN in sequenceN:
      if not (conditionN):
        continue  # Skip this element

      # Output the value of
      # the expression.

This means that when there are
multiple for...in clauses but no if
clauses, the length of the
resulting output will be equal to
the product of the lengths of all
the sequences. If you have two
lists of length 3, the output list
is 9 elements long:

>>> seq1 = 'abc'
>>> seq2 = (1,2,3)
>>> [(x, y) for x in seq1 for y in seq2]
[('a', 1), ('a', 2), ('a', 3),
 ('b', 1), ('b', 2), ('b', 3),
 ('c', 1), ('c', 2), ('c', 3)]

To avoid introducing an ambiguity
into Python's grammar, if
expression is creating a tuple, it
must be surrounded with
parentheses. The first list
comprehension below is a syntax
error, while the second one is
correct:

# Syntax error
[x, y for x in seq1 for y in seq2]
# Correct
[(x, y) for x in seq1 for y in seq2]


4 Generators

Generators are a special class of
functions that simplify the task of
writing iterators. Regular
functions compute a value and
return it, but generators return an
iterator that returns a stream of
values.

    evince -p viii ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 8]

You're doubtless familiar with how
regular function calls work in
Python or C. When you call a
function, it gets a private
namespace where its local variables
are created. When the function
reaches a return statement, the
local variables are destroyed and
the value is returned to the
caller. A later call to the same
function creates a new private
namespace and a fresh set of local
variables. But, what if the local
variables weren't thrown away on
exiting a function? What if you
could later resume the function
where it left off? This is what
generators provide; they can be
thought of as resumable functions.

Here's the simplest example of a
generator function:

>>> def generate_ints(N):
...   for i in range(N):
...     yield i

Any function containing a yield
keyword is a generator function;
this is detected by Python's
bytecode compiler which compiles
the function specially as a result.

When you call a generator function,
it doesn't return a single value;
instead it returns a generator
object that supports the iterator
protocol. On executing the yield
expression, the generator outputs
the value of i, similar to a return
statement. The big difference
between yield and a return
statement is that on reaching a
yield the generator's state of
execution is suspended and local
variables are preserved. On the
next call to the generator's
__next__() method, the function
will resume executing.

Here's a sample usage of the
generate_ints() generator:

>>> gen = generate_ints(3)
>>> gen
<generator object generate_ints at ...>
>>> next(gen)
0
>>> next(gen)
1
>>> next(gen)
2
>>> next(gen)
Traceback (most recent call last):
  File "stdin", line 1, in ?
  File "stdin", line 2, in generate_ints
StopIteration

You could equally write for i in
generate_ints(5), or a,b,c =
generate_ints(3).

Inside a generator function, return
value is semantically equivalent to
raise StopIteration(value). If no
value is returned or the bottom of
the function is reached, the
procession of values ends and the
generator cannot return any further
values.

You could achieve the effect of
generators manually by writing your
own class and storing all the local
variables of the generator as
instance variables. For example,
returning a list of integers could
be done by setting self.count to 0,
and having the __next__() method
increment self.count and return it.
However, for a moderately
complicated generator, writing a
corresponding class can be much
messier.

The test suite included with
Python's library,
Lib/test/test_generators.py,
contains a number of more
interesting examples. Here's one
generator that implements an
in-order traversal of a tree using
generators recursively.

# A recursive generator that generates Tree leaves in in-order.
def inorder(t):
  if t:
    for x in inorder(t.left):
      yield x
    yield t.label

    for x in inorder(t.right):
      yield x

    evince -p ix ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 9]

Two other examples in
test_generators.py produce
solutions for the N-Queens problem
(placing N queens on an NxN chess
board so that no queen threatens
another) and the Knight's Tour
(finding a route that takes a
knight to every square of an NxN
chessboard without visiting any
square twice).


4.1 Passing values into a generator

In Python 2.4 and earlier,
generators only produced output.
Once a generator's code was invoked
to create an iterator, there was no
way to pass any new information
into the function when its
execution is resumed. You could
hack together this ability by
making the generator look at a
global variable or by passing in
some mutable object that callers
then modify, but these approaches
are messy.

In Python 2.5 there's a simple way
to pass values into a generator.
yield became an expression,
returning a value that can be
assigned to a variable or otherwise
operated on:

val = (yield i)

TT@I recommend that you always put
parentheses around a yield
expression when you're doing
something with the returned value,
as in the above example. The
parentheses aren't always
necessary, but it's easier to
always add them instead of having
to remember when they're needed.

(PEP 342 explains the exact rules,
which are that a yield-expression
must always be parenthesized except
when it occurs at the top-level
expression on the right-hand side
of an assignment. This means you
can write val = yield i but have to
use parentheses when there's an
operation, as in val = (yield i) +
12.)

Values are sent into a generator by
calling its send(value) method.
This method resumes the generator's
code and the yield expression
returns the specified value. If the
regular __next__() method is
called, the yield returns None.

Here's a simple counter that
increments by 1 and allows changing
the value of the internal counter.

def counter(maximum):
  i = 0
  while i < maximum:
    val = (yield i)
    # If value provided, change counter
    if val is not None:
      i = val
    else:
      i += 1

And here's an example of changing
the counter:

>>> it = counter(10)
>>> next(it)
0
>>> next(it)
1
>>> it.send(8)
8
>>> next(it)
9
>>> next(it)
Traceback (most recent call last):
  File "t.py", line 15, in ?
    it.next()
StopIteration

    evince -p x ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 10]

Because yield will often be
returning None, you should always
check for this case. Don't just use
its value in expressions unless
you're sure that the send() method
will be the only method used resume
your generator function.

In addition to send(), there are
two other methods on generators:

- throw(type, value=None,
  traceback=None) is used to raise
  an exception inside the
  generator; the exception is
  raised by the yield expression
  where the generator's execution
  is paused.
- close() raises a GeneratorExit
  exception inside the generator to
  terminate the iteration. On
  receiving this exception, the
  generator's code must either
  raise GeneratorExit or
  StopIteration; catching the
  exception and doing anything else
  is illegal and will trigger a
  RuntimeError. close() will also
  be called by Python's garbage
  collector when the generator is
  garbage-collected.

  If you need to run cleanup code
  when a GeneratorExit occurs, I
  suggest using a try: ...
  finally: suite instead of
  catching GeneratorExit.

The cumulative effect of these
changes is to turn generators from
one-way producers of information
into both producers and consumers.

Generators also become coroutines,
a more generalized form of
subroutines. Subroutines are
entered at one point and exited at
another point (the top of the
function, and a return statement),
but coroutines can be entered,
exited, and resumed at many
different points (the yield
statements).


5 Built-in functions

Let's look in more detail at
built-in functions often used with
iterators.

Two of Python's built-in functions,
map() and filter() duplicate the
features of generator expressions:

map(f, iterA, iterB, ...)

  returns an iterator over the
  sequence f(iterA[0], iterB[0]),
  f(iterA[1], iterB[1]),
  f(iterA[2], iterB[2]), ....

>>> def upper(s):
...   return s.upper()
>>> list(map(upper, ['sentence', 'fragment']))
['SENTENCE', 'FRAGMENT']
>>> [upper(s) for s in ['sentence', 'fragment']]
['SENTENCE', 'FRAGMENT']

You can of course achieve the same
effect with a list comprehension.

filter(predicate, iter) returns an
iterator over all the sequence
elements that meet a certain
condition, and is similarly
duplicated by list comprehensions.
A predicate is a function that
returns the truth value of some
condition; for use with filter(),
the predicate must take a single
value.

>>> def is_even(x):
...   return (x % 2) == 0
>>> list(filter(is_even, range(10)))
[0, 2, 4, 6, 8]

This can also be written as a list
comprehension:

>>> list(x for x in range(10) if is_even(x))
[0, 2, 4, 6, 8]

    evince -p xi ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 11]

enumerate(iter) counts off the
elements in the iterable, returning
2-tuples containing the count and
each element.

>>> for item in enumerate(['subject', 'verb', 'object']):
...   print(item)
(0, 'subject')
(1, 'verb')
(2, 'object')

enumerate() is often used when
looping through a list and
recording the indexes at which
certain conditions are met:

f = open('data.txt', 'r')
for i, line in enumerate(f):
  if line.strip() == '':
    print('Blank line at line #%i' % i)

sorted(iterable, key=None,
reverse=False) collects all the
elements of the iterable into a
list, sorts the list, and returns
the sorted result. The key, and
reverse arguments are passed
through to the constructed list's
sort() method.

>>> import random
>>> # Generate 8 random numbers between [0, 10000)
>>> rand_list = random.sample(range(10000), 8)
>>> rand_list
[769, 7953, 9828, 6431, 8442, 9878, 6213, 2207]
>>> sorted(rand_list)
[769, 2207, 6213, 6431, 7953, 8442, 9828, 9878]
>>> sorted(rand_list, reverse=True)
[9878, 9828, 8442, 7953, 6431, 6213, 2207, 769]

(For a more detailed discussion of
sorting, see the sortinghowto.)

The any(iter) and all(iter)
built-ins look at the truth values
of an iterable's contents. any()
returns True if any element in the
iterable is a true value, and all()
returns True if all of the elements
are true values:

>>> any([0,1,0])
True
>>> any([0,0,0])
False
>>> any([1,1,1])
True
>>> all([0,1,0])
False
>>> all([0,0,0])
False
>>> all([1,1,1])
True

  zip(iterA, iterB, ...) takes one
  element from each iterable and
  returns them in a tuple:

zip(['a', 'b', 'c'], (1, 2, 3)) =>
   ('a', 1), ('b', 2), ('c', 3)

It doesn't construct an in-memory
list and exhaust all the input
iterators before returning; instead
tuples are constructed and returned
only if they're requested. (The
technical term for this behaviour
is lazy evaluation.)

This iterator is intended to be
used with iterables that are all of
the same length. If the iterables
are of different lengths, the
resulting stream will be the same
length as the shortest iterable.

    evince -p xii ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 12]

zip(['a', 'b'], (1, 2, 3)) =>
   ('a', 1), ('b', 2)

You should avoid doing this,
though, because an element may be
taken from the longer iterators and
discarded. This means you can't go
on to use the iterators further
because you risk skipping a
discarded element.


6 The itertools module

The itertools module contains a
number of commonly-used iterators
as well as functions for combining
several iterators. This section
will introduce the module's
contents by showing small examples.

The module's functions fall into a
few broad classes:

- Functions that create a new
  iterator based on an existing
  iterator.
- Functions for treating an
  iterator's elements as function
  arguments.
- Functions for selecting portions
  of an iterator's output.
- A function for grouping an
  iterator's output.


6.1 Creating new iterators

itertools.count(n) returns an
infinite stream of integers,
increasing by 1 each time. You can
optionally supply the starting
number, which defaults to 0:

itertools.count() =>
  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
itertools.count(10) =>
  10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

itertools.cycle(iter) saves a copy
of the contents of a provided
iterable and returns a new iterator
that returns its elements from
first to last. The new iterator
will repeat these elements
infinitely.

itertools.cycle([1,2,3,4,5]) =>
  1, 2, 3, 4, 5, 1, 2, 3, 4, 5, ...

itertools.repeat(elem, [n]) returns
the provided element n times, or
returns the element endlessly if n
is not provided.

itertools.repeat('abc') =>
  abc, abc, abc, abc, abc, abc, abc, abc, abc, abc, ...
itertools.repeat('abc', 5) =>
  abc, abc, abc, abc, abc

itertools.chain(iterA, iterB, ...)
takes an arbitrary number of
iterables as input, and returns all
the elements of the first iterator,
then all the elements of the
second, and so on, until all of the
iterables have been exhausted.

itertools.chain(['a', 'b', 'c'], (1, 2, 3)) =>
  a, b, c, 1, 2, 3

itertools.islice(iter, [start],
stop, [step]) returns a stream
that's a slice of the iterator.
With a single stop argument, it
will return the first stop
elements. If you supply a starting
index, you'll get stop-start
elements, and if you supply a value
for step, elements will be skipped
accordingly. Unlike Python's string
and list slicing, you can't use
negative values for start, stop, or
step.

    evince -p xiii ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 13]

itertools.islice(range(10), 8) =>
  0, 1, 2, 3, 4, 5, 6, 7
itertools.islice(range(10), 2, 8) =>
  2, 3, 4, 5, 6, 7
itertools.islice(range(10), 2, 8, 2) =>
  2, 4, 6

itertools.tee(iter, [n]) replicates
an iterator; it returns n
independent iterators that will all
return the contents of the source
iterator. If you don't supply a
value for n, the default is 2.
Replicating iterators requires
saving some of the contents of the
source iterator, so this can
consume significant memory if the
iterator is large and one of the
new iterators is consumed more than
the others.

itertools.tee(itertools.count()) =>
  iterA, iterB

where iterA ->
  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

and

iterB ->
  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...


6.2 Calling functions on elements

The operator module contains a set
of functions corresponding to
Python's operators. Some examples
are operator.add(a, b) (adds two
values), operator.ne(a, b) (same as
a != b), and
operator.attrgetter('id') (returns
a callable that fetches the .id
attribute).

itertools.starmap(func, iter)
assumes that the iterable will
return a stream of tuples, and
calls func using these tuples as
the arguments:

itertools.starmap(os.path.join,
  [('/bin', 'python'), ('/usr', 'bin', 'java'),
   ('/usr', 'bin', 'perl'), ('/usr', 'bin', 'ruby')])
=>
  /bin/python, /usr/bin/java, /usr/bin/perl, /usr/bin/ruby


6.3 Selecting elements

Another group of functions chooses
a subset of an iterator's elements
based on a predicate.

itertools.filterfalse(predicate,
iter) is the opposite, returning
all elements for which the
predicate returns false:

itertools.filterfalse(is_even, itertools.count()) =>
  1, 3, 5, 7, 9, 11, 13, 15, ...

itertools.takewhile(predicate,
iter) returns elements for as long
as the predicate returns true. Once
the predicate returns false, the
iterator will signal the end of its
results.

def less_than_10(x):
  return x < 10

itertools.takewhile(less_than_10, itertools.count()) =>
  0, 1, 2, 3, 4, 5, 6, 7, 8, 9

    evince -p xiv ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 14]

itertools.takewhile(is_even, itertools.count()) =>
  0

itertools.dropwhile(predicate,
iter) discards elements while the
predicate returns true, and then
returns the rest of the iterable's
results.

itertools.dropwhile(less_than_10, itertools.count()) =>
  10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

itertools.dropwhile(is_even, itertools.count()) =>
  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...


6.4 Grouping elements

The last function I'll discuss,
itertools.groupby(iter,
key_func=None), is the most
complicated. key_func(elem) is a
function that can compute a key
value for each element returned by
the iterable. If you don't supply a
key function, the key is simply
each element itself.

groupby() collects all the
consecutive elements from the
underlying iterable that have the
same key value, and returns a
stream of 2-tuples containing a key
value and an iterator for the
elements with that key.

city_list = [('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL'),
             ('Anchorage', 'AK'), ('Nome', 'AK'),
             ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ'),
             ...
            ]

def get_state(city_state):
  return city_state[1]

itertools.groupby(city_list, get_state) =>
  ('AL', iterator-1),
  ('AK', iterator-2),
  ('AZ', iterator-3), ...

where

iterator-1 =>
  ('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL')
iterator-2 =>
  ('Anchorage', 'AK'), ('Nome', 'AK')
iterator-3 =>
  ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ')

groupby() assumes that the
underlying iterable's contents will
already be sorted based on the key.
Note that the returned iterators
also use the underlying iterable,
so you have to consume the results
of iterator-1 before requesting
iterator-2 and its corresponding
key.


7 The functools module

The functools module in Python 2.5
contains some higher-order
functions. A higher-order function
takes one or more functions as
input and returns a new function.
The most useful tool in this module
is the functools.partial()
function.

    evince -p xv ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 15]

For programs written in a
functional style, you'll sometimes
want to construct variants of
existing functions that have some
of the parameters filled in.
Consider a Python function f(a, b,
c); you may wish to create a new
function g(b, c) that's equivalent
to f(1, b, c); you're filling in a
value for one of f()'s parameters.
This is called "partial function
application".

The constructor for partial() takes
the arguments (function, arg1,
arg2, ..., kwarg1=value1,
kwarg2=value2). The resulting
object is callable, so you can just
call it to invoke function with the
filled-in arguments.

Here's a small but realistic example:

import functools

def log(message, subsystem):
  """Write the contents of 'message' to the specified subsystem."""
  print('%s: %s' % (subsystem, message))
  ...

server_log = functools.partial(log, subsystem='server')
server_log('Unable to open socket')

functools.reduce(func, iter,
[initial_value]) cumulatively
performs an operation on all the
iterable's elements and, therefore,
can't be applied to infinite
iterables. func must be a function
that takes two elements and returns
a single value. functools.reduce()
takes the first two elements A and
B returned by the iterator and
calculates func(A, B). It then
requests the third element, C,
calculates func(func(A, B), C),
combines this result with the
fourth element returned, and
continues until the iterable is
exhausted. If the iterable returns
no values at all, a TypeError
exception is raised. If the initial
value is supplied, it's used as a
starting point and
func(initial_value, A) is the first
calculation.

>>> import operator, functools
>>> functools.reduce(operator.concat, ['A', 'BB', 'C'])
'ABBC'
>>> functools.reduce(operator.concat, [])
Traceback (most recent call last):
  ...
TypeError: reduce() of empty sequence with no initial value
>>> functools.reduce(operator.mul, [1,2,3], 1)
6
>>> functools.reduce(operator.mul, [], 1)
1

If you use operator.add() with
functools.reduce(), you'll add up
all the elements of the iterable.
This case is so common that there's
a special built-in called sum() to
compute it:

>>> import functools
>>> functools.reduce(operator.add, [1,2,3,4], 0)
10
>>> sum([1,2,3,4])
10
>>> sum([])
0

For many uses of
functools.reduce(), though, it can
be clearer to just write the
obvious for loop:

import functools
# Instead of:
product = functools.reduce(operator.mul, [1,2,3], 1)

    evince -p xvi ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 16]

# You can write:
product = 1
for i in [1,2,3]:
  product *= i


7.1 The operator module

The operator module was mentioned
earlier. It contains a set of
functions corresponding to Python's
operators. These functions are
often useful in functional-style
code because they save you from
writing trivial functions that
perform a single operation.

Some of the functions in this
module are:

- Math operations:
  add(), sub(), mul(), floordiv(), abs(), ...
- Logical operations:
  not_(), truth().
- Bitwise operations:
  and_(), or_(), invert().
- Comparisons:
  eq(), ne(), lt(), le(), gt(), and ge().
- Object identity:
  is_(), is_not().

Consult the operator module's
documentation for a complete list.


8 Small functions and the lambda expression

When writing functional-style
programs, you'll often need little
functions that act as predicates or
that combine elements in some way.

If there's a Python built-in or a
module function that's suitable,
you don't need to define a new
function at all:

stripped_lines = [line.strip() for line in lines]
existing_files = filter(os.path.exists, file_list)

If the function you need doesn't
exist, you need to write it. One
way to write small functions is to
use the lambda statement. lambda
takes a number of parameters and an
expression combining these
parameters, and creates an
anonymous function that returns the
value of the expression:

adder = lambda x, y: x+y

print_assign = lambda name, value: name + '=' + str(value)

An alternative is to just use the
def statement and define a function
in the usual way:

def adder(x, y):
  return x + y

def print_assign(name, value):
  return name + '=' + str(value)

Which alternative is preferable?
That's a style question; my usual
course is to avoid using lambda.

One reason for my preference is
that lambda is quite limited in the
functions it can define. The result
has to be computable as a single
expression, which means you can't
have multiway if... elif... else
comparisons or try... except
statements. If you try to do too
much in a lambda statement, you'll
end up with an overly complicated
expression that's hard to read.
Quick, what's the following code
doing?

    evince -p xvii ~/Empire/Doks/Comp/lang/py/funct/pyfunct2.pdf &
      [-p 17]

import functools
total = functools.reduce(lambda a, b: (0, a[1] + b[1]), items)[1]

You can figure it out, but it takes
time to disentangle the expression
to figure out what's going on.
Using a short nested def statements
makes things a little bit better:

import functools
def combine(a, b):
  return 0, a[1] + b[1]

total = functools.reduce(combine, items)[1]

But it would be best of all if I
had simply used a for loop:

total = 0
for a, b in items:
  total += b

Or the sum() built-in and a
generator expression:

total = sum(b for a,b in items)

Many uses of functools.reduce() are
clearer when written as for loops.

TTT@NN@Fredrik Lundh once suggested
the following set of rules for
refactoring uses of lambda:

- 1. Write a lambda function.
- 2. Write a comment explaining
  what the heck that lambda does.
- 3. Study the comment for a while,
  and think of a name that captures
  the essence of the comment.
- 4. Convert the lambda to a def
  statement, using that name.
- 5. Remove the comment.

I really like these rules, but
you're free to disagree about
whether this lambda-free style is
better.



__pyfunc3

    evince -p 1 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Functional Programming with Python

Functional programming is a
paradigm where functions, not
objects or procedures, are used as
the fundamental building blocks of
a program.

While talking about "functional"
characteristics, we come across
host of terms, some familiar and
few not so familiar. Let's dig
deeper into them and dissect each
one of them. The things that we
hear are immutable data, first
class functions and tail call
optimisation. These are language
features that aid functional
programming. Next, we hear about
mapping, reducing, pipelining,
recursing, currying4 and the use of
higher order functions. These are
programming techniques used to
write functional code. The list
goes on... parallelization, lazy
evaluation6 and determinism. These
are advantageous properties of
functional programs.

An immutable piece of data is one
that cannot be changed. Some
languages, like Clojure, make all
values immutable by default. Any
"mutating" operations copy the
value, change it and pass back the
changed copy.

    evince -p 2 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

This eliminates bugs that arise
from a programmer's incomplete
model of the possible states their
program may enter.

Languages that support first class
functions allow functions to be
treated like any other value. This
means they can be created, passed
to functions, returned from
functions and stored inside data
structures.

Tail call optimization is a
programming language feature. Each
time a function recurses, a new
stack frame is created. A stack
frame is used to store the
arguments and local values for the
current function invocation. If a
function recurses a large number of
times, it is possible for the
interpreter or compiler to run out
of memory. Languages with tail call
optimisation reuse the same stack
frame for their entire sequence of
recursive calls. Languages like
Python that do not have tail call
optimization generally limit the
number of times a function may
recurse to some number in the
thousands.

Currying means decomposing a
function that takes multiple
arguments into a function that
takes the first argument and
returns a function that takes the
next argument, and so forth for all
the arguments.

    evince -p 3 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Functions are first class
(objects). That is, everything you
can do with "data" can be done with
functions themselves (such as
passing a function to another
function).

However, Functional code can be
characterized just by one thing:
the absence of side effects. It
doesn't rely on data outside the
current function, and it doesn't
change data that exists outside the
current function. Every other
"functional" thing can be derived
from this property.

Following is a piece of code
written in Python, which is
un-functional:

a = 0

def increment1():
  global a
    # forces the interpreter to
    # use the global variable, a
  a += 1

This is an example of functional
function:

def increment2(a):
  return a + 1

    evince -p 4 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Functions as Objects

Functions are first-class objects
in Python, meaning they have
attributes and can be referenced
and assigned to variables.

def i_am_an_object(myarg):
  '''I am a nice function.
     Please be my friend.'''
  return myarg ...

    evince -p 5 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

print(i_am_an_object(1))
1

an_object_by_any_other_name = i_am_an_object

print(an_object_by_any_other_name(2))
2

print(i_am_an_object)
<function i_am_an_object at 0x100432aa0>

print(an_object_by_any_other_name)

<function i_am_an_object at 0x100432aa0>
print(i_am_an_object.__doc__)

'I am a nice function.\n    Please be my friend.'

    evince -p 6 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Higher-Order Functions

Python supports higher-order
functions, meaning that functions
can accept other functions as
arguments and return functions to
the caller.

Map, reduce and filter, the
functions, which help us to work
with data structures rely on higher
order functions for their
operations. All the three functions
accept a function reference as an
argument to act upon each of the
elements of the list passed as the
second argument.

resultant_list = map(func, seq)

Here, func is a function whose
reference/pointer is passed onto
the function map.

Below is an example of higher order
function

def transformer (element):
  return element * 5

def my_map(funct,element_list)
  modified_list = []
  for element in element_list:
    modified_list.append(funct(element))
  return modified_list

seq = [1,2,3,4,5]
transformed_list = my_map(transformer,seq)

print(transformed_list)
[5,10,15,20,25]

    evince -p 7 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Anonymous Functions

Anonymous functions in Python are
created by using
the lambda statement. A lambda is
an inline function. This approach
is most commonly used when passing
a simple function as an argument to
another function. It takes any
number of arguments (including
optional arguments) and returns the
value of a single expression.
lambda functions can not contain
commands, and they can not contain
more than one expression.

The syntax consists of
the lambda keyword followed by a
list of arguments, a colon, and the
expression to evaluate and return.
The parameters of the lambda are
defined to the left of the colon.
The function body is defined to the
right of the colon. The result of
running the function body is
(implicitly) returned.

    evince -p 8 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

def f(x):
  return x * 2

print(f(3))
6

g = lambda x: x * 2  <1>

g(3)
6

(lambda x: x * 2)(3)
6

The lambda function can be used,
even without assigning it to a
variable. This may not be the most
useful thing in the world, but it
just goes to show that a lambda is
just an in-line function.

Another example:

processFunc = collapse and (lambda s: " ".join(s.split())) or (lambda s: s)

a lambda function is always true in
a boolean context. (That doesn't
mean that a lambda function can't
return a false value. The function
is always true; its return value
could be anything.)

    evince -p 9 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

In the above example, processFunc
is now a function, but which
function it is depends on the value
of the collapse variable. If
collapse is true,
processFunc(string) will collapse
whitespace; otherwise,
processFunc(string) will return its
argument unchanged.

    evince -p 10 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Nested Functions

Functions can be defined within the
scope of another function. If this
type of function definition is
used, the inner function is only in
scope inside the outer function, so
it is most often useful when the
inner function is being returned
(moving it to the outer scope) or
when it is being passed into
another function.

In the given example, a new
instance of the function inner() is
created on each call toouter().
That is because it is defined
during the execution of outer().
The creation of the second instance
has no impact on the first.

def outer(): ...
  def inner(a): ...
    return a ...
  return inner ...

f = outer()
print(f)
<function inner at 0x1004340c8>

print(f(10))
10

f2 = outer()
print(f2)
<function inner at 0x1004341b8>

print(f2(11))
11

    evince -p 12 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Closures

Essentially, a closure is a
function object that remembers
values in enclosing scopes
regardless of whether those scopes
are still present in memory.

A nested function has access to the
environment in which it was
defined. The definition occurs
during the execution of the outer
function. Therefore, it is possible
to return an inner function that
remembers the state of the outer
function, even after the outer
function has completed execution.
This model is referred to as a
closure.

    evince -p 13 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

def outer2(a): ...
  def inner2(b): ...
    return a + b ...
  return inner2 ...

add1 = outer2(1)

    evince -p 14 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

print(add1)
<function inner2 at 0x100519c80>

print(add1(4))
5

print(add1(5))
6

add2 = outer2(2)

print(add2)
<function inner2 at 0x100519cf8>

print(add2(4))
6

print(add2(5))
7

    evince -p 15 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Closures can be useful in the
following scenarios:

- Replacing hard-coded constants
- Eleminating globals
- Providing consistent function
  signatures
- Implementing Object Orientation

    evince -p 16 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

List comprehensions

List comprehensions is an easy way
of generating lists from a given
list. Consider the following code
to generate a list containing
numbers from 1 to 25:

    evince -p 17 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

alist=[]
for i in range(1, 26):
...  alist.append(i)
print(alist)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]

    evince -p 18 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

An alternative, functional approach
to the same would be as follows:

    evince -p 19 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

alist=[i for i in range(1, 26)]
print(alist)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]

    evince -p 20 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Notice that this code is more
intuitive and easy to understand
and becomes shorter as well!

However, while implementing a list
comprehension make sure to take
care of any exception scenario, as
the code is likely to throw
exceptions if the exception
scenarios are not accounted for.

Another example - how to strip
white-space from a list of strings:

    evince -p 21 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

alist=[' This is line 1  \n', 'This is line 2  \n', '', '  This is line 3\n']
[line.strip() for line in alist]
['This is line 1', 'This is line 2', '', 'This is line 3']

    evince -p 22 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Here's a modification of the above,
which tests if a string is a null
string, and doesn't print if that
is the case.

    evince -p 23 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

[line.strip() for line in alist
...  if line != '']

['This is line 1', 'This is line 2', 'This is line 3']

The actual syntax of list
comprehension is as follows:

[expression for item in sequence if condition]

This can also be remembered in the
following way:

Resultant_list = [transform  iteration  filter]

The equivalent Python code for the
above is shown below:

for item in sequence:
  if (condition):
    expression

    evince -p 24 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Loop of Loops:

It's worth mentioning that you can
use list comprehensions to iterate
on more than one list. For example:

list_a = ['A', 'B']
list_b = [1, 2]
[(x, y) for x in list_a for y in list_b]
[('A', 1), ('A', 2), ('B', 1), ('B', 2)]

Here, the first list is the outer
loop and the last loop is the inner
loop. Also note that this method
returns a list of tuples. If you'd
like nested lists, you can also
nest one list comprehension within
another.

list_a = ['A', 'B']
list_b = ['C', 'D']
[[x + y for x in list_a] for y in list_b]
[['AC', 'BC'], ['AD', 'BD']]

Here, the latter list is executed
first, which means that it is the
outer loop and then the first loop
is executed, making it the inner
loop.

    evince -p 25 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Dictionary comprehension

Dictionary comprehensions are an
easy way to generate a dictionary
by processing a given dictionary.

My_dict = {'a':5, 'b':10, 'c':15, 'd':20, 'e':25, 'f':30}

another_dict = {k:v * 2 for k:v in my_dict}

print("another_dict = %s " %(another_dict))

another_dict = {'a':10, 'b':20, 'c':30, 'd':40, 'e':50, 'f':60}

    evince -p 26 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Set Comprehensions

Set comprehensions allow sets to be
constructed using the same
principles as list comprehensions,
the only difference is that
resulting sequence is a set.

Say we have a list of names. The
list can contain names,which only
differ in the case used to
represent them, duplicates and
names consisting of only one
character. We are only interested
in names longer then one character
and wish to represent all names in
the same format: The first letter
should be capitalised, all other
characters should be lower case.

Given the list:

names = ['Bob', 'JOHN', 'alice', 'bob', 'ALICE', 'J', 'Bob']

We require the set:

{'Bob', 'John', 'Alice'}

Note the new syntax for denoting a
set. Members are enclosed in curly
braces.

    evince -p 27 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

The following set comprehension
accomplishes this:

{name[0].upper() + name[1:].lower() for name in names if len(name) > 1}


Map

Map takes a function and a
collection of items. It makes a
new, empty collection, runs the
function on each item in the
original collection and inserts
each return value into the new
collection. It returns the new
collection.

Map is thus used to create a new
collection out of a collection
after processing each item of the
collection using a given function.

r = map(func, seq)

The first argument func is the name
of a function and the second a
sequence (e.g. a list) seq. map()
applies the function func to all
the elements of the sequence seq.
It returns a new list with the
elements changed by func

    evince -p 28 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Example:

def fahrenheit(T):
  return ((float(9) / 5) * T + 32)

def celsius(T):
  return (float(5) / 9) * (T - 32)

temp = (36.5, 37, 37.5,39)
F = map(fahrenheit, temp)
C = map(celsius, F)

This is a simple map that takes a
list of names and returns a list of
the lengths of those names:

name_lengths = map(len, ["Mary", "Isla", "Sam"])
print name_lengths
  # => [4, 4, 3]

This is a map that squares every
number in the passed collection:

squares = map(lambda x: x * x, [0, 1, 2, 3, 4])
print squares
  # => [0, 1, 4, 9, 16]

    evince -p 29 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

Reduce

Reduce takes a function and a
collection of items. It returns a
value that is created by combining
the items. Thus, Reduce is used to
aggregate(or reduce) a particular
collection into a value by
processing each item of the given
collection with a particular
function.

This is a simple reduce. It returns
the sum of all the items in the
collection.

sum = reduce(lambda a, x: a + x, [0, 1, 2, 3, 4])
print sum
  # => 10

x is the current item being
iterated over. a is the
accumulator. It is the value
returned by the execution of the
lambda on the previous
item. reduce() walks through the
items. For each one, it runs the
lambda on the current a and x and
returns the result as the a of the
next iteration.

    evince -p 30 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

What is a in the first iteration?
There is no previous iteration
result for it to pass
along. reduce() uses the first item
in the collection for a in the
first iteration and starts
iterating at the second item. That
is, the first x is the second item.


Filter

As the name
suggests, filter extracts each
element in the sequence for which
the function returns True.

The signature of the Filter
function:

modified_list = filter(func,list)

It takes a function, func as its
first argument. func returns a
Boolean value, i.e. either True or
False. This function will be
applied to every element of the
list list.

Only if func returns True will the
element of the list be included in
the resultant modified_list.

    evince -p 31 ~/Empire/Doks/Comp/lang/py/funct/pyfunc3.pdf &

fib = [0,1,1,2,3,5,8,13,21,34,55]
result = filter(lambda x: x % 2, fib)
print result
[1, 1, 3, 5, 13, 21, 55]

result = filter(lambda x: x % 2 == 0, fib)
print result
[0, 2, 8, 34]



__abcode_pyfiltering

#!/usr/bin/env python3
# ~/Dropbox/transcr/abcode/abcoding-0*.jpg
# used modules: sys, io, argparse

import sys

frequency_filename = sys.argv[1]
short_list_filename = sys.argv[2]

def get_words_map():
  short_words = []
  with open(short_list_filename) as short_list:
    short_words = map(lambda l: l.strip(), short_list.readlines())
  return short_words

def get_words_iterate():
  short_words = []
  with open(short_list_filename) as short_list:
    while True:
      short_word = short_list.readline().strip()
      if not short_word:
        break
      short_words.append(short_word)
  return short_words

short_words = get_words_iterate()

with open(frequency_filename) as frequency_list:
  while True:
    line = frequency_list.readline()
    if not line:
      break
    (word, frequency) = line.strip().split()
    tag_ext = " ___short" if word in short_words else ""
    print(f'{word} {frequency}{tag_ext}')


#!/usr/bin/env python3

import io
import sys

frequency_filename = sys.argv[1]
short_list_filename = sys.argv[2]

def get_words_map(short_list: io.IOBase):
  short_words = []
  with open(short_list_filename) as short_list:
    short_words = map(lambda l: l.strip(), short_list.readlines())
  return short_words

def get_words_iterate():
  short_words = []
  with open(short_list_filename) as short_list:
    while True:
      short_word = short_list.readline().strip()
      if not short_word:
        break
      short_words.append(short_word)
  return short_words

short_words = get_words_iterate()

with open(frequency_filename) as frequency_list:
  while True:
    line = frequency_list.readline()
    if not line:
      break
    (word, frequency) = line.strip().split()
    tag_ext = " ___short" if word in short_words else ""
    print(f'{word} {frequency}{tag_ext}')


#!/usr/bin/env python3

import io
import sys

frequency_filename = sys.argv[1]
short_list_filename = sys.argv[2]

def get_words_map(short_list: io.IOBase):
  short_words = []
  short_words = map(lambda l: l.strip(), short_list.readlines())
  return short_words

def get_words_iterate(short_list: io.IOBase):
  short_words = []
  while True:
    short_word = short_list.readline().strip()
    if not short_word:
      break
    short_words.append(short_word)
  return short_words

with open(short_list_filename) as short_list:
  short_words = get_words_iterate(short_list)

with open(frequency_filename) as frequency_list:
  while True:
    line = frequency_list.readline()
    if not line:
        break
    (word, frequency) = line.strip().split()
    tag_ext = " ___short" if word in short_words else ""
    print(f'{word} {frequency}{tag_ext}')


#!/usr/bin/env python3

import io
import sys

frequency_filename = sys.argv[1]
short_list_filename = sys.argv[2]

def get_words_map(short_list: io.IOBase):
  short_words = []
  short_words = map(lambda l: l.strip(), short_list.readlines())
  return short_words

def get_words_iterate(short_list: io.IOBase):
  short_words = []
  while True:
    short_word = short_list.readline().strip()
    if not short_word:
      break
    short_words.append(short_word)
  return short_words

def print_ref_with_tags(frequency_list: io.IOBase, short_list: list[str]):
  while True:
    line = frequency_list.readline()
    if not line:
      break
    (word, frequency) = line.strip().split()
    tag_ext =" ___short" if word in short_words else ""
    print(f'{word} {frequency}{tag_ext}')


with open(short_list_filename) as short_list:
  short_words = get_words_iterate(short_list)

with open(frequency_filename) as frequency_list:
  print_ref_with_tags(frequency_list, short_list)


#!/usr/bin/env python3

import argparse
import io
import sys

def get_words_map(short_list: io.IOBase):
  short_words = []
  short_words = map(lambda l: l.strip(), short_list.readlines())
  return short_words

def get_words_iterate(short_list: io.IOBase):
  short_words = []
  while True:
    short_word = short_list.readline().strip()
    if not short_word:
      break
    short_words.append(short_word)
  return short_words

def print_ref_with_tags(frequency_list: io.IOBase, short_words: list[str]):
  while True:
    line = frequency_list.readline()
    if not line:
      break
    (word, frequency) = line.strip().split()
    tag_ext = " ___short" if word in short_words else ""
    print(f'{word} {frequency}{tag_ext}')

def main():
  parser = argparse.ArgumentParser()
  parser.add_argument('-f', '--frequency', type=argparse.FileType('r', encoding='UTF-8'), required=True)
  parser.add_argument('-s', '--short-list', type=argparse.FileType('r', encoding='UTF-8'), required=True)
  args = parser.parse_args()

  short_words = get_words_iterate(args.short_list)

  print_ref_with_tags(args.frequency, short_words)

if __name__ == '__main__':
    main()

# kolla på sys.stdout
# kolla på sys.stderr
# ----
# formatsträngar: f" "  = använd inte .format
#   = dessa är inte kompatibla med varandra


__JupyterCb

    evince -p 419 ~/Empire/Doks/Comp/lang/py/datasci/jup/jupyter-cb_2018.pdf &

How to do it...

___Functional
We can use this script to see the
word counts for a file:

import pyspark

if not 'sc' in globals():
  sc = pyspark.SparkContext()

text_file = sc.textFile("B09656_09_word_count.ipynb")
counts = text_file.flatMap(lambda line: line.split(" ")) \
  .map(lambda word: (word, 1)) \
  .reduceByKey(lambda a, b: a + b)

for x in counts.collect():
  print(x)


    evince -p 424 ~/Empire/Doks/Comp/lang/py/datasci/jup/jupyter-cb_2018.pdf &

How to do it...

We can slightly modify the previous script to produce a sorted listed
as follows:

import pyspark

if not 'sc' in globals():
  sc = pyspark.SparkContext()

text_file = sc.textFile("B09656_09_word_count.ipynb")
sorted_counts = text_file.flatMap(lambda line: line.split(" ")) \
  .map(lambda word: (word, 1)) \
  .reduceByKey(lambda a, b: a + b) \
  .sortByKey()

for x in sorted_counts.collect():
  print(x)


    evince -p 437 ~/Empire/Doks/Comp/lang/py/datasci/jup/jupyter-cb_2018.pdf &

How to do it...

For this example, I am using text from an online article from Atlantic
Monthly called The World Might Be Better Off Without College for
Everyone at https://www.theatlantic.com/magazine/archive/2018/01/whats-college-good-for/546590/.

I am using this script:

import pyspark
if not 'sc' in globals():
  sc = pyspark.SparkContext()

sentences = sc.textFile('B09656_09_article.txt') \
  .glom() \
  .map(lambda x: " ".join(x)) \
  .flatMap(lambda x: x.split("."))
print(sentences.count(),"sentences")

bigrams = sentences.map(lambda x:x.split()) \
  .flatMap(lambda x: [((x[i],x[i+1]),1) for i in
range(0,len(x)-1)])
print(bigrams.count(),"bigrams")

frequent_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \
  .map(lambda x:(x[1],x[0])) \
  .sortByKey(False)
frequent_bigrams.take(10)

This produces this output when executed:

Out[1]: [(11, ('of', 'the')),
         (8, ...

[+MORE EXAMPLES, MAYBE FROM BEFORE EXAMPLES ABOVE]



___PyStdLib3_ByEx=Funct
    evince -p 143 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 181]

Chapter 3 - Algorithms


Python includes several modules for
implementing algorithms elegantly
and concisely using whatever style
is most appropriate for the task.
It supports purely procedural,
objectoriented, and functional
styles, and all three styles are
frequently mixed within different
parts of the same program.

functools (page 143) includes
functions for creating function
decorators, enabling
aspect-oriented programming and
code reuse beyond what a
traditional object-oriented
approach supports. It also provides
a class decorator for implementing
all of the rich comparison APIs
using a shortcut, and partial
objects for creating references to
functions with their arguments
included.

The itertools (page 163) module
includes functions for creating and
working with iterators and
generators used in functional
programming. The operator (page
183) module eliminates the need for
many trivial lambda functions when
using a functional programming
style by providing function-based
interfaces to built-in operations
such as arithmetic or item lookup.

No matter which style is used in a
program, contextlib (page 191)
makes resource management easier,
more reliable, and more concise.
Combining context managers and the
with statement reduces the number
of try:finally blocks and
indentation levels needed, while
ensuring that files, sockets,
database transactions, and other
resources are closed and released
at the right time.


3.1 functools: Tools for Manipulating Functions

The functools module provides tools
for adapting or extending functions
and other callable objects, without
completely rewriting them.


3.1.1 Decorators

The primary tool supplied by the
functools module is the class
partial, which can be used to
"wrap" a callable object with
default arguments. The resulting
object is itself callable and can
be treated as though it is the
original function. It takes all of
the same arguments as the original,
and can be invoked with extra
positional or named arguments as
well. A partial can be used instead
of a lambda to provide default
arguments to a function, while
leaving some arguments unspecified.

    evince -p 144 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 182]

3.1.1.1 Partial Objects

The first example shows two simple
partial objects for the function
myfunc(). The output of
show_details() includes the func,
args, and keywords attributes of
the partial object.

Listing 3.1: functools_partial.py

import functools

def myfunc(a, b=2):
  "Docstring for myfunc()."
  print('called myfunc with:', (a, b))

def show_details(name, f, is_partial=False):
  "Show details of a callable object."
  print('{}:'.format(name))
  print('object:', f)
  if not is_partial:
    print('__name__:', f.__name__)
  if is_partial:
    print('func:', f.func)
    print('args:', f.args)
    print('keywords:', f.keywords)
  return

show_details('myfunc', myfunc)
myfunc('a', 3)
print()

# Set a different default value for 'b', but require
# the caller to provide 'a'.
p1 = functools.partial(myfunc, b=4)
show_details('partial with named default', p1, True)
p1('passing a')
p1('override b', b=5)
print()

# Set default values for both 'a' and 'b'.
p2 = functools.partial(myfunc, 'default a', b=99)
show_details('partial with defaults', p2, True)
p2()
p2(b='override b')
print()

print('Insufficient arguments:')
p1()

    evince -p 145 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 183]

At the end of the example, the
first partial created is invoked
without passing a value for a,
causing an exception.

$ python3 functools_partial.py

myfunc:
  object: <function myfunc at 0x1007a6a60>
    __name__: myfunc
  called myfunc with: ('a', 3)

partial with named default:
  object: functools.partial(<function myfunc at 0x1007a6a60>, b=4)
  func: <function myfunc at 0x1007a6a60>
  args: ()
  keywords: {'b': 4}
  called myfunc with: ('passing a', 4)
  called myfunc with: ('override b', 5)

partial with defaults:
  object: functools.partial(<function myfunc at 0x1007a6a60>, 'default a', b=99)
  func: <function myfunc at 0x1007a6a60>
  args: ('default a',)
  keywords: {'b': 99}
  called myfunc with: ('default a', 99)
  called myfunc with: ('default a', 'override b')

Insufficient arguments:
Traceback (most recent call last):
  File "functools_partial.py", line 51, in <module>
    p1()
TypeError: myfunc() missing 1 required positional argument: 'a'


3.1.1.2 Acquiring Function Properties

The partial object does not have
__name__ or __doc__ attributes by
default, and without those
attributes, decorated functions are
more difficult to debug.
update_wrapper() can be used to
copy or add attributes from the
original function to the partial
object.

    evince -p 146 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 184]

Listing 3.2: functools_update_wrapper.py

import functools

def myfunc(a, b=2):
  "Docstring for myfunc()."
  print('called myfunc with:', (a, b))

def show_details(name, f):
  "Show details of a callable object."
  print('{}:'.format(name))
  print('object:', f)
  print('__name__:', end=' ')
  try:
    print(f.__name__)
  except AttributeError:
    print('(no __name__)')
  print('__doc__', repr(f.__doc__))
  print()

show_details('myfunc', myfunc)

p1 = functools.partial(myfunc, b=4)
show_details('raw wrapper', p1)

print('Updating wrapper:')
print('assign:', functools.WRAPPER_ASSIGNMENTS)
print('update:', functools.WRAPPER_UPDATES)
print()

functools.update_wrapper(p1, myfunc)
show_details('updated wrapper', p1)

The attributes added to the wrapper
are defined in WRAPPER_ASSIGNMENTS,
while WRAPPER_UPDATES lists values
to be modified.

$ python3 functools_update_wrapper.py

myfunc:
  object: <function myfunc at 0x1018a6a60>
  __name__: myfunc
  __doc__ 'Docstring for myfunc().'

raw wrapper:
  object: functools.partial(<function myfunc at 0x1018a6a60>, b=4)
  __name__: (no __name__)
  __doc__ 'partial(func, *args, **keywords) - new function with partial application\n   of the given arguments and keywords.\n'

Updating wrapper:
  assign: ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')
  update: ('__dict__',)
  updated wrapper:
    object: functools.partial(<function myfunc at 0x1018a6a60>, b=4)
    __name__: myfunc
    __doc__ 'Docstring for myfunc().'

    evince -p 147 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 185]

3.1.1.3 Other Callables

Partials work with any callable
object, not just with stand-alone
functions.

Listing 3.3: functools_callable.py

import functools

class MyClass:
  "Demonstration class for functools"

  def __call__(self, e, f=6):
    "Docstring for MyClass.__call__"
    print('called object with:', (self, e, f))

def show_details(name, f):
  "Show details of a callable object."
  print('{}:'.format(name))
  print('object:', f)
  print('__name__:', end=' ')
  try:
    print(f.__name__)
  except AttributeError:
    print('(no __name__)')
  print('__doc__', repr(f.__doc__))
  return

o = MyClass()

show_details('instance', o)
o('e goes here')
print()

p = functools.partial(o, e='default for e', f=8)
functools.update_wrapper(p, o)
show_details('instance wrapper', p)
p()

    evince -p 148 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 186]

This example creates partials from
an instance of a class with a
__call__() method.

$ python3 functools_callable.py

instance:
  object: <__main__.MyClass object at 0x1011b1cf8>
  __name__: (no __name__)
  __doc__ 'Demonstration class for functools'
  called object with: (<__main__.MyClass object at 0x1011b1cf8>, 'e goes here', 6)

instance wrapper:
  object: functools.partial(<__main__.MyClass object at 0x1011b1cf8>, f=8, e='default for e')
  __name__: (no __name__)
  __doc__ 'Demonstration class for functools'
  called object with: (<__main__.MyClass object at 0x1011b1cf8>, 'default for e', 8)


3.1.1.4 Methods and Functions

While partial() returns a callable
ready to be used directly,
partialmethod() returns a callable
ready to be used as an unbound
method of an object. In the
following example, the same
stand-alone function is added as an
attribute of MyClass twice, once
using partialmethod() as method1()
and again using partial() as
method2().

Listing 3.4: functools_partialmethod.py

import functools

def standalone(self, a=1, b=2):
  "Standalone function"
  print('called standalone with:', (self, a, b))
  if self is not None:
    print('self.attr =', self.attr)

class MyClass:
  "Demonstration class for functools"

  def __init__(self):
    self.attr = 'instance attribute'

  method1 = functools.partialmethod(standalone)
  method2 = functools.partial(standalone)

o = MyClass()
print('standalone')
standalone(None)
print()

print('method1 as partialmethod')
o.method1()
print()

print('method2 as partial')
try:
  o.method2()
except TypeError as err:
  print('ERROR: {}'.format(err))

    evince -p 149 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 187]

method1() can be called from an
instance of MyClass, and the
instance is passed as the first
argument, just as with methods that
are defined in the usual way.
method2() is not set up as a bound
method, so the self argument must
be passed explicitly; otherwise,
the call will result in a
TypeError.

$ python3 functools_partialmethod.py

standalone
  called standalone with: (None, 1, 2)

method1 as partialmethod
  called standalone with: (<__main__.MyClass object at 0x1007b1d30>, 1, 2)
  self.attr = instance attribute

method2 as partial
ERROR: standalone() missing 1 required positional argument:
'self'


3.1.1.5 Acquiring Function Properties for Decorators

Updating the properties of a
wrapped callable is especially
useful for decorators, because the
transformed function ends up with
properties of the original "bare"
function.

    evince -p 150 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 188]

Listing 3.5: functools_wraps.py

import functools

def show_details(name, f):
  "Show details of a callable object."
  print('{}:'.format(name))
  print('object:', f)
  print('__name__:', end=' ')
  try:
    print(f.__name__)
  except AttributeError:
    print('(no __name__)')
  print('__doc__', repr(f.__doc__))
  print()

def simple_decorator(f):
  @functools.wraps(f)
  def decorated(a='decorated defaults', b=1):
    print('decorated:', (a, b))
    print('', end=' ')
    return f(a, b=b)
  return decorated

def myfunc(a, b=2):
  "myfunc() is not complicated"
  print('myfunc:', (a, b))
  return

# The raw function
show_details('myfunc', myfunc)
myfunc('unwrapped, default b')
myfunc('unwrapped, passing b', 3)
print()

# Wrap explicitly.
wrapped_myfunc = simple_decorator(myfunc)
show_details('wrapped_myfunc', wrapped_myfunc)
wrapped_myfunc()
wrapped_myfunc('args to wrapped', 4)
print()

# Wrap with decorator syntax.
@simple_decorator
def decorated_myfunc(a, b):
  myfunc(a, b)
  return

show_details('decorated_myfunc', decorated_myfunc)
decorated_myfunc()
decorated_myfunc('args to decorated', 4)

functools provides a decorator,
wraps(), that applies
update_wrapper() to the decorated
function.

    evince -p 151 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 189]

$ python3 functools_wraps.py

myfunc:
  object: <function myfunc at 0x101241b70>
  __name__: myfunc
  __doc__ 'myfunc() is not complicated'

  myfunc: ('unwrapped, default b', 2)
  myfunc: ('unwrapped, passing b', 3)

wrapped_myfunc:
  object: <function myfunc at 0x1012e62f0>
  __name__: myfunc
  __doc__ 'myfunc() is not complicated'

  decorated: ('decorated defaults', 1)
     myfunc: ('decorated defaults', 1)
  decorated: ('args to wrapped', 4)
     myfunc: ('args to wrapped', 4)

decorated_myfunc:
  object: <function decorated_myfunc at 0x1012e6400>
  __name__: decorated_myfunc
  __doc__ None

  decorated: ('decorated defaults', 1)
     myfunc: ('decorated defaults', 1)
  decorated: ('args to decorated', 4)
     myfunc: ('args to decorated', 4)


3.1.2 Comparison

Under Python 2, classes could
define a __cmp__() method that
returns -1, 0, or 1 based on
whether the object is less than,
equal to, or greater than,
respectively, the item being
compared. Python 2.1 introduced the
rich comparison methods API
(__lt__(), __le__(), __eq__(),
__ne__(), __gt__(), and __ge__()),
which perform a single comparison
operation and return a boolean
value. Python 3 deprecated
__cmp__() in favor of these new
methods, and functools provides
tools to make it easier to write
classes that comply with the new
comparison requirements in Python
3.


3.1.2.1 Rich Comparison

The rich comparison API is designed
to allow classes with complex
comparisons to implement each test
in the most efficient way possible.
However, for classes where
comparison is relatively simple,
there is no point in manually
creating each of the rich
comparison methods. The
total_ordering() class decorator
takes a class that provides some of
the methods, and adds the rest of
them.

    evince -p 152 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 190]

Listing 3.6: functools_total_ordering.py

import functools
import inspect
from pprint import pprint

@functools.total_ordering
class MyObject:

  def __init__(self, val):
    self.val = val

  def __eq__(self, other):
    print('testing __eq__({}, {})'.format(
      self.val, other.val))
    return self.val == other.val

  def __gt__(self, other):
    print('testing __gt__({}, {})'.format(
      self.val, other.val))
    return self.val > other.val

print('Methods:\n')
pprint(inspect.getmembers(MyObject, inspect.isfunction))

a = MyObject(1)
b = MyObject(2)

print('\nComparisons:')
for expr in ['a < b', 'a <= b', 'a == b', 'a >= b', 'a > b']:
  print('\n{:<6}:'.format(expr))
  result = eval(expr)
  print('result of {}: {}'.format(expr, result))

The class must provide
implementation of __eq__() and one
other rich comparison method. The
decorator adds implementations of
the rest of the methods that work
by using the comparisons provided.
If a comparison cannot be made, the
method should return NotImplemented
so the comparison can be tried
using the reverse comparison
operators on the other object,
before failing entirely.

$ python3 functools_total_ordering.py

Methods:
[('__eq__', <function MyObject.__eq__ at 0x10139a488>),
 ('__ge__', <function _ge_from_gt at 0x1012e2510>),
 ('__gt__', <function MyObject.__gt__ at 0x10139a510>),
 ('__init__', <function MyObject.__init__ at 0x10139a400>),
 ('__le__', <function _le_from_gt at 0x1012e2598>),
 ('__lt__', <function _lt_from_gt at 0x1012e2488>)]

    evince -p 153 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 191]

Comparisons:

a < b:
  testing __gt__(1, 2)
  testing __eq__(1, 2)
  result of a < b: True

a <= b:
  testing __gt__(1, 2)
  result of a <= b: True

a == b:
  testing __eq__(1, 2)
  result of a == b: False

a >= b:
  testing __gt__(1, 2)
  testing __eq__(1, 2)
  result of a >= b: False

a > b:
  testing __gt__(1, 2)
  result of a > b: False


3.1.2.2 Collation Order

Since old-style comparison
functions are deprecated in Python
3, the cmp argument to functions
like sort() is also no longer
supported. Older programs that use
comparison functions can use
cmp_to_key() to convert them to a
function that returns a collation
key, which is used to determine the
position in the final sequence.

Listing 3.7: functools_cmp_to_key.py

import functools

class MyObject:
def __init__(self, val):
self.val = val
def __str__(self):
return 'MyObject({})'.format(self.val)

    evince -p 154 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 192]

def compare_obj(a, b):
  """Old-style comparison function.
  """
  print('comparing {} and {}'.format(a, b))
  if a.val < b.val:
    return -1
  elif a.val > b.val:
    return 1
  return 0

# Make a key function using cmp_to_key().
get_key = functools.cmp_to_key(compare_obj)

def get_key_wrapper(o):
  "Wrapper function for get_key to allow for print statements."
  new_key = get_key(o)
  print('key_wrapper({}) -> {!r}'.format(o, new_key))
  return new_key

objs = [MyObject(x) for x in range(5, 0, -1)]

for o in sorted(objs, key=get_key_wrapper):
  print(o)

Normally cmp_to_key() would be used
directly, but in this example an
extra wrapper function is
introduced to print out more
information as the key function is
being called.

The output shows that sorted()
starts by calling get_key_wrapper()
for each item in the sequence to
produce a key. The keys returned by
cmp_to_key() are instances of a
class defined in functools that
implements the rich comparison API
using the old-style comparison
function passed in. After all of
the keys are created, the sequence
is sorted by comparing the keys.

$ python3 functools_cmp_to_key.py

key_wrapper(MyObject(5)) -> <functools.KeyWrapper object at 0x1011c5530>
key_wrapper(MyObject(4)) -> <functools.KeyWrapper object at 0x1011c5510>
key_wrapper(MyObject(3)) -> <functools.KeyWrapper object at 0x1011c54f0>
key_wrapper(MyObject(2)) -> <functools.KeyWrapper object at 0x1011c5390>
key_wrapper(MyObject(1)) -> <functools.KeyWrapper object at 0x1011c5710>
comparing MyObject(4) and MyObject(5)
comparing MyObject(3) and MyObject(4)
comparing MyObject(2) and MyObject(3)
comparing MyObject(1) and MyObject(2)
MyObject(1)
MyObject(2)
MyObject(3)
MyObject(4)
MyObject(5)

    evince -p 155 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 193]

3.1.3 Caching

The lru_cache() decorator wraps a
function in a "least recently used"
cache. Arguments to the function
are used to build a hash key, which
is then mapped to the result.
Subsequent calls with the same
arguments will fetch the value from
the cache instead of calling the
function. The decorator also adds
methods to the function to examine
the state of the cache
(cache_info()) and empty the cache
(cache_clear()).

Listing 3.8: functools_lru_cache.py

import functools

@functools.lru_cache()
def expensive(a, b):
  print('expensive({}, {})'.format(a, b))
  return a * b

MAX = 2

print('First set of calls:')
for i in range(MAX):
  for j in range(MAX):
    expensive(i, j)
print(expensive.cache_info())

print('\nSecond set of calls:')
for i in range(MAX + 1):
  for j in range(MAX + 1):
    expensive(i, j)
print(expensive.cache_info())

print('\nClearing cache:')
expensive.cache_clear()
print(expensive.cache_info())

print('\nThird set of calls:')
for i in range(MAX):
  for j in range(MAX):
    expensive(i, j)
print(expensive.cache_info())

    evince -p 156 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 194]

This example makes several calls to
expensive() in a set of nested
loops. The second time those calls
are made with the same values, the
results appear in the cache. When
the cache is cleared and the loops
are run again, the values must be
recomputed.

$ python3 functools_lru_cache.py

First set of calls:
expensive(0, 0)
expensive(0, 1)
expensive(1, 0)
expensive(1, 1)
CacheInfo(hits=0, misses=4, maxsize=128, currsize=4)

Second set of calls:
expensive(0, 2)
expensive(1, 2)
expensive(2, 0)
expensive(2, 1)
expensive(2, 2)
CacheInfo(hits=4, misses=9, maxsize=128, currsize=9)

Clearing cache:
CacheInfo(hits=0, misses=0, maxsize=128, currsize=0)

Third set of calls:
expensive(0, 0)
expensive(0, 1)
expensive(1, 0)
expensive(1, 1)
CacheInfo(hits=0, misses=4, maxsize=128, currsize=4)

To prevent the cache from growing
without bounds in a long-running
process, it is given a maximum
size. The default is 128 entries,
but that size can be changed for
each cache using the maxsize
argument.

Listing 3.9: functools_lru_cache_expire.py

import functools

@functools.lru_cache(maxsize=2)
def expensive(a, b):
  print('called expensive({}, {})'.format(a, b))
  return a * b

def make_call(a, b):
  print('({}, {})'.format(a, b), end=' ')
  pre_hits = expensive.cache_info().hits
  expensive(a, b)
  post_hits = expensive.cache_info().hits
  if post_hits > pre_hits:
    print('cache hit')

print('Establish the cache')
make_call(1, 2)
make_call(2, 3)

print('\nUse cached items')
make_call(1, 2)
make_call(2, 3)

print('\nCompute a new value, triggering cache expiration')
make_call(3, 4)

print('\nCache still contains one old item')
make_call(2, 3)

print('\nOldest item needs to be recomputed')
make_call(1, 2)

    evince -p 157 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 195]

In this example, the cache size is
set to 2 entries. When the third
set of unique arguments (3,4) is
used, the oldest item in the cache
is dropped and replaced with the
new result.

$ python3 functools_lru_cache_expire.py

Establish the cache
(1, 2) called expensive(1, 2)
(2, 3) called expensive(2, 3)

Use cached items
(1, 2) cache hit
(2, 3) cache hit

Compute a new value, triggering cache expiration
(3, 4) called expensive(3, 4)

Cache still contains one old item
(2, 3) cache hit

Oldest item needs to be recomputed
(1, 2) called expensive(1, 2)

The keys for the cache managed by
lru_cache() must be hashable, so
all of the arguments to the
function wrapped with the cache
lookup must be hashable.

    evince -p 158 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 196]

Listing 3.10: functools_lru_cache_arguments.py

import functools

@functools.lru_cache(maxsize=2)
def expensive(a, b):
  print('called expensive({}, {})'.format(a, b))
  return a * b

def make_call(a, b):
  print('({}, {})'.format(a, b), end=' ')
  pre_hits = expensive.cache_info().hits
  expensive(a, b)
  post_hits = expensive.cache_info().hits
  if post_hits > pre_hits:
    print('cache hit')

make_call(1, 2)

try:
  make_call([1], 2)
except TypeError as err:
  print('ERROR: {}'.format(err))

try:
  make_call(1, {'2': 'two'})
except TypeError as err:
  print('ERROR: {}'.format(err))

If an object that cannot be hashed
is passed in to the function, a
TypeError is raised.

$ python3 functools_lru_cache_arguments.py

(1, 2) called expensive(1, 2)
([1], 2) ERROR: unhashable type: 'list'
(1, {'2': 'two'}) ERROR: unhashable type: 'dict'


3.1.4 Reducing a Data Set

The reduce() function takes a
callable and a sequence of data as
input. It produces a single value
as output based on invoking the
callable with the values from the
sequence and accumulating the
resulting output.

    evince -p 159 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 197]

Listing 3.11: functools_reduce.py

import functools

def do_reduce(a, b):
  print('do_reduce({}, {})'.format(a, b))
  return a + b

data = range(1, 5)
print(data)
result = functools.reduce(do_reduce, data)
print('result: {}'.format(result))

This example adds up the numbers in
the input sequence.

$ python3 functools_reduce.py

range(1, 5)
do_reduce(1, 2)
do_reduce(3, 3)
do_reduce(6, 4)
result: 10

The optional initializer argument
is placed at the front of the
sequence and processed along with
the other items. This can be used
to update a previously computed
value with new inputs.

Listing 3.12: functools_reduce_initializer.py

import functools

def do_reduce(a, b):
  print('do_reduce({}, {})'.format(a, b))
  return a + b

data = range(1, 5)
print(data)
result = functools.reduce(do_reduce, data, 99)
print('result: {}'.format(result))

In this example, a previous sum of
99 is used to initialize the value
computed by reduce().

$ python3 functools_reduce_initializer.py

range(1, 5)
do_reduce(99, 1)
do_reduce(100, 2)
do_reduce(102, 3)
do_reduce(105, 4)
result: 109

    evince -p 160 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 198]

Sequences with a single item
automatically reduce to that value
when no initializer is present.
Empty lists generate an error,
unless an initializer is provided.

Listing 3.13: functools_reduce_short_sequences.py

import functools

def do_reduce(a, b):
  print('do_reduce({}, {})'.format(a, b))
  return a + b

print('Single item in sequence:',
      functools.reduce(do_reduce, [1]))

print('Single item in sequence with initializer:',
      functools.reduce(do_reduce, [1], 99))

print('Empty sequence with initializer:',
      functools.reduce(do_reduce, [], 99))

try:
  print('Empty sequence:', functools.reduce(do_reduce, []))
except TypeError as err:
  print('ERROR: {}'.format(err))

Because the initializer argument
serves as a default, but is also
combined with the new values if the
input sequence is not empty, it is
important to consider carefully
whether its use is appropriate.
When it does not make sense to
combine the default with new
values, it is better to catch the
TypeError rather than passing an
initializer.

$ python3 functools_reduce_short_sequences.py

Single item in sequence: 1
do_reduce(99, 1)
Single item in sequence with initializer: 100
Empty sequence with initializer: 99
ERROR: reduce() of empty sequence with no initial value

    evince -p 161 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 199]

3.1.5 Generic Functions

In a dynamically typed language
like Python, there is often a need
to perform slightly different
operations based on the type of an
argument, especially when dealing
with the difference between a list
of items and a single item. It is
simple enough to check the type of
an argument directly, but in cases
where the behavioral difference can
be isolated into separate
functions, functools provides the
singledispatch() decorator to
register a set of generic functions
for automatic switching based on
the type of the first argument to a
function.

Listing 3.14: functools_singledispatch.py

import functools

@functools.singledispatch
def myfunc(arg):
  print('default myfunc({!r})'.format(arg))

@myfunc.register(int)
def myfunc_int(arg):
  print('myfunc_int({})'.format(arg))

@myfunc.register(list)
def myfunc_list(arg):
  print('myfunc_list()')
  for item in arg:
    print('{}'.format(item))

myfunc('string argument')
myfunc(1)
myfunc(2.3)
myfunc(['a', 'b', 'c'])

The register() attribute of the new
function serves as another
decorator for registering
alternative implementations. The
first function wrapped with
singledispatch() is the default
implementation if no other
type-specific function is found, as
with the float case in this
example.

$ python3 functools_singledispatch.py

default myfunc('string argument')
myfunc_int(1)
default myfunc(2.3)
myfunc_list()
  a
  b
  c

    evince -p 162 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 200]

When no exact match is found for
the type, the inheritance order is
evaluated and the closest matching
type is used.

Listing 3.15: functools_singledispatch_mro.py

import functools

class A:
  pass

class B(A):
  pass

class C(A):
  pass

class D(B):
  pass

class E(C, D):
  pass

@functools.singledispatch
def myfunc(arg):
  print('default myfunc({})'.format(arg.__class__.__name__))

@myfunc.register(A)
def myfunc_A(arg):
  print('myfunc_A({})'.format(arg.__class__.__name__))

@myfunc.register(B)
def myfunc_B(arg):
  print('myfunc_B({})'.format(arg.__class__.__name__))

@myfunc.register(C)
def myfunc_C(arg):
  print('myfunc_C({})'.format(arg.__class__.__name__))

myfunc(A())
myfunc(B())
myfunc(C())
myfunc(D())
myfunc(E())

    evince -p 163 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 201]

In this example, classes D and E do
not match exactly with any
registered generic functions, and
the function selected depends on
the class hierarchy.

$ python3 functools_singledispatch_mro.py

myfunc_A(A)
myfunc_B(B)
myfunc_C(C)
myfunc_B(D)
myfunc_C(E)

  Tip: Related Reading

  - Standard library documentation
    for functools.1
  - Rich comparison methods2 :
    Description of the rich
    comparison methods from the
    Python Reference Guide.
  - Isolated @memoize3 : Article on
    creating memoizing decorators
    that work well with unit tests,
    by Ned Batchelder.
  - PEP 4434 : Single-dispatch
    generic functions.
  - inspect (page 1311):
    Introspection API for live
    objects.


3.2 itertools: Iterator Functions

The itertools module includes a set
of functions for working with
sequence data sets. The functions
provided are inspired by similar
features of functional programming
languages such as Clojure, Haskell,
APL, and SML. They are intended to
be fast and use memory efficiently.
They can also be hooked together to
express more complicated
iteration-based algorithms.


1:
https://docs.python.org/3.5/library/functools.html
2:
https://docs.python.org/reference/datamodel.html#object.__lt__
3:
http://nedbatchelder.com/blog/201601/isolated_memoize.html
4: www.python.org/dev/peps/pep-0443


    evince -p 164 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 202]

Iterator-based code offers better
memory consumption characteristics
than code that uses lists. Since
data is not produced from the
iterator until it is needed, all of
the data does not need to be stored
in memory at the same time. This
"lazy" processing model can reduce
swapping and other side effects of
large data sets, improving
performance.

In addition to the functions
defined in itertools, the examples
in this section rely on some of the
built-in functions for iteration.


3.2.1 Merging and Splitting Iterators

The chain() function takes several
iterators as arguments and returns
a single iterator that produces the
contents of all of the inputs as
though they came from a single
iterator.

Listing 3.16: itertools_chain.py

from itertools import *

for i in chain([1, 2, 3], ['a', 'b', 'c']):
  print(i, end=' ')
print()

chain() makes it easy to process
several sequences without
constructing one large list.

$ python3 itertools_chain.py

1 2 3 a b c

If the iterables to be combined are
not all known in advance, or if
they need to be evaluated lazily,
chain.from_iterable() can be used
to construct the chain instead.

Listing 3.17: itertools_chain_from_iterable.py

from itertools import *

def make_iterables_to_chain():
  yield [1, 2, 3]
  yield ['a', 'b', 'c']

for i in chain.from_iterable(make_iterables_to_chain()):
  print(i, end=' ')
print()

$ python3 itertools_chain_from_iterable.py

1 2 3 a b c

    evince -p 165 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 203]

The built-in function zip() returns
an iterator that combines the
elements of several iterators into
tuples.

Listing 3.18: itertools_zip.py

for i in zip([1, 2, 3], ['a', 'b', 'c']):
  print(i)

As with the other functions in this
module, the return value is an
iterable object that produces
values one at a time.

$ python3 itertools_zip.py

(1, 'a')
(2, 'b')
(3, 'c')

zip() stops when the first input
iterator is exhausted. To process
all of the inputs, even if the
iterators produce different numbers
of values, use zip_longest().

Listing 3.19: itertools_zip_longest.py

from itertools import *

r1 = range(3)
r2 = range(2)

print('zip stops early:')
print(list(zip(r1, r2)))

r1 = range(3)
r2 = range(2)

print('\nzip_longest processes all of the values:')
print(list(zip_longest(r1, r2)))

By default, zip_longest()
substitutes None for any missing
values. Use the fillvalue argument
to use a different substitute
value.

$ python3 itertools_zip_longest.py

zip stops early:
[(0, 0), (1, 1)]

zip_longest processes all of the values:
[(0, 0), (1, 1), (2, None)]

    evince -p 166 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 204]

The islice() function returns an
iterator that returns selected
items from the input iterator, by
index.

Listing 3.20: itertools_islice.py

from itertools import *

print('Stop at 5:')
for i in islice(range(100), 5):
  print(i, end=' ')
print('\n')

print('Start at 5, Stop at 10:')
for i in islice(range(100), 5, 10):
  print(i, end=' ')
print('\n')

print('By tens to 100:')
for i in islice(range(100), 0, 100, 10):
  print(i, end=' ')
print('\n')

islice() takes the same arguments
as the slice operator for lists:
start, stop, and step. The start
and step arguments are optional.

$ python3 itertools_islice.py

Stop at 5:
0 1 2 3 4

Start at 5, Stop at 10:
5 6 7 8 9

By tens to 100:
0 10 20 30 40 50 60 70 80 90

The tee() function returns several
independent iterators (defaults to
2) based on a single original
input.

Listing 3.21: itertools_tee.py

from itertools import *

r = islice(count(), 5)
i1, i2 = tee(r)

print('i1:', list(i1))
print('i2:', list(i2))

    evince -p 167 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 205]

tee() has semantics similar to the
Unix tee utility, which repeats the
values it reads from its input and
writes them to a named file and
standard output. The iterators
returned by tee() can be used to
feed the same set of data into
multiple algorithms to be processed
in parallel.

$ python3 itertools_tee.py

i1: [0, 1, 2, 3, 4]
i2: [0, 1, 2, 3, 4]

The new iterators created by tee()
share their input, so the original
iterator should not be used after
the new ones are created.

Listing 3.22: itertools_tee_error.py

from itertools import *

r = islice(count(), 5)
i1, i2 = tee(r)

print('r:', end=' ')
for i in r:
  print(i, end=' ')
  if i > 1:
    break
print()

print('i1:', list(i1))
print('i2:', list(i2))

If values are consumed from the
original input, the new iterators
will not produce those values.

$ python3 itertools_tee_error.py

r: 0 1 2
i1: [3, 4]
i2: [3, 4]


3.2.2 Converting Inputs

The built-in map() function returns
an iterator that calls a function
on the values in the input
iterators, and returns the results.
It stops when any input iterator is
exhausted.

    evince -p 168 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 206]

Listing 3.23: itertools_map.py

def times_two(x):
  return 2 * x

def multiply(x, y):
  return (x, y, x * y)

print('Doubles:')
for i in map(times_two, range(5)):
  print(i)

print('\nMultiples:')
r1 = range(5)
r2 = range(5, 10)
for i in map(multiply, r1, r2):
  print('{:d} * {:d} = {:d}'.format(*i))

print('\nStopping:')
r1 = range(5)
r2 = range(2)
for i in map(multiply, r1, r2):
  print(i)

In the first example, the lambda
function multiplies the input
values by 2. In the second example,
the lambda function multiplies two
arguments, taken from separate
iterators, and returns a tuple with
the original arguments and the
computed value. The third example
stops after producing two tuples
because the second range is
exhausted.

$ python3 itertools_map.py

Doubles:
0
2
4
6
8

Multiples:
0 * 5 = 0
1 * 6 = 6
2 * 7 = 14
3 * 8 = 24
4 * 9 = 36

Stopping:
(0, 0, 0)
(1, 1, 1)

    evince -p 169 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 207]

The starmap() function is similar
to map(), but instead of
constructing a tuple from multiple
iterators, it splits up the items
in a single iterator as arguments
to the mapping function using the *
syntax.

Listing 3.24: itertools_starmap.py

from itertools import *

values = [(0, 5), (1, 6), (2, 7), (3, 8), (4, 9)]

for i in starmap(lambda x, y: (x, y, x * y), values):
  print('{} * {} = {}'.format(*i))

Where the mapping function to map()
is called f(i1,i2), the mapping
function passed to starmap() is
called f(*i).

$ python3 itertools_starmap.py

0 * 5 = 0
1 * 6 = 6
2 * 7 = 14
3 * 8 = 24
4 * 9 = 36


3.2.3 Producing New Values

The count() function returns an
iterator that produces consecutive
integers, indefinitely. The first
number can be passed as an argument
(the default is zero). There is no
upper bound argument (see the
built-in range() for more control
over the result set).

Listing 3.25: itertools_count.py

from itertools import *

for i in zip(count(1), ['a', 'b', 'c']):
  print(i)

This example stops because the list argument is consumed.

$ python3 itertools_count.py

(1, 'a')
(2, 'b')
(3, 'c')

    evince -p 170 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 208]

The start and step arguments to
count() can be any numerical values
that can be added together.

Listing 3.26: itertools_count_step.py

import fractions
from itertools import *

start = fractions.Fraction(1, 3)
step = fractions.Fraction(1, 3)

for i in zip(count(start, step), ['a', 'b', 'c']):
  print('{}: {}'.format(*i))

In this example, the start point
and steps are Fraction objects from
the fraction module.

$ python3 itertools_count_step.py

1/3: a
2/3: b
1: c

The cycle() function returns an
iterator that repeats the contents
of the arguments it is given
indefinitely. Because it has to
remember the entire contents of the
input iterator, it may consume
quite a bit of memory if the
iterator is long.

Listing 3.27: itertools_cycle.py

from itertools import *

for i in zip(range(7), cycle(['a', 'b', 'c'])):
  print(i)

A counter variable is used to break
out of the loop after a few cycles
in this example.

$ python3 itertools_cycle.py

(0, 'a')
(1, 'b')
(2, 'c')
(3, 'a')
(4, 'b')
(5, 'c')
(6, 'a')

The repeat() function returns an
iterator that produces the same
value each time it is accessed.

    evince -p 171 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 209]

Listing 3.28: itertools_repeat.py

from itertools import *

for i in repeat('over-and-over', 5):
  print(i)

The iterator returned by repeat()
keeps returning data forever,
unless the optional times argument
is provided to limit it.

$ python3 itertools_repeat.py

over-and-over
over-and-over
over-and-over
over-and-over
over-and-over

It is useful to combine repeat()
with zip() or map() when invariant
values should be included with the
values from the other iterators.

Listing 3.29: itertools_repeat_zip.py

from itertools import *

for i, s in zip(count(), repeat('over-and-over', 5)):
  print(i, s)

A counter value is combined with
the constant returned by repeat()
in this example.

$ python3 itertools_repeat_zip.py

0 over-and-over
1 over-and-over
2 over-and-over
3 over-and-over
4 over-and-over

This example uses map() to multiply
the numbers in the range 0 through
4 by 2.

Listing 3.30: itertools_repeat_map.py

from itertools import *

for i in map(lambda x, y: (x, y, x * y), repeat(2), range(5)):
  print('{:d} * {:d} = {:d}'.format(*i))

    evince -p 172 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 210]

The repeat() iterator does not need
to be explicitly limited, since
map() stops processing when any of
its inputs ends, and the range()
returns only five elements.

$ python3 itertools_repeat_map.py

2 * 0 = 0
2 * 1 = 2
2 * 2 = 4
2 * 3 = 6
2 * 4 = 8


3.2.4 Filtering

The dropwhile() function returns an
iterator that produces elements of
the input iterator after a
condition becomes false for the
first time.

Listing 3.31: itertools_dropwhile.py

from itertools import *

def should_drop(x):
  print('Testing:', x)
  return x < 1

for i in dropwhile(should_drop, [-1, 0, 1, 2, -2]):
  print('Yielding:', i)

dropwhile() does not filter every
item of the input. After the
condition is false the first time,
all of the remaining items in the
input are returned.

$ python3 itertools_dropwhile.py

Testing: -1
Testing: 0
Testing: 1
Yielding: 1
Yielding: 2
Yielding: -2

The opposite of dropwhile() is
takewhile(). It returns an iterator
that itself returns items from the
input iterator as long as the test
function returns true.

Listing 3.32: itertools_takewhile.py

from itertools import *

def should_take(x):
  print('Testing:', x)
  return x < 2

for i in takewhile(should_take, [-1, 0, 1, 2, -2]):
  print('Yielding:', i)

    evince -p 173 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 211]

As soon as should_take() returns
false, takewhile() stops processing
the input.

$ python3 itertools_takewhile.py

Testing: -1
Yielding: -1
Testing: 0
Yielding: 0
Testing: 1
Yielding: 1
Testing: 2

The built-in function filter()
returns an iterator that includes
only items for which the test
function returns true.

Listing 3.33: itertools_filter.py

from itertools import *

def check_item(x):
  print('Testing:', x)
  return x < 1

for i in filter(check_item, [-1, 0, 1, 2, -2]):
  print('Yielding:', i)

filter() differs from dropwhile()
and takewhile() in that every item
is tested before it is returned.

$ python3 itertools_filter.py

Testing: -1
Yielding: -1
Testing: 0
Yielding: 0
Testing: 1
Testing: 2
Testing: -2
Yielding: -2

    evince -p 174 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 212]

filterfalse() returns an iterator
that includes only items where the
test function returns false.

Listing 3.34: itertools_filterfalse.py

from itertools import *

def check_item(x):
  print('Testing:', x)
  return x < 1

for i in filterfalse(check_item, [-1, 0, 1, 2, -2]):
  print('Yielding:', i)

The test expression in check_item()
is the same, so the results in this
example with filterfalse() are the
opposite of the results from the
previous example.

$ python3 itertools_filterfalse.py

Testing: -1
Testing: 0
Testing: 1
Yielding: 1
Testing: 2
Yielding: 2
Testing: -2

compress() offers another way to
filter the contents of an iterable.
Instead of calling a function, it
uses the values in another iterable
to indicate when to accept a value
and when to ignore it.

Listing 3.35: itertools_compress.py

from itertools import *

every_third = cycle([False, False, True])
data = range(1, 10)

for i in compress(data, every_third):
  print(i, end=' ')
print()

The first argument is the data
iterable to process. The second
argument is a selector iterable
that produces boolean values
indicating which elements to take
from the data input (a true value
causes the value to be produced; a
false value causes it to be
ignored).

$ python3 itertools_compress.py

3 6 9

    evince -p 175 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 213]

3.2.5 Grouping Data

The groupby() function returns an
iterator that produces sets of
values organized by a common key.
This example illustrates grouping
of related values based on an
attribute.

Listing 3.36: itertools_groupby_seq.py

import functools
from itertools import *
import operator
import pprint

@functools.total_ordering
class Point:

  def __init__(self, x, y):
    self.x = x
    self.y = y

  def __repr__(self):
    return '({}, {})'.format(self.x, self.y)

  def __eq__(self, other):
    return (self.x, self.y) == (other.x, other.y)

  def __gt__(self, other):
    return (self.x, self.y) > (other.x, other.y)

# Create a data set of Point instances.
data = list(map(Point,
            cycle(islice(count(), 3)),
            islice(count(), 7)))
print('Data:')
pprint.pprint(data, width=35)
print()

# Try to group the unsorted data based on X values.
print('Grouped, unsorted:')
for k, g in groupby(data, operator.attrgetter('x')):
  print(k, list(g))
print()

# Sort the data.
data.sort()
print('Sorted:')
pprint.pprint(data, width=35)
print()

# Group the sorted data based on X values.
print('Grouped, sorted:')
for k, g in groupby(data, operator.attrgetter('x')):
  print(k, list(g))
print()

    evince -p 176 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 214]

The input sequence needs to be
sorted on the key value so that the
groupings will work out as
expected.

$ python3 itertools_groupby_seq.py

Data:
[(0, 0),
 (1, 1),
 (2, 2),
 (0, 3),
 (1, 4),
 (2, 5),
 (0, 6)]

Grouped, unsorted:
0 [(0, 0)]
1 [(1, 1)]
2 [(2, 2)]
0 [(0, 3)]
1 [(1, 4)]
2 [(2, 5)]
0 [(0, 6)]

Sorted:
[(0, 0),
 (0, 3),
 (0, 6),
 (1, 1),
 (1, 4),
 (2, 2),
 (2, 5)]

Grouped, sorted:
0 [(0, 0), (0, 3), (0, 6)]
1 [(1, 1), (1, 4)]
2 [(2, 2), (2, 5)]


3.2.6 Combining Inputs

The accumulate() function processes
the input iterable, passing the nth
and n+1st item to a function and
producing the return value instead
of either input. The default
function used to combine the two
values adds them, so accumulate()
can be used to produce the
cumulative sum of a series of
numerical inputs.

    evince -p 177 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 215]

Listing 3.37: itertools_accumulate.py

from itertools import *

print(list(accumulate(range(5))))
print(list(accumulate('abcde')))

When used with a sequence of
non-integer values, the results
depend on what it means to "add"
two items together. The second
example in this script shows that
when accumulate() receives a string
input, each response is a
progressively longer prefix of that
string.

$ python3 itertools_accumulate.py

[0, 1, 3, 6, 10]
['a', 'ab', 'abc', 'abcd', 'abcde']

accumulate() may be combined with
any other function that takes two
input values to achieve different
results.

Listing 3.38: itertools_accumulate_custom.py

from itertools import *

def f(a, b):
  print(a, b)
  return b + a + b

print(list(accumulate('abcde', f)))

This example combines the string
values in a way that makes a series
of (nonsensical) palindromes. Each
step of the way when f() is called,
it prints the input values passed
to it by accumulate().

$ python3 itertools_accumulate_custom.py

a b
bab c
cbabc d
dcbabcd e
['a', 'bab', 'cbabc', 'dcbabcd', 'edcbabcde']

Nested for loops that iterate over
multiple sequences can often be
replaced with product(), which
produces a single iterable whose
values are the Cartesian product of
the set of input values.

    evince -p 178 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 216]

Listing 3.39: itertools_product.py

from itertools import *
import pprint

FACE_CARDS = ('J', 'Q', 'K', 'A')
SUITS = ('H', 'D', 'C', 'S')

DECK = list(
  product(
    chain(range(2, 11), FACE_CARDS),
    SUITS,
  )
)

for card in DECK:
  print('{:>2}{}'.format(*card), end=' ')
  if card[1] == SUITS[-1]:
    print()

The values produced by product()
are tuples, with the members taken
from each of the iterables passed
in as arguments in the order they
are passed. The first tuple
returned includes the first value
from each iterable. The last
iterable passed to product() is
processed first, followed by the
next-to-last, and so on. The result
is that the return values are in
order based on the first iterable,
then the next iterable, and so on.

In this example, the cards are
ordered first by value and then by
suit.

$ python3 itertools_product.py

 2H  2D  2C  2S
 3H  3D  3C  3S
 4H  4D  4C  4S
 5H  5D  5C  5S
 6H  6D  6C  6S
 7H  7D  7C  7S
 8H  8D  8C  8S
 9H  9D  9C  9S
10H 10D 10C 10S
 JH  JD  JC  JS
 QH  QD  QC  QS
 KH  KD  KC  KS
 AH  AD  AC  AS

To change the order of the cards,
change the order of the arguments
to product().

Listing 3.40: itertools_product_ordering.py

from itertools import *
import pprint

FACE_CARDS = ('J', 'Q', 'K', 'A')
SUITS = ('H', 'D', 'C', 'S')

DECK = list(
  product(
    SUITS,
    chain(range(2, 11), FACE_CARDS),
  )
)

for card in DECK:
  print('{:>2}{}'.format(card[1], card[0]), end=' ')
  if card[1] == FACE_CARDS[-1]:
    print()

    evince -p 179 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 217]

The print loop in this example
looks for an ace card, instead of
the spade suit, and then adds a
newline to break up the output.

$ python3 itertools_product_ordering.py

2H 3H 4H 5H 6H 7H 8H 9H 10H JH QH KH AH
2D 3D 4D 5D 6D 7D 8D 9D 10D JD QD KD AD
2C 3C 4C 5C 6C 7C 8C 9C 10C JC QC KC AC
2S 3S 4S 5S 6S 7S 8S 9S 10S JS QS KS AS

To compute the product of a
sequence with itself, specify how
many times the input should be
repeated.

Listing 3.41: itertools_product_repeat.py

from itertools import *

def show(iterable):
  for i, item in enumerate(iterable, 1):
    print(item, end=' ')
    if (i % 3) == 0:
      print()
  print()

print('Repeat 2:\n')
show(list(product(range(3), repeat=2)))

print('Repeat 3:\n')
show(list(product(range(3), repeat=3)))

    evince -p 180 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 218]

Since repeating a single iterable
is like passing the same iterable
multiple times, each tuple produced
by product() will contain a number
of items equal to the repeat
counter.

$ python3 itertools_product_repeat.py

Repeat 2:

(0, 0) (0, 1) (0, 2)
(1, 0) (1, 1) (1, 2)
(2, 0) (2, 1) (2, 2)

Repeat 3:

(0, 0, 0) (0, 0, 1) (0, 0, 2)
(0, 1, 0) (0, 1, 1) (0, 1, 2)
(0, 2, 0) (0, 2, 1) (0, 2, 2)
(1, 0, 0) (1, 0, 1) (1, 0, 2)
(1, 1, 0) (1, 1, 1) (1, 1, 2)
(1, 2, 0) (1, 2, 1) (1, 2, 2)
(2, 0, 0) (2, 0, 1) (2, 0, 2)
(2, 1, 0) (2, 1, 1) (2, 1, 2)
(2, 2, 0) (2, 2, 1) (2, 2, 2)

The permutations() function
produces items from the input
iterable combined in the possible
permutations of the given length.
It defaults to producing the full
set of all permutations.

Listing 3.42: itertools_permutations.py

from itertools import *

def show(iterable):
  first = None
  for i, item in enumerate(iterable, 1):
    if first != item[0]:
      if first is not None:
        print()
      first = item[0]
    print(''.join(item), end=' ')
  print()

print('All permutations:\n')
show(permutations('abcd'))

print('\nPairs:\n')
show(permutations('abcd', r=2))

Use the r argument to limit the
length and number of the individual
permutations returned.

    evince -p 181 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 219]

$ python3 itertools_permutations.py

All permutations:

abcd abdc acbd acdb adbc adcb
bacd badc bcad bcda bdac bdca
cabd cadb cbad cbda cdab cdba
dabc dacb dbac dbca dcab dcba

Pairs:

ab ac ad
ba bc bd
ca cb cd
da db dc

To limit the values to unique
combinations rather than
permutations, use combinations().
As long as the members of the input
are unique, the output will not
include any repeated values.

Listing 3.43: itertools_combinations.py

from itertools import *

def show(iterable):
  first = None
  for i, item in enumerate(iterable, 1):
    if first != item[0]:
      if first is not None:
        print()
      first = item[0]
    print(''.join(item), end=' ')
  print()

print('Unique pairs:\n')
show(combinations('abcd', r=2))

Unlike with permutations, the r
argument to combinations() is
required.

$ python3 itertools_combinations.py

Unique pairs:

ab ac ad
bc bd
cd

    evince -p 182 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 220]

While combinations() does not
repeat individual input elements,
sometimes it is useful to consider
combinations that do include
repeated elements. For those cases,
use
combinations_with_replacement().

Listing 3.44: itertools_combinations_with_replacement.py

from itertools import *

def show(iterable):
  first = None
  for i, item in enumerate(iterable, 1):
    if first != item[0]:
      if first is not None:
        print()
      first = item[0]
    print(''.join(item), end=' ')
  print()

print('Unique pairs:\n')
show(combinations_with_replacement('abcd', r=2))

In this output, each input item is
paired with itself as well as all
of the other members of the input
sequence.

$ python3 itertools_combinations_with_replacement.py

Unique pairs:

aa ab ac ad
bb bc bd
cc cd
dd

  Tip: Related Reading

  - Standard library documentation
    for itertools.5
  - Python 2 to 3 porting notes for
    itertools (page 1359).
  - The Standard ML Basis Library6:
    The library for SML.
  - Definition of Haskell and the
    Standard Libraries7: Standard
    library specification for the
    functional language Haskell.
  - Clojure8: Clojure is a dynamic
    functional language that runs
    on the Java Virtual Machine.
  - tee9: Unix command-line tool
    for splitting one input into
    multiple identical output
    streams.
  - Wikipedia: Cartesian product10:
    Mathematical definition of the
    Cartesian product of two
    sequences.


5:
https://docs.python.org/3.5/library/itertools.html
6: www.standardml.org/Basis/ 7:
www.haskell.org/definition/ 8:
http://clojure.org

    evince -p 183 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 221]

3.3 operator: Functional Interface to Built-In Operators

Programming using iterators
occasionally requires creating
small functions for simple
expressions. Sometimes, these can
be implemented as lambda functions,
but for some operations new
functions are not needed at all.
The operator module defines
functions that correspond to the
built-in arithmetic, comparison,
and other operations for the
standard object APIs.


3.3.1 Logical Operations

Functions are provided for
determining the boolean equivalent
for a value, negating a value to
create the opposite boolean value,
and comparing objects to see if
they are identical.

Listing 3.45: operator_boolean.py

from operator import *

a = -1
b = 5

print('a =', a)
print('b =', b)
print()

print('not_(a)     :', not_(a))
print('truth(a)    :', truth(a))
print('is_(a, b)   :', is_(a, b))
print('is_not(a, b):', is_not(a, b))

not_() includes a trailing
underscore because not is a Python
keyword. truth() applies the same
logic used when testing an
expression in an if statement or
converting an expression to a bool.
is_() implements the same check
used by the is keyword, and
is_not() does the same test and
returns the opposite answer.

$ python3 operator_boolean.py

a = -1
b = 5

not_(a)     : False
truth(a)    : True
is_(a, b)   : False
is_not(a, b): True


9:
http://man7.org/linux/man-pages/man1/tee.1.html
10:
https://en.wikipedia.org/wiki/Cartesian_product

    evince -p 184 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 222]

3.3.2 Comparison Operators

All of the rich comparison
operators are supported.

Listing 3.46: operator_comparisons.py

from operator import *

a = 1
b = 5.0

print('a =', a)
print('b =', b)
for func in (lt, le, eq, ne, ge, gt):
  print('{}(a, b): {}'.format(func.__name__, func(a, b)))

The functions are equivalent to the
expression syntax using <, <=, ==,
>=, and >.

$ python3 operator_comparisons.py

a = 1
b = 5.0
lt(a, b): True
le(a, b): True
eq(a, b): False
ne(a, b): True
ge(a, b): False
gt(a, b): False


3.3.3 Arithmetic Operators

The arithmetic operators for
manipulating numerical values are
also supported.

Listing 3.47: operator_math.py

from operator import *

a = -1
b = 5.0
c = 2
d = 6

print('a =', a)

    evince -p 185 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 223]

print('b =', b)
print('c =', c)
print('d =', d)

print('\nPositive/Negative:')
print('abs(a):', abs(a))
print('neg(a):', neg(a))
print('neg(b):', neg(b))
print('pos(a):', pos(a))
print('pos(b):', pos(b))

print('\nArithmetic:')
print('add(a, b)     :', add(a, b))
print('floordiv(a, b):', floordiv(a, b))
print('floordiv(d, c):', floordiv(d, c))
print('mod(a, b)     :', mod(a, b))
print('mul(a, b)     :', mul(a, b))
print('pow(c, d)     :', pow(c, d))
print('sub(b, a)     :', sub(b, a))
print('truediv(a, b) :', truediv(a, b))
print('truediv(d, c) :', truediv(d, c))

print('\nBitwise:')
print('and_(c, d)  :', and_(c, d))
print('invert(c)   :', invert(c))
print('lshift(c, d):', lshift(c, d))
print('or_(c, d)   :', or_(c, d))
print('rshift(d, c):', rshift(d, c))
print('xor(c, d)   :', xor(c, d))

Two different division operators
are provided: floordiv() (integer
division as implemented in Python
before version 3.0) and truediv()
(floating-point division).

$ python3 operator_math.py

a = -1
b = 5.0
c = 2
d = 6

Positive/Negative:
abs(a): 1
neg(a): 1
neg(b): -5.0
pos(a): -1
pos(b): 5.0

    evince -p 186 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 224]

Arithmetic:
add(a, b)     : 4.0
floordiv(a, b): -1.0
floordiv(d, c): 3
mod(a, b)     : 4.0
mul(a, b)     : -5.0
pow(c, d)     : 64
sub(b, a)     : 6.0
truediv(a, b) : -0.2
truediv(d, c) : 3.0

Bitwise:
and_(c, d)  : 2
invert(c)   : -3
lshift(c, d): 128
or_(c, d)   : 6
rshift(d, c): 1
xor(c, d)   : 4


3.3.4 Sequence Operators

The operators for working with
sequences can be organized into
four groups: building up sequences,
searching for items, accessing
contents, and removing items from
sequences.

Listing 3.48: operator_sequences.py

from operator import *

a = [1, 2, 3]
b = ['a', 'b', 'c']

print('a =', a)
print('b =', b)

print('\nConstructive:')
print('concat(a, b):', concat(a, b))

print('\nSearching:')
print('contains(a, 1)  :', contains(a, 1))
print('contains(b, "d"):', contains(b, "d"))
print('countOf(a, 1)   :', countOf(a, 1))
print('countOf(b, "d") :', countOf(b, "d"))
print('indexOf(a, 5)   :', indexOf(a, 1))

print('\nAccess Items:')
print('getitem(b, 1)                :',
      getitem(b, 1))
print('getitem(b, slice(1, 3))      :',
      getitem(b, slice(1, 3)))

    evince -p 187 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 225]

print('setitem(b, 1, "d")             :', end=' ')
setitem(b, 1, "d")
print(b)
print('setitem(a, slice(1, 3), [4, 5]):', end=' ')
setitem(a, slice(1, 3), [4, 5])
print(a)

print('\nDestructive:')
print('delitem(b, 1)          :', end=' ')
delitem(b, 1)
print(b)
print('delitem(a, slice(1, 3)):', end=' ')
delitem(a, slice(1, 3))
print(a)

Some of these operations, such as
setitem() and delitem(), modify the
sequence in place and do not return
a value.

$ python3 operator_sequences.py

a = [1, 2, 3]
b = ['a', 'b', 'c']

Constructive:
  concat(a, b): [1, 2, 3, 'a', 'b', 'c']

Searching:
contains(a, 1)  : True
contains(b, "d"): False
countOf(a, 1)   : 1
countOf(b, "d") : 0
indexOf(a, 5)   : 0

Access Items:
getitem(b, 1)                  : b
getitem(b, slice(1, 3))        : ['b', 'c']
setitem(b, 1, "d")             : ['a', 'd', 'c']
setitem(a, slice(1, 3), [4, 5]): [1, 4, 5]

Destructive:
delitem(b, 1)          : ['a', 'c']
delitem(a, slice(1, 3)): [1]


3.3.5 In-Place Operators

In addition to the standard
operators, many types of objects
support "in-place" modification
through special operators such as
+=. Equivalent functions are
available for in-place
modifications as well.

    evince -p 188 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 226]

Listing 3.49: operator_inplace.py

from operator import *

a = -1
b = 5.0
c = [1, 2, 3]
d = ['a', 'b', 'c']
print('a =', a)
print('b =', b)
print('c =', c)
print('d =', d)
print()

a = iadd(a, b)
print('a = iadd(a, b) =>', a)
print()

c = iconcat(c, d)
print('c = iconcat(c, d) =>', c)

These examples demonstrate just a
few of the functions. Refer to the
standard library documentation for
complete details.

$ python3 operator_inplace.py

a = -1
b = 5.0
c = [1, 2, 3]
d = ['a', 'b', 'c']

a = iadd(a, b) => 4.0

c = iconcat(c, d) => [1, 2, 3, 'a', 'b', 'c']


3.3.6 Attribute and Item "Getters"

One of the most unusual features of
the operator module is the concept
of getters. These callable objects
are constructed at runtime and
retrieve attributes of objects or
contents from sequences. Getters
are especially useful when working
with iterators or generator
sequences, as they incur less
overhead than a lambda or Python
function.

    evince -p 189 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 227]

Listing 3.50: operator_attrgetter.py

from operator import *

class MyObj:
  """example class for attrgetter"""
  def __init__(self, arg):
    super().__init__()
    self.arg = arg

  def __repr__(self):
    return 'MyObj({})'.format(self.arg)

l = [MyObj(i) for i in range(5)]
print('objects   :', l)

# Extract the 'arg' value from each object.
g = attrgetter('arg')
vals = [g(i) for i in l]
print('arg values:', vals)

# Sort using arg.
l.reverse()
print('reversed  :', l)
print('sorted    :', sorted(l, key=g))

Attribute getters work like lambda
x,n='attrname': getattr(x,n):

$ python3 operator_attrgetter.py

objects   : [MyObj(0), MyObj(1), MyObj(2), MyObj(3), MyObj(4)]
arg values: [0, 1, 2, 3, 4]
reversed  : [MyObj(4), MyObj(3), MyObj(2), MyObj(1), MyObj(0)]
sorted    : [MyObj(0), MyObj(1), MyObj(2), MyObj(3), MyObj(4)]

Item getters work like lambda
x,y=5: x[y]:

Listing 3.51: operator_itemgetter.py

from operator import *

l = [dict(val=-1 * i) for i in range(4)]
print('Dictionaries:')
print('original:', l)
g = itemgetter('val')
vals = [g(i) for i in l]
print('   values:', vals)
print('   sorted:', sorted(l, key=g))

print
l = [(i, i * -2) for i in range(4)]
print('\nTuples:')
print('original:', l)
g = itemgetter(1)

vals = [g(i) for i in l]
print('   values:', vals)
print('   sorted:', sorted(l, key=g))

    evince -p 190 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 228]

Item getters work with mappings as
well as sequences.

$ python3 operator_itemgetter.py

Dictionaries:
 original: [{'val': 0}, {'val': -1}, {'val': -2}, {'val': -3}]
   values: [0, -1, -2, -3]
   sorted: [{'val': -3}, {'val': -2}, {'val': -1}, {'val': 0}]

Tuples:
 original: [(0, 0), (1, -2), (2, -4), (3, -6)]
   values: [0, -2, -4, -6]
   sorted: [(3, -6), (2, -4), (1, -2), (0, 0)]


3.3.7 Combining Operators and Custom Classes

The functions in the operator
module work via the standard Python
interfaces when performing their
operations. Thus, they work with
user-defined classes as well as the
built-in types.

Listing 3.52: operator_classes.py

from operator import *

class MyObj:
  """Example for operator overloading"""

  def __init__(self, val):
    super(MyObj, self).__init__()
    self.val = val

  def __str__(self):
    return 'MyObj({})'.format(self.val)

  def __lt__(self, other):
    """compare for less-than"""
    print('Testing {} < {}'.format(self, other))
    return self.val < other.val

  def __add__(self, other):
    """add values"""
    print('Adding {} + {}'.format(self, other))
    return MyObj(self.val + other.val)

a = MyObj(1)
b = MyObj(2)

print('Comparison:')
print(lt(a, b))

print('\nArithmetic:')
print(add(a, b))

    evince -p 191 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 229]

Refer to the Python reference guide
for a complete list of the special
methods used by each operator.

$ python3 operator_classes.py

Comparison:
Testing MyObj(1) < MyObj(2)
True

Arithmetic:
Adding MyObj(1) + MyObj(2)
MyObj(3)

  Tip: Related Reading

  - Standard library documentation
    for operator.11
  - functools (page 143):
    Functional programming tools,
    including the total_ordering()
    decorator for adding rich
    comparison methods to a class.
  - itertools (page 163): Iterator
    operations.
  - collections (page 75): Abstract
    types for collections.
  - numbers: Abstract types for
    numerical values.


3.4 contextlib: Context Manager Utilities

The contextlib module contains
utilities for working with context
managers and the with statement.


3.4.1 Context Manager API

A context manager is responsible
for a resource within a code block,
possibly creating it when the block
is entered and then cleaning it up
after the block is exited. For
example, files support the context
manager API, which ensures that the
files are closed after all reading
or writing is done.


11:
https://docs.python.org/3.5/library/operator.html

    evince -p 192 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 230]

Listing 3.53: contextlib_file.py

with open('/tmp/pymotw.txt', 'wt') as f:
  f.write('contents go here')
# File is automatically closed

A context manager is enabled by the
with statement, and the API
involves two methods. The
__enter__() method is run when
execution flow enters the code
block inside the with statement. It
returns an object to be used within
the context. When execution flow
leaves the with block, the
__exit__() method of the context
manager is called to clean up any
resources that were used.

Listing 3.54: contextlib_api.py

class Context:

  def __init__(self):
    print('__init__()')

  def __enter__(self):
    print('__enter__()')
    return self

  def __exit__(self, exc_type, exc_val, exc_tb):
    print('__exit__()')

with Context():
  print('Doing work in the context')

Combining a context manager and the
with statement is a more compact
way of writing a try:finally block,
since the context manager's
__exit__() method is always called,
even if an exception is raised.

$ python3 contextlib_api.py

__init__()
__enter__()
Doing work in the context
__exit__()

The __enter__() method can return
any object to be associated with a
name specified in the as clause of
the with statement. In this
example, the Context returns an
object that uses the open context.

    evince -p 193 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 231]

Listing 3.55: contextlib_api_other_object.py

class WithinContext:

  def __init__(self, context):
    print('WithinContext.__init__({})'.format(context))

  def do_something(self):
    print('WithinContext.do_something()')

  def __del__(self):
    print('WithinContext.__del__')

class Context:

  def __init__(self):

    print('Context.__init__()')
  def __enter__(self):
    print('Context.__enter__()')
    return WithinContext(self)

  def __exit__(self, exc_type, exc_val, exc_tb):
    print('Context.__exit__()')

with Context() as c:
  c.do_something()

The value associated with the
variable c is the object returned
by __enter__(), which is not
necessarily the Context instance
created in the with statement.

$ python3 contextlib_api_other_object.py

Context.__init__()
Context.__enter__()
WithinContext.__init__(<__main__.Context object at 0x1007b1c50>)
WithinContext.do_something()
Context.__exit__()
WithinContext.__del__

The __exit__() method receives
arguments containing details of any
exception raised in the with block.

Listing 3.56: contextlib_api_error.py

class Context:

  def __init__(self, handle_error):
    print('__init__({})'.format(handle_error))
    self.handle_error = handle_error

  def __enter__(self):
    print('__enter__()')
    return self

  def __exit__(self, exc_type, exc_val, exc_tb):
    print('__exit__()')
    print('exc_type =', exc_type)
    print('exc_val =', exc_val)
    print('exc_tb
    =', exc_tb)
    return self.handle_error

with Context(True):
  raise RuntimeError('error message handled')

print()

with Context(False):
  raise RuntimeError('error message propagated')

    evince -p 194 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 232]

If the context manager can handle
the exception, __exit__() should
return a true value to indicate
that the exception does not need to
be propagated. Returning a false
value causes the exception to be
raised again after __exit__()
returns.

$ python3 contextlib_api_error.py

__init__(True)
__enter__()
__exit__()
  exc_type = <class 'RuntimeError'>
  exc_val  = error message handled
  exc_tb   = <traceback object at 0x10115cc88>

__init__(False)
__enter__()
__exit__()
  exc_type = <class 'RuntimeError'>
  exc_val  = error message propagated
  exc_tb   = <traceback object at 0x10115cc88>
Traceback (most recent call last):
  File "contextlib_api_error.py", line 33, in <module>
    raise RuntimeError('error message propagated')
RuntimeError: error message propagated


3.4.2 Context Managers as Function Decorators

The class ContextDecorator adds
support to regular context manager
classes so that they can be used as
function decorators as well as
context managers.

    evince -p 195 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 233]

Listing 3.57: contextlib_decorator.py

import contextlib

class Context(contextlib.ContextDecorator):
  def __init__(self, how_used):
    self.how_used = how_used
    print('__init__({})'.format(how_used))

  def __enter__(self):
    print('__enter__({})'.format(self.how_used))
    return self

  def __exit__(self, exc_type, exc_val, exc_tb):
    print('__exit__({})'.format(self.how_used))

@Context('as decorator')
def func(message):
  print(message)

print()
with Context('as context manager'):
  print('Doing work in the context')

print()
func('Doing work in the wrapped function')

One difference that arises when
using the context manager as a
decorator is that the value
returned by __enter__() is not
available inside the function being
decorated, unlike the case when
with and as are used. Arguments
passed to the decorated function
are available in the usual way.

$ python3 contextlib_decorator.py

__init__(as decorator)

__init__(as context manager)
__enter__(as context manager)
Doing work in the context
__exit__(as context manager)

__enter__(as decorator)
Doing work in the wrapped function
__exit__(as decorator)

    evince -p 196 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 234]

3.4.3 From Generator to Context Manager

Creating context managers the
traditional way - that is, by
writing a class with __enter__()
and __exit__() methods - is not
difficult. Nevertheless, writing
everything out fully creates extra
overhead when only a trivial bit of
context is being managed. In those
sorts of situations, the best
approach is to use the
contextmanager() decorator to
convert a generator function into a
context manager.

Listing 3.58: contextlib_contextmanager.py

import contextlib

@contextlib.contextmanager
def make_context():
  print('entering')
  try:
    yield {}
  except RuntimeError as err:
    print('ERROR:', err)
  finally:
    print('exiting')

print('Normal:')
with make_context() as value:
  print('inside with statement:', value)

print('\nHandled error:')
with make_context() as value:
  raise RuntimeError('showing example of handling an error')

print('\nUnhandled error:')
with make_context() as value:
  raise ValueError('this exception is not handled')

The generator should initialize the
context, invoke yield exactly one
time, and then clean up the
context. The value yielded, if any,
is bound to the variable in the as
clause of the with statement.
Exceptions from within the with
block are raised again inside the
generator, so they can be handled
there.

$ python3 contextlib_contextmanager.py

Normal:
  entering
  inside with statement: {}
  exiting

Handled error:
  entering
  ERROR: showing example of handling an error
  exiting

Unhandled error:
  entering
  exiting
Traceback (most recent call last):
  File "contextlib_contextmanager.py", line 32, in <module>
    raise ValueError('this exception is not handled')
ValueError: this exception is not handled

    evince -p 197 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 235]

The context manager returned by
contextmanager() is derived from
ContextDecorator, so it also works
as a function decorator.

Listing 3.59: contextlib_contextmanager_decorator.py

import contextlib

@contextlib.contextmanager
def make_context():
  print('entering')
  try:
    # Yield control, but not a value, because any value
    # yielded is not available when the context manager
    # is used as a decorator.
    yield
  except RuntimeError as err:
    print('ERROR:', err)
  finally:
    print('exiting')

@make_context()
def normal():
  print('inside with statement')

@make_context()
def throw_error(err):
  raise err

print('Normal:')
normal()

print('\nHandled error:')
throw_error(RuntimeError('showing example of handling an error'))

print('\nUnhandled error:')
throw_error(ValueError('this exception is not handled'))

    evince -p 198 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 236]

As shown in the preceding
ContextDecorator example, when the
context manager is used as a
decorator, the value yielded by the
generator is not available inside
the function being decorated.
Arguments passed to the decorated
function are still available, as
demonstrated by throw_error() in
this example.

$ python3 contextlib_contextmanager_decorator.py

Normal:
  entering
  inside with statement
  exiting

Handled error:
  entering
  ERROR: showing example of handling an error
  exiting

Unhandled error:
  entering
  exiting
Traceback (most recent call last):
  File "contextlib_contextmanager_decorator.py", line 43, in <module>
    throw_error(ValueError('this exception is not handled'))
  File ".../lib/python3.5/contextlib.py", line 30, in inner
    return func(*args, **kwds)
  File "contextlib_contextmanager_decorator.py", line 33, in throw_error
    raise err
  ValueError: this exception is not handled


3.4.4 Closing Open Handles

The file class supports the context
manager API directly, but some
other objects that represent open
handles do not. The example given
in the standard library
documentation for contextlib is the
object returned from
urllib.urlopen(). Some other legacy
classes use a close() method but do
not support the context manager
API. To ensure that a handle is
closed, use closing() to create a
context manager for it.

Listing 3.60: contextlib_closing.py

import contextlib

class Door:

  def __init__(self):
    print('__init__()')
    self.status = 'open'

  def close(self):
    print('close()')
    self.status = 'closed'

print('Normal Example:')
with contextlib.closing(Door()) as door:
  print('inside with statement: {}'.format(door.status))
print('outside with statement: {}'.format(door.status))

print('\nError handling example:')
try:
  with contextlib.closing(Door()) as door:
    print('raising from inside with statement')
    raise RuntimeError('error message')
except Exception as err:
  print('Had an error:', err)

    evince -p 199 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 237]

The handle is closed whether there
is an error in the with block or
not.

$ python3 contextlib_closing.py

Normal Example:
  __init__()
  inside with statement: open
  close()
  outside with statement: closed

Error handling example:
  __init__()
  raising from inside with statement
  close()
  Had an error: error message


3.4.5 Ignoring Exceptions

It is frequently useful to ignore
exceptions raised by libraries,
because the error indicates that
the desired state has already been
achieved or can otherwise be
ignored. The most common way to
ignore exceptions is with a
try:except statement that includes
only a pass statement in the except
block.

Listing 3.61: contextlib_ignore_error.py

import contextlib

class NonFatalError(Exception):
  pass

def non_idempotent_operation():
  raise NonFatalError(
    'The operation failed because of existing state'
  )

try:
  print('trying non-idempotent operation')
  non_idempotent_operation()
  print('succeeded!')
except NonFatalError:
  pass

print('done')

    evince -p 200 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 238]

In this case, the operation fails
and the error is ignored.

$ python3 contextlib_ignore_error.py

trying non-idempotent operation
done

The try:except form can be replaced
with contextlib.suppress() to more
explicitly suppress a class of
exceptions happening anywhere
within the with block.

Listing 3.62: contextlib_suppress.py

import contextlib

class NonFatalError(Exception):
  pass

def non_idempotent_operation():
  raise NonFatalError(
    'The operation failed because of existing state'
  )

with contextlib.suppress(NonFatalError):
  print('trying non-idempotent operation')
  non_idempotent_operation()
  print('succeeded!')

print('done')

In this updated version, the
exception is discarded entirely.

    evince -p 201 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 239]

$ python3 contextlib_suppress.py

trying non-idempotent operation
done


3.4.6 Redirecting Output Streams

Poorly designed library code may
write directly to sys.stdout or
sys.stderr, without providing
arguments to configure different
output destinations. The
redirect_stdout() and
redirect_stderr() context managers
can be used to capture output from
these kinds of functions, for which
the source cannot be changed to
accept a new output argument.

Listing 3.63: contextlib_redirect.py

from contextlib import redirect_stdout, redirect_stderr
import io
import sys

def misbehaving_function(a):
  sys.stdout.write('(stdout) A: {!r}\n'.format(a))
  sys.stderr.write('(stderr) A: {!r}\n'.format(a))

capture = io.StringIO()
with redirect_stdout(capture), redirect_stderr(capture):
  misbehaving_function(5)

print(capture.getvalue())

In this example,
misbehaving_function() writes to
both stdout and stderr, but the two
context managers send that output
to the same io.StringIO instance,
where it is saved for later use.

$ python3 contextlib_redirect.py

(stdout) A: 5
(stderr) A: 5

  note: Both redirect_stdout() and
  redirect_stderr() modify the
  global state by replacing objects
  in the sys (page 1178) module;
  for this reason, they should be
  used with care. The functions are
  not really thread-safe, so
  calling them in a multithreaded
  application will have
  nondeterministic results. They
  also may interfere with other
  operations that expect the
  standard output streams to be
  attached to terminal devices.

    evince -p 202 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 240]

3.4.7 Dynamic Context Manager Stacks

Most context managers operate on
one object at a time, such as a
single file or database handle. In
these cases, the object is known in
advance and the code using the
context manager can be built around
that one object. In other cases, a
program may need to create an
unknown number of objects within a
context, with all of those objects
expected to be cleaned up when
control flow exits the context.
ExitStack was created to handle
these more dynamic cases.

An ExitStack instance maintains a
stack data structure of cleanup
callbacks. The callbacks are
populated explicitly within the
context, and any registered
callbacks are called in the reverse
order when control flow exits the
context. The result is similar to
having multiple nested with
statements, except they are
established dynamically.


3.4.7.1 Stacking Context Managers

Several approaches may be used to
populate the ExitStack. This
example uses enter_ context() to
add a new context manager to the
stack.

Listing 3.64: contextlib_exitstack_enter_context.py

import contextlib

@contextlib.contextmanager
def make_context(i):
  print('{} entering'.format(i))
  yield {}
  print('{} exiting'.format(i))

def variable_stack(n, msg):
  with contextlib.ExitStack() as stack:
    for i in range(n):
      stack.enter_context(make_context(i))
    print(msg)

variable_stack(2, 'inside context')

enter_context() first calls
__enter__() on the context manager.
It then registers its __exit__()
method as a callback to be invoked
as the stack is undone.

$ python3 contextlib_exitstack_enter_context.py

0 entering
1 entering
inside context
1 exiting
0 exiting

    evince -p 203 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 241]

The context managers given to
ExitStack are treated as though
they appear within a series of
nested with statements. Errors that
happen anywhere within the context
propagate through the normal error
handling of the context managers.
The following context manager
classes illustrate the way errors
propagate.

Listing 3.65: contextlib_context_managers.py

import contextlib

class Tracker:
  "Base class for noisy context managers."

  def __init__(self, i):
    self.i = i

  def msg(self, s):
    print('{}({}): {}'.format(
    self.__class__.__name__, self.i, s))

  def __enter__(self):
    self.msg('entering')

class HandleError(Tracker):
  "If an exception is received, treat it as handled."

  def __exit__(self, *exc_details):
    received_exc = exc_details[1] is not None
    if received_exc:
      self.msg('handling exception {!r}'.format(
        exc_details[1]))
    self.msg('exiting {}'.format(received_exc))
    # Return a boolean value indicating whether the exception
    # was handled.
    return received_exc

class PassError(Tracker):
  "If an exception is received, propagate it."

  def __exit__(self, *exc_details):
    received_exc = exc_details[1] is not None
    if received_exc:
      self.msg('passing exception {!r}'.format(
        exc_details[1]))
    self.msg('exiting')
    # Return False, indicating any exception was not handled.
    return False

class ErrorOnExit(Tracker):
  "Cause an exception."

  def __exit__(self, *exc_details):
    self.msg('throwing error')
    raise RuntimeError('from {}'.format(self.i))

class ErrorOnEnter(Tracker):
  "Cause an exception."

  def __enter__(self):
    self.msg('throwing error on enter')

  raise RuntimeError('from {}'.format(self.i))
    def __exit__(self, *exc_info):
    self.msg('exiting')

    evince -p 204 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 242]

The following examples using these
classes are based on
variable_stack(), which uses the
context managers passed to
construct an ExitStack, building up
the overall context in a
step-by-step manner. The examples
pass different context managers to
explore the error handling
behavior. The first example
presents the normal case of no
exceptions.

print('No errors:')
variable_stack([
  HandleError(1),
  PassError(2),
])

The next example illustrates
handling exceptions within the
context managers at the end of the
stack, in which all of the open
contexts are closed as the stack is
unwound.

print('\nError at the end of the context stack:')
variable_stack([
  HandleError(1),
  HandleError(2),
  ErrorOnExit(3),
])

In the next example, exceptions are
handled within the context managers
in the middle of the stack. The
error does not occur until some
contexts are already closed, so
those contexts do not see the
error.

print('\nError in the middle of the context stack:')
variable_stack([
  HandleError(1),
  PassError(2),
  ErrorOnExit(3),
  HandleError(4),
])

    evince -p 205 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 243]

The final example shows the case in
which the exception remains
unhandled and propagates up to the
calling code.

try:
  print('\nError ignored:')
  variable_stack([
    PassError(1),
    ErrorOnExit(2),
  ])
except RuntimeError:
  print('error handled outside of context')

If any context manager in the stack
receives an exception and returns a
True value, it prevents that
exception from propagating up to
any other context managers.

$ python3 contextlib_exitstack_enter_context_errors.py

No errors:
  HandleError(1): entering
  PassError(2): entering
  PassError(2): exiting
  HandleError(1): exiting False
  outside of stack, any errors were handled

Error at the end of the context stack:
  HandleError(1): entering
  HandleError(2): entering
  ErrorOnExit(3): entering
  ErrorOnExit(3): throwing error
  HandleError(2): handling exception RuntimeError('from 3',)
  HandleError(2): exiting True
  HandleError(1): exiting False
  outside of stack, any errors were handled

Error in the middle of the context stack:
  HandleError(1): entering
  PassError(2): entering
  ErrorOnExit(3): entering
  HandleError(4): entering
  HandleError(4): exiting False
  ErrorOnExit(3): throwing error
  PassError(2): passing exception RuntimeError('from 3',)
  PassError(2): exiting
  HandleError(1): handling exception RuntimeError('from 3',)
  HandleError(1): exiting True
  outside of stack, any errors were handled

Error ignored:
  PassError(1): entering
  ErrorOnExit(2): entering
  ErrorOnExit(2): throwing error
  PassError(1): passing exception RuntimeError('from 2',)
  PassError(1): exiting
error handled outside of context

    evince -p 206 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 244]

3.4.7.2 Arbitrary Context Callbacks

ExitStack also supports arbitrary
callbacks for closing a context,
making it easy to clean up
resources that are not controlled
via a context manager.

Listing 3.66: contextlib_exitstack_callbacks.py

import contextlib

def callback(*args, **kwds):
  print('closing callback({}, {})'.format(args, kwds))

with contextlib.ExitStack() as stack:
  stack.callback(callback, 'arg1', 'arg2')
  stack.callback(callback, arg3='val3')

Just as with the __exit__() methods
of full context managers, the
callbacks are invoked in the
reverse order that they are
registered.

$ python3 contextlib_exitstack_callbacks.py

closing callback((), {'arg3': 'val3'})
closing callback(('arg1', 'arg2'), {})

The callbacks are invoked
regardless of whether an error
occurred, and they are not given
any information about whether an
error occurred. Their return value
is ignored.

Listing 3.67: contextlib_exitstack_callbacks_error.py

import contextlib

def callback(*args, **kwds):
  print('closing callback({}, {})'.format(args, kwds))

try:
  with contextlib.ExitStack() as stack:
    stack.callback(callback, 'arg1', 'arg2')
    stack.callback(callback, arg3='val3')
    raise RuntimeError('thrown error')
except RuntimeError as err:
  print('ERROR: {}'.format(err))

    evince -p 207 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 245]

Because they do not have access to
the error, callbacks are unable to
prevent exceptions from propagating
through the rest of the stack of
context managers.

$ python3 contextlib_exitstack_callbacks_error.py

closing callback((), {'arg3': 'val3'})
closing callback(('arg1', 'arg2'), {})
ERROR: thrown error

Callbacks offer a convenient way to
clearly define cleanup logic
without the overhead of creating a
new context manager class. To
improve code readability, that
logic can be encapsulated in an
inline function, and callback() can
be used as a decorator.

Listing 3.68: contextlib_exitstack_callbacks_decorator.py

import contextlib

with contextlib.ExitStack() as stack:

  @stack.callback
  def inline_cleanup():
    print('inline_cleanup()')
    print('local_resource = {!r}'.format(local_resource))

  local_resource = 'resource created in context'
  print('within the context')

There is no way to specify the
arguments for functions registered
using the decorator form of
callback(). However, if the cleanup
callback is defined inline, scope
rules give it access to variables
defined in the calling code.

$ python3 contextlib_exitstack_callbacks_decorator.py

within the context
inline_cleanup()
local_resource = 'resource created in context'

    evince -p 208 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 246]

3.4.7.3 Partial Stacks

Sometimes when building complex
contexts, it is useful to be able
to abort an operation if the
context cannot be completely
constructed, but to delay the
cleanup of all resources until a
later time if they can all be set
up properly. For example, if an
operation needs several long-lived
network connections, it may be best
to not start the operation if one
connection fails. However, if all
of the connections can be opened,
they need to stay open longer than
the duration of a single context
manager. The pop_all() method of
ExitStack can be used in this
scenario.

pop_all() clears all of the context
managers and callbacks from the
stack on which it is called, and
returns a new stack prepopulated
with those same context managers
and callbacks. The close() method
of the new stack can be invoked
later, after the original stack is
gone, to clean up the resources.

Listing 3.69: contextlib_exitstack_pop_all.py

import contextlib

from contextlib_context_managers import *

def variable_stack(contexts):
  with contextlib.ExitStack() as stack:
    for c in contexts:
      stack.enter_context(c)
    # Return the close() method of a new stack as a clean-up
    # function.
    return stack.pop_all().close
  # Explicitly return None, indicating that the ExitStack could
  # not be initialized cleanly but that cleanup has already
  # occurred.
  return None

print('No errors:')
cleaner = variable_stack([
  HandleError(1),
  HandleError(2),
])
cleaner()

print('\nHandled error building context manager stack:')
try:
  cleaner = variable_stack([
    HandleError(1),
    ErrorOnEnter(2),
  ])
except RuntimeError as err:
  print('caught error {}'.format(err))

else:
  if cleaner is not None:
    cleaner()
  else:
    print('no cleaner returned')

print('\nUnhandled error building context manager stack:')
try:
  cleaner = variable_stack([
    PassError(1),
    ErrorOnEnter(2),
  ])
except RuntimeError as err:
  print('caught error {}'.format(err))
else:
  if cleaner is not None:
    cleaner()
  else:
    print('no cleaner returned')

    evince -p 209 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 247]

This example uses the same context
manager classes defined earlier,
but ErrorOnEnter produces an error
on __enter__() instead of
__exit__(). Inside
variable_stack(), if all of the
contexts are entered without error,
then the close() method of a new
ExitStack is returned. If a handled
error occurs, variable_stack()
returns None to indicate that the
cleanup work has already been done.
If an unhandled error occurs, the
partial stack is cleaned up and the
error is propagated.

$ python3 contextlib_exitstack_pop_all.py

No errors:
  HandleError(1): entering
  HandleError(2): entering
  HandleError(2): exiting False
  HandleError(1): exiting False

Handled error building context manager stack:
  HandleError(1): entering
  ErrorOnEnter(2): throwing error on enter
  HandleError(1): handling exception RuntimeError('from 2',)
  HandleError(1): exiting True
no cleaner returned

Unhandled error building context manager stack:
  PassError(1): entering
  ErrorOnEnter(2): throwing error on enter
  PassError(1): passing exception RuntimeError('from 2',)
  PassError(1): exiting
caught error from 2

    evince -p 210 ~/Empire/Doks/Comp/lang/py/lib/pystdlib3-byex_2017.pdf &
      [-p 248]

  Tip: Related Reading

  - Standard library documentation
    for contextlib.12
  - PEP 34313: The with statement.
  - Context Manager Types14:
    Description of the context
    manager API from the standard
    library documentation.
  - with Statement Context
    Managers15: Description of the
    context manager API from the
    Python Reference Guide.
  - Resource management in Python
    3.3, or contextlib.ExitStack
    FTW!16 : Description of using
    ExitStack to deploy safe code
    from Barry Warsaw.


12:
https://docs.python.org/3.5/library/contextlib.html
13:
www.python.org/dev/peps/pep-0343
14:
https://docs.python.org/library/stdtypes.html#typecontextmanager
15:
https://docs.python.org/reference/datamodel.html#context-managers
16:
www.wefearchange.org/2013/05/resource-management-in-python-33-or.html


___MPyCb=Funct
    ___Functional ___Reactive
    evince -p 349 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 378]

Chapter 8 - Functional and Reactive Programming Features


In this chapter, we'll look at the
following recipes:

- Writing generator functions with
  the yield statement
- Using stacked generator
  expressions
- Applying transformations to a
  collection
- Picking a subset - three ways to
  filter
- Summarizing a collection - how to
  reduce
- Combining map and reduce
  transformations
- Implementing "there exists"
  processing
- Creating a partial function
- Simplifying complex algorithms
  with immutable data structures
- Writing recursive generator
  functions with the yield from
  statement


Introduction

The idea of functional programming
is to focus on writing small,
expressive functions that perform
the required data transformations.
Combining functions can often
create code which is more succinct
and expressive than long strings of
procedural statements or the
methods of complex, stateful
objects. Python allows all three
kinds of programming.

    evince -p 350 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 379]

Conventional mathematics defines
many things as functions. Multiple
functions are combined to build up
a complex result from previous
transformations. For example, we
might have two functions, f(x) and
g(y), that need to be combined to
create a useful result:

y = f(x)
z = g(y)

Ideally, we can create a composite
function from these two functions:

z = (g ∘ f)(x)

Using a composite function, (g ∘
f), can help to clarify how a
program works. It allows us to take
a number of small details and
combine them into a larger
knowledge chunk.

Since programming often works with
collections of data, we'll often be
applying a function to a whole
collection. This fits nicely with
the mathematical idea of a set
builder or set comprehension.

There are three common patterns for
applying one function to a set of
data:

- Mapping: This applies a function
  to all elements of a collection
  {M(x): x ∈ C}. We apply some
  function, M, to each item, x, of
  a larger collection, C.
- Filtering: This uses a function
  to select elements from a
  collection. {x:c ∈ C if F(x)}. We
  use a function, F, to determine
  whether to pass or reject an
  item, x, from a larger
  collection, C.
- Reducing: This summarizes a
  collection. The details vary, but
  one of the most common reductions
  is creating a sum of all items,
  x, in a collection, C: Sigma ....

We'll often combine these patterns
to create more complex
applications. What's important here
is that small functions, such as
M(x) and F(x), are combined via
higher-order functions such as
mapping and filtering. The combined
operation can be sophisticated even
though the individual pieces are
quite simple.

The idea of reactive programming is
to have processing rules that are
evaluated when the inputs become
available or change. This fits with
the idea of lazy programming. When
we define lazy properties of a
class definition, we've created
reactive programs.

    evince -p 351 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 380]

Reactive programming fits with
functional programming because
there may be multiple
transformations required to react
to a change in the input values.
Often, this is most clearly
expressed as functions that are
combined or stacked into a
composite function that responds to
change. See the Using properties
for lazy attributes recipe in
Chapter 6, Basics of Classes and
Objects, for some examples of
reactive class design.


Writing generator functions with the yield statement

Most of the recipes we've looked at
have been designed to work with all
of the items in a single
collection. The approach has been
to use a for statement to step
through each item within the
collection, either mapping the
value to a new item, or reducing
the collection to some kind of
summary value.

Producing a single result from a
collection is one of two ways to
work with a collection. The
alternative is to produce
incremental results instead of a
single result.

This approach is very helpful in
the cases where we can't fit an
entire collection in memory. For
example, analyzing gigantic web log
files is best done in small doses
rather than by creating an
in-memory collection.

Is there some way to disentangle
the collection structure from the
processing function? Can we yield
results from processing as soon as
each individual item is available?


Getting ready

We'll look at some web log data
that has date-time string values.
We need to parse these to create
proper datetime objects. To keep
things focused in this recipe,
we'll use a simplified log produced
by Flask.

The entries start out as lines of
text that look like this:

[2016-05-08 11:08:18,651] INFO in ch09_r09: Sample Message One
[2016-05-08 11:08:18,651] DEBUG in ch09_r09: Debugging
[2016-05-08 11:08:18,652] WARNING in ch09_r09: Something might have gone wrong

    evince -p 352 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 381]

We've seen other examples of
working with this kind of log in
the Using more complex structures -
maps of lists recipe in Chapter 7,
More Advanced Class Design. Using
REs from the String parsing with
regular expressions recipe in
Chapter 1, Numbers, Strings, and
Tuples, we can decompose each line
to look like the following
collection of rows:

>>> data = [
...   ('2016-04-24 11:05:01,462', 'INFO', 'module1', 'Sample Message One'),
...   ('2016-04-24 11:06:02,624', 'DEBUG', 'module2', 'Debugging'),
...   ('2016-04-24 11:07:03,246', 'WARNING', 'module1', 'Something might have gone wrong')
... ]

We can't use ordinary string
parsing to convert the complex
date-time stamp into something more
useful. We can, however, write a
generator function which can
process each row of the log,
producing a more useful
intermediate data structure.

A generator function is a function
that uses a yield statement. When a
function has a yield, it builds the
results incrementally, yielding
each individual value in a way that
can be consumed by a client. The
consumer might be a for statement
or it might be another function
that needs a sequence of values.


How to do it...

- 1. This requires the datetime
  module:

import datetime

- 2. Define a function that
  processes a source collection:

def parse_date_iter(source):

We've included the suffix _iter as
a reminder that this function will
be an iterable object, not a simple
collection.

- 3. Include a for statement that
  visits each item in the source
  collection:

for item in source:

    evince -p 353 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 382]

- 4. The body of the for statement can map the item to a new item:

date = datetime.datetime.strptime(
  item[0],
  "%Y-%m-%d %H:%M:%S,%f")
new_item = (date,)+item[1:]

In this case, we mapped a single
field from string to datetime
object. The variable date is built
from the string in item[0].

Then we mapped the log message
three-tuple to a new tuple,
replacing the date string with the
proper datetime object. Since the
value of the item is a tuple, we
created a singleton tuple with
(date,) and then concatenated this
with the item[1:] tuple.

- 5. Yield the new item with a yield statement:

yield new_item

The whole construct looks like
this, properly indented:

import datetime
def parse_date_iter(source):
  for item in source:
    date = datetime.datetime.strptime(
      item[0],
      "%Y-%m-%d %H:%M:%S,%f")
    new_item = (date,)+item[1:]
    yield new_item

The parse_date_iter() function
expects an iterable input object. A
collection is an example of an
iterable object. More importantly,
though, other generators are also
iterable. We can leverage this to
build stacks of generators which
process data from other generators.

This function doesn't create a
collection. It yields each item, so
that the items can be processed
individually. The source collection
is consumed in small pieces,
allowing huge amounts of data to be
processed. In some recipes, the
data will start out from an
in-memory collection. In later
recipes, we'll work with data from
external files - processing
external files benefits the most
from this technique.

    evince -p 354 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 383]

Here's how we can use this function:

>>> from pprint import pprint
>>> from ch08_r01 import parse_date_iter
>>> for item in parse_date_iter(data):
...   pprint(item)
(datetime.datetime(2016, 4, 24, 11, 5, 1, 462000),
 'INFO',
 'module1',
 'Sample Message One')
(datetime.datetime(2016, 4, 24, 11, 6, 2, 624000),
 'DEBUG',
 'module2',
 'Debugging')
(datetime.datetime(2016, 4, 24, 11, 7, 3, 246000),
 'WARNING',
 'module1',
 'Something might have gone wrong')

We've used a for statement to
iterate through the results of the
parse_date_iter() function, one
item at a time. We've used the
pprint() function to display each
item.

We could also collect the items
into a proper list using something
like this:

>>> details = list(parse_date_iter(data))

In this example, the list()
function consumes all of the items
produced by the parse_date_iter()
function. It's essential to use a
function such as list() or a for
statement to consume all of the
items from the generator. A
generator is a relatively passive
construct - until data is demanded,
it doesn't do any work.

If we don't actively consume the
data, we'll see something like
this:

>>> parse_date_iter(data)
<generator object parse_date_iter at 0x10167ddb0>

The value of the parse_date_iter()
function is a generator. It's not a
collection of items, but a function
that will produce items on demand.

    evince -p 355 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 384]

How it works...

Writing generator functions can
change the way we perceive an
algorithm. There are two common
patterns: mappings and reductions.
A mapping transforms each item to a
new item, perhaps computing some
derived value. A reduction
accumulates a summary such as a
sum, mean, variance, or hash from
the source collection. These can be
decomposed into the item-by-item
transformation or filter, separate
from the overall loop that handles
the collection.

Python has a sophisticated
construct called an iterator which
lies at the heart of generators and
collections. An iterator will
provide each value from a
collection while doing all of the
internal bookkeeping required to
maintain the state of the process.
A generator function behaves like
an iterator - it provides a
sequence of values and maintains
its own internal state.

Consider the following common piece
of Python code:

for i in some_collection:
  process(i)

Behind the scenes, something like
the following is going on:

the_iterator = iter(some_collection)
try:
  while True:
    i = next(the_iterator)
    process(i)
except StopIteration:
  pass

Python evaluates the iter()
function on a collection to create
an iterator object for that
collection. The iterator is bound
to the collection and maintains
some internal state information.
The code uses next() on the
iterator to get each value. When
there are no more values, the
iterator raises the StopIteration
exception.

Each of Python's collections can
produce an iterator. The iterator
produced by a Sequence or Set will
visit each item in the collection.
The iterator produced by a Mapping
will visit each key for the
mapping. We can use the values()
method of a mapping to iterate over
the values instead of the keys. We
can use the items() method of a
mapping to visit a sequence of
(key, value) two-tuples. The
iterator for a file will visit each
line in the file.

    evince -p 356 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 385]

The iterator concept can also be
applied to functions. A function
with a yield statement is called a
generator function. It fits the
template for an iterator. To do
this, the generator returns itself
in response to the iter() function.
In response to the next() function,
it yields the next value.

When we apply list() to a
collection or a generator function,
the same essential mechanism used
by the for statement gets the
individual values. The iter() and
next() functions are used by list()
to get the items. The items are
then turned into a sequence.

Evaluating next() on a generator
function is interesting. The
generator function is evaluated
until it reaches a yield statement.
This value is the result of next().
Each time next() is evaluated, the
function resumes processing after
the yield statement and continues
to the next yield statement.

Here's a small function which
yields two objects:

>>> def gen_func():
...   print("pre-yield")
...   yield 1
...   print("post-yield")
...   yield 2

Here's what happens when we
evaluate next(). On the generator
this function produces:

>>> y = gen_func()
>>> next(y)
pre-yield
1
>>> next(y)
post-yield
2

The first time we evaluated next(),
the first print() function was
evaluated, then the yield statement
produced a value. The function's
processing was suspended and the
>>> prompt was given. The second
time we evaluated the next()
function, the statements between
the two yield statements were
evaluated. The function was again
suspended and a >>> prompt will be
displayed.

    evince -p 357 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 386]

What happens next? We're out of yield statements:

>>> next(y)
Traceback (most recent call last):
  File "<pyshell...>", line 1, in <module>
    next(y)
StopIteration

The StopIteration exception is
raised at the end of a generator
function.


There's more...

The core value of generator
functions comes from being able to
break complex processing into two
parts:

- The transformation or filter to
  apply
- The source set of data with which
  to work

Here's an example of using a
generator to filter data. In this
case, we'll filter the input values
and keep only the prime numbers,
rejecting all composite numbers.

We can write the processing out as
a Python function like this:

def primeset(source):
  for i in source:
    if prime(i):
      yield prime

For each value in the source, we'll
evaluate the prime() function. If
the result is true, we'll yield the
source value. If the result is
false, the source value will be
rejected. We can use primeset()
like this:

p_10 = set(primeset(range(2,2000000)))

The primeset() function will yield
individual prime values from a
source collection. The source
collection will be the integers in
the range of 2 to 2 million. The
result is a set object built from
the values provided.

All that's missing from this is the
prime() function to determine
whether a number is prime. We'll
leave that as an exercise for the
reader.

    evince -p 358 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 387]

Mathematically, it's common to see
set builder or set comprehension
notation to specify a rule for
building one set from another.

We might see something like this:

P10 = {i: i ∈ ℕ ∧ 2 ≤ 1 < 2,000,000 if P(i)}

This tells us that P10 is the set
of all numbers, i, in the set of
natural numbers, ℕ, and between 2
and 2 million if P(i) is true. This
defines a rule for building a set.

We can write this in Python too:

p_10 = {i for i in range(2,2000000) if prime(i)}

This is Python notation for the
subset of prime numbers. The
clauses are rearranged slightly
from the mathematical abstraction,
but all of the same essential parts
of the expression are present.

When we start looking at generator
expressions like this, we can see
that a great deal of programming
fits some common overall patterns:

- Map: {m(x): x ∈ S} becomes (m(x)
  for x in S).
- Filter: {x: x ∈ S if f(x)}
  becomes (x for x in S if f(x)).
- Reduce: This is a bit more
  complex, but common reductions
  include sums and counts. Sigma...
  is sum(x for x in S). Other
  common reductions include finding
  the maximum or the minimum of a
  set of data.

We can also write these various
higher-level functions using the
yield statement. Here's the
definition of a generic mapping:

def map(m, S):
  for s in S:
    yield m(s)

This function applies some other
function, m(), to each data element
in the source collection, S. The
result of the mapping function is
yielded as a sequence of result
values.

We can write a similar definition
for a generic filter function:

def filter(f, S):
  for s in S:
    if f(s):
      yield s

    evince -p 359 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 388]

As with the generic mapping, we
apply a function, f(), to each
element in the source collection,
S. Where the function is true, the
values are yielded. Where the
function is false, the values are
rejected.

We can use this to create a set of
primes like this:

p_10 = set(filter(prime, range(2,2000000)))

This will apply the prime()
function to the source range of
data. Note that we write just prime
- without () characters - because
we're naming the function, not
evaluating it. Each individual
value will be checked by the
prime() function. Those that pass
will be yielded to be assembled
into the final set. Those values
which are composite will be
rejected and won't wind up in the
final set.


See also

- In the Using stacked generator
  expressions recipe, we'll combine
  generator functions to build
  complex processing stacks from
  simple components.
- In the Applying transformations
  to a collection recipe, we'll see
  how the built-in map() function
  can be used to create complex
  processing from a simple function
  and an iterable source of data.
- In the Picking a subset - three
  ways to filter recipe, we'll see
  how the built-in filter()
  function can also be used to
  build complex processing from a
  simple function and an iterable
  source of data.
- See
  https://projecteuler.net/problem=10
  for a challenging problem related
  to prime numbers less than 2
  million. Parts of the problem
  seem obvious. It can be
  difficult, however, to test all
  of those numbers for being prime.


Using stacked generator expressions

In the Writing generator functions
with the yield statement recipe, we
created a simple generator function
that performed a single
transformation on a piece of data.
As a practical matter, we often
have several functions that we'd
like to apply to incoming data.

How can we stack or combine
multiple generator functions to
create a composite function?

    evince -p 360 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 389]

Getting ready

We have a spreadsheet that is used
to record fuel consumption on a
large sailboat. It has rows which
look like this:

___Insert_Figure

date
engine on
fuel height
engine off
fuel height
Other notes
10/25/2013 08:24
29
13:15
27
calm seas - anchor solomon's island
10/26/2013 09:12
27
18:25
22
choppy - anchor in jackson's creek

For more background on this data,
see the Slicing and dicing a list
recipe in Chapter 4, Builtin Data
Structures - list, set, dict.

As a sidebar, we can take the data
like this. We'll look at this in
detail in the Reading delimited
files with the csv module recipe in
Chapter 9, Input/Output, Physical
Format, and Logical Layout:

>>> from pathlib import Path
>>> import csv
>>> with Path('code/fuel.csv').open() as source_file:
...   reader = csv.reader(source_file)
...   log_rows = list(reader)
>>> log_rows[0]
['date', 'engine on', 'fuel height']
>>> log_rows[-1]
['', "choppy -- anchor in jackson's creek", '']

We've used the csv module to read
the log details. A csv.reader() is
an iterable object. In order to
collect the items into a single
list, we applied the list()
function to the generator function.
We printed at the first and last
item in the list to confirm that we
really have a listof-lists
structure.

We'd like to apply two
transformations to this
list-of-lists:

- Convert the date and two times
  into two date-time values
- Merge three rows into one row so
  that we have a simple
  organization to the data

    evince -p 361 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 390]

If we create a useful pair of
generator functions, we can have
software that looks like this:

total_time = datetime.timedelta(0)
total_fuel = 0
for row in date_conversion(row_merge(source_data)):
  total_time += row['end_time']-row['start_time']
  total_fuel += row['end_fuel']-row['start_fuel']

The combined generator functions,
date_conversion(row_merge(...)),
will yield a sequence of single
rows with starting information,
ending information, and notes. This
structure can easily be summarized
or analyzed to create simple
statistical correlations and
trends.


How to do it...

- 1. Define an initial reduce
  operation that combines rows. We
  have several ways to tackle this.
  One is to always group three rows
  together.

An alternative is to note that
column zero has data at the start
of a group; it's empty for the next
two lines of a group. This gives us
a slightly more general approach to
creating groups of rows. This is a
kind of head-tail merge algorithm.
We'll collect data and yield the
data each time we get to the head
of the next group:

def row_merge(source_iter):
  group = []
  for row in source_iter:
    if len(row[0]) != 0:
      if group:
        yield group
      group = row.copy()
    else:
      group.extend(row)
  if group:
    yield group

This algorithm uses len(row[0]) to
determine whether this is the head
of a group or a row in the tail of
the group. In the case of a head
row, any previous group is yielded.
After that has been consumed, the
value of the group collection is
reset to be the column data from
the head row.

    evince -p 362 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 391]

The rows in the tail of the group
are simply appended to the group
collection. When the data is
exhausted, there will - generally -
be one final group in the group
variable. If there's no data at
all, then the final value of group
will also be a zero-length list,
which should be ignored.

We'll address the copy() method
later. It's essential because we're
working with a list of lists data
structure and lists are mutable
objects. We can write processing
which changes the data structures,
making some processing awkward to
explain.

- 2. Define the various mapping
  operations that will be performed
  on the merged data. These apply
  to the data in the original row.
  We'll use separate functions to
  convert each of the two time
  columns and merge the times with
  the date column:

import datetime
def start_datetime(row):
  travel_date = datetime.datetime.strptime(row[0], "%m/%d/%y").date()
  start_time = datetime.datetime.strptime(row[1], "%I:%M:%S %p").time()
  start_datetime = datetime.datetime.combine(travel_date, start_time)
  new_row = row+[start_datetime]
  return new_row

def end_datetime(row):
  travel_date = datetime.datetime.strptime(row[0], "%m/%d/%y").date()
  end_time = datetime.datetime.strptime(row[4], "%I:%M:%S %p").time()
  end_datetime = datetime.datetime.combine(travel_date, end_time)
  new_row = row+[end_datetime]
  return new_row

We'll combine the date in column
zero with the time in column one to
create a starting datetime object.
Similarly, we'll combine the date
in column zero with the time in
column four to create an ending
datetime object.

These two functions have a lot of
overlaps and could be refactored
into a single function with the
column number as an argument value.
For now, however, our goal is to
write something that simply works.
Refactoring for efficiency can come
later.

    evince -p 363 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 392]

- 3. Define mapping operations that
  apply to the derived data.
  Columns eight and nine contain
  the date-time stamps:

for starting and ending.def duration(row):
  travel_hours = round((row[10]-row[9]).total_seconds()/60/60, 1)
  new_row = row+[travel_hours]
  return new_row

We've used the values created by
start_datetime and end_datetime as
inputs. We've computed the delta
time, which provides a result in
seconds. We converted seconds to
hours, which is a more useful unit
of time for this set of data.

- 4. Fold in any filters required
  to reject or exclude bad data. In
  this case, we have a header row
  that must be excluded:

def skip_header_date(rows):
  for row in rows:
    if row[0] == 'date':
      continue
    yield row

This function will reject any row
that has date in the first column.
The continue statement resumes the
for statement, skipping all other
statements in the body; it skips
the yield statement. All other rows
will be passed through this
process. The input is an iterable
and this generator will yield rows
that have not been transformed in
any way.

- 5. Combine the operations. We can
  either write a sequence of
  generator expressions or use the
  built-in map() function. Here's
  how it might look using generator
  expressions:

def date_conversion(source):
  tail_gen = skip_header_date(source)
  start_gen = (start_datetime(row) for row in tail_gen)
  end_gen = (end_datetime(row) for row in start_gen)
  duration_gen = (duration(row) for row in end_gen)
  return duration_gen

This operation consists of a series
of transformations. Each one does a
small transformation on one value
from the original collection of
data. It's relatively simple to add
operations or change operations,
since each one is defined
independently:

- The tail_gen generator yields
  rows after skipping the first row
  of the source
- The start_gen generator appends a
  datetime object to the end of
  each row with the start time
  built from strings into source
  columns
- The end_gen generator appends a
  datetime object to each row that
  has the end time built from
  strings
- The duration_gen generator
  appends a float object with the
  duration of the leg

    evince -p 364 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 393]

The output from this overall
date_conversion() function is a
generator. It can be consumed with
a for statement or a list can be
built from the items.


How it works...

When we write a generator function,
the argument value can be a
collection, or it can be another
kind of iterable. Since generator
functions are iterables, it becomes
possible to create a kind of
pipeline of generator functions.

Each function can embody a small
transformation that changes one
feature of the input to create the
output. We've then wrapped each of
these small transformations in
generator expressions. Because each
transformation is reasonably well
isolated from the others, we can
make changes to one without
breaking the entire processing
pipeline.

The processing works incrementally.
Each function is evaluated until it
yields a single value. Consider
this statement:

for row in date_conversion(row_merge(data)):
  print(row[11])

We've defined a composition of
several generators. This
composition uses a variety of
techniques:

- The row_merge() function is a
  generator which will yield rows
  of data. In order to yield one
  row, it will read four lines from
  the source, assemble a merged
  row, and yield it. Each time
  another row is required, it will
  read three more rows of input to
  assemble the output row.
- The date_conversion() function is
  a complex generator built from
  multiple generators.
- skip_header_date() is designed to
  yield a single value. Sometimes
  it will have to read two values
  from the source iterator. If an
  input row has date in column
  zero, the row is skipped. In that
  case, it will read the second
  value, getting another row from
  row_merge(); which must, in turn,
  read three more lines of input to
  produce a merged line of output.
  We've assigned the generator to
  the tail_gen variable.
- The start_gen, end_gen, and
  duration_gen generator
  expressions will apply relatively
  simple functions such as
  start_datetime() and
  end_datetime() to each row of its
  input, yielding rows with more
  useful data.

    evince -p 365 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 394]

The final for statement shown in
the example will be gathering
values from the date_conversion()
iterator by evaluating the next()
function repeatedly. Here's the
step by step view of what will
happen to create the needed result.
Note that this works on a very
small bit of data - each step makes
one small change:

- 1. The date_conversion() function
  result was the duration_gen
  object. For this to return a
  value, it needs a row from its
  source, end_gen. Once it has the
  data, it can apply the duration()
  function and yield the row.
- 2. The end_gen expression needs a
  row from its source, start_gen.
  It can then apply the
  end_datetime() function and yield
  the row.
- 3. The start_gen expression needs
  a row from its source, tail_gen.
  It can then apply the
  start_datetime() function and
  yield the row.
- 4. The tail_gen expression is
  simply the generator
  skip_header_date(). This function
  will read as many rows as
  required from its source until it
  finds a row where column zero is
  not the column header date. It
  yields one non-date row. The
  source for this is the output
  from the row_merge() function.
- 5. The row_merge() function will
  read multiple rows from its
  source until it can assemble a
  collection of rows that fits the
  required pattern. It will yield a
  combined row that has some text
  in column zero, followed by rows
  that have no text in column zero.
  The source for this is a
  list-of-lists collection of the
  raw data.
- 6. The collection of rows will be
  processed by a for statement
  inside the row_merge() function.
  This processing will implicitly
  create an iterator for the
  collection so that each
  individual row is yielded as
  needed by the body of the
  row_merge() function.

Each individual row of data will
pass through this pipeline of
steps. Some stages of the pipeline
will consume multiple source rows
for a single result row,
restructuring the data as it is
processed. Other stages consume a
single value.

    evince -p 366 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 395]

This example relies on
concatenating items into a long
sequence of values. Items are
identified by position. A small
change to the order of the stages
in the pipeline will alter the
positions of the items. There are a
number of ways to improve on this
that we'll look at next.

What's central to this is that only
individual rows are being
processed. If the source is a
gigantic collection of data, the
processing can proceed very
quickly. This technique allows a
small Python program to process
vast volumes of data quickly and
simply.


There's more...

In effect, a set of interrelated
generators is a kind of composite
function. We might have several
functions, defined separately like
this:

y = f(x)
z = g(y)

We can combine them by applying the
results of the first function to
the second function:

z = g(f(x))

This can become awkward as the
number of functions grows. When we
use this pair of functions in
multiple places, we break the Don't
Repeat Yourself (DRY) principle.
Having multiple copies of this
complex expression isn't ideal.

What we'd like to have is a way to
create a composite function -
something like this:

z = (g ∘ f)(x)

Here, we've defined a new function,
(g ∘ f), that combines the two
original functions into a new,
single, composite function. We can
now modify this composite to add or
change features.

This concept drives the definition
of the composite date_conversion()
function. This function is composed
of a number of functions, each of
which can be applied to items of
collections. If we need to make
changes, we can easily write more
simple functions and drop them into
the pipeline defined by the
date_conversion() function.

    evince -p 367 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 396]

We can see some slight differences
among the functions in the
pipeline. We have some type
conversions. However, the duration
calculation isn't really a type
conversion. It's a separate
computation that's based on the
results of the date conversions. If
we want to compute fuel use per
hour, we'd need to add several more
calculations. None of these
additional summaries is properly
part of date conversion.

We should really break the
high-level data_conversion() into
two parts. We should write another
function that does duration and
fuel use calculations, named
fuel_use(). This other function can
then wrap date_conversion().

We might aim for something like this:

for row in fuel_use(date_conversion(row_merge(data))):
  print(row[11])

We now have a very sophisticated
computation that's defined in a
number of very small and (almost)
completely independent chunks. We
can modify one piece without having
to think deeply about how the other
pieces work.


Namespace instead of list

An important change is to stop
avoiding the use of a simple list
for the data values. Doing
computations on row[10] is a
potential disaster in the making.
We should properly convert the
input data into some kind of
namespace.

A namedtuple can be used. We'll
look at that in the Simplifying
complex algorithms with immutable
data structures recipe.

A SimpleNamespace can, in some
ways, further simplify this
processing. A SimpleNamespace is a
mutable object, and can be updated.
It's not always the best idea to
mutate an object. It has the
advantage of being simple, but it
can also be slightly more difficult
to write tests for state changes in
mutable objects.

A function such as make_namespace()
can provide a set of names instead
of positions. This is a generator
that must be used after the rows
are merged, but before any of the
other processing:

from types import SimpleNamespace

def make_namespace(merge_iter):
  for row in merge_iter:
    ns = SimpleNamespace(
      date = row[0],
      start_time = row[1],
      start_fuel_height = row[2],
      end_time = row[4],
      end_fuel_height = row[5],
      other_notes = row[7]
    )
    yield ns

    evince -p 368 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 397]

This will produce an object that
allows us to write row.date instead
of row[0]. This, of course, will
change the definitions for the
other functions, including
start_datetime(), end_datetime(),
and duration().

Each of these functions can emit a
new SimpleNamespace object instead
of updating the list of values that
represents each row. We can then
write functions that look like
this:

def duration(row_ns):
  travel_time = row_ns.end_timestamp - row_ns.start_timestamp
  travel_hours = round(travel_time.total_seconds()/60/60, 1)
  return SimpleNamespace(
    **vars(row_ns),
    travel_hours=travel_hours
  )

Instead of processing a row as a
list object, this function
processes a row as a
SimpleNamespace object. The columns
have clear and meaningful names
such as row_ns.end_timestamp
instead of the cryptic row[10].

There's a three-part process to
building a new SimpleNamespace from
an old namespace:

- 1. Use the vars() function to
  extract the dictionary inside the
  SimpleNamespace instance.
- 2. Use the **vars(row_ns) object
  to build a new namespace based on
  the old namespace.
- 3. Any additional keyword
  parameters such as travel_hours =
  travel_hours provides additional
  values that will load the new
  object.

The alternative is to update the
namespace and return the updated
object:

def duration(row_ns):
  travel_time = row_ns.end_timestamp - row_ns.start_timestamp
  row_ns.travel_hours = round(travel_time.total_seconds()/60/60, 1)
  return row_ns

    evince -p 369 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 398]

This has the advantage of being
slightly simpler. The disadvantage
is the small consideration that
stateful objects can sometimes be
confusing. When modifying an
algorithm, it's possible to fail to
set attributes in the proper order
so that lazy (or reactive)
programming operates properly.

While stateful objects are common,
they should always be viewed as one
of two alternatives. An immutable
namedtuple might be a better choice
than a mutable SimpleNamespace.


See also

- See the Writing generator
  functions with the yield
  statement recipe for an
  introduction to generator
  functions
- See the Slicing and dicing a list
  recipe in Chapter 4, Built-in
  Data Structures - list, set,
  dict, for more information on the
  fuel consumption dataset
- See the Combining map and reduce
  transformations recipe for
  another way to combine operations


Applying transformations to a collection

In the Writing generator functions
with the yield statement recipe, we
looked at writing a generator
function. The examples we saw
combined two elements: a
transformation and a source of
data. They generally look like
this:

for item in source:
  new_item = some transformation of item
  yield new_item

This template for writing a
generator function isn't a
requirement. It's merely a common
pattern. There's a transformation
process buried inside a for
statement. The for statement is
largely boilerplate code. We can
refactor this to make the
transformation function explicit
and separate from the for
statement.

In the Using stacked generator
expressions recipe, we defined a
start_datetime() function which
computed a new datetime object from
the string values in two separate
columns of the source collection of
data.

    evince -p 370 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 399]

We could use this function in a
generator function's body like
this:

def start_gen(tail_gen):
  for row in tail_gen:
    new_row = start_datetime(row)
    yield new_row

This function applies the
start_datetime() function to each
item in a source of data, tail_gen.
Each resulting row is yielded so
that another function or a for
statement can consume it.

In the Using stacked generator
expressions recipe, we looked at
another way to apply these
transformation functions to a
larger collection of data. In this
example, we used a generator
expression. The code looks like
this:

start_gen = (start_datetime(row) for row in tail_gen)

This applies the start_datetime()
function to each item in a source
of data, tail_gen. Another function
or for statement can consume the
values available in the start_gen
iterable.

Both the complete generator
function and the shorter generator
expression are essentially the same
thing with slightly different
syntax. Both of these are parallel
to the mathematical notion of a set
builder or set comprehension. We
could describe this operation
mathematically as:

s = [S(r): r ∈ T]

In this expression, S is the
start_datetime() function and T is
the sequence of values called
tail_gen. The resulting sequence is
the value of S(r), where each value
for r is an element of the set T.

Both generator functions and
generator expressions have similar
boilerplate code. Can we simplify
these?


Getting ready...

We'll look at the web log data from
the Writing generator functions
with the yield statement recipe.
This had date as a string that we
would like to transform into a
proper timestamp.

    evince -p 371 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 400]

Here's the example data:

>>> data = [
...   ('2016-04-24 11:05:01,462', 'INFO', 'module1', 'Sample Message One'),
...   ('2016-04-24 11:06:02,624', 'DEBUG', 'module2', 'Debugging'),
...   ('2016-04-24 11:07:03,246', 'WARNING', 'module1', 'Something might have gone wrong')
... ]

We can write a function like this
to transform the data:

import datetime
def parse_date_iter(source):
  for item in source:
    date = datetime.datetime.strptime(
      item[0],
      "%Y-%m-%d %H:%M:%S,%f")
    new_item = (date,)+item[1:]
    yield new_item

This function will examine each
item in the source using a for
statement. The value in column zero
is a date string, which can be
transformed into a proper datetime
object. A new item, new_item, is
built from the datetime object and
the remaining items starting with
column one.

Because the function uses the yield
statement to produce results, it's
a generator function. We use it
with a for statement like this:

for row in parse_date_iter(data):
  print(row[0], row[3])

This statement will gather each
value as it's produced by the
generator function and print two of
the selected values.

The parse_date_iter() function has
two essential elements combined
into a single function. The outline
looks like this:

for item in source:
  new_item = transformation(item)
  yield new_item

The for and yield statements are
largely boilerplate code. The
transformation() function is a
really useful and interesting part
of this.

    evince -p 372 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 401]

How to do it...

- 1. Write the transformation
  function that applies to a single
  row of the data. This is not a
  generator, and doesn't use the
  yield statement. It simply
  revises a single item from a
  collection:

def parse_date(item):
  date = datetime.datetime.strptime(
    item[0],
    "%Y-%m-%d %H:%M:%S,%f")
  new_item = (date,)+item[1:]
  return new_item

This can be used in three ways:
statements, expressions, and the
map() function. Here's the explicit
for...yield pattern of statements:

for item in collection:
  new_item = parse_date(item)
  yield new_item

This uses a for statement to
process each item in the collection
using the isolated parse_date()
function. The second choice is a
generator expression that looks
like this:

(parse_date(item) for item in data)

This is a generator expression that
applies the parse_date() function
to each item. The third choice is
the map() function.

- 2. Use the map() function to
  apply the transformation to the
  source data.

map(parse_date, data)

We provide the name of the
function, parse_date, without any
() after the name. We aren't
applying the function at this time.
We're providing the name of the
object to the map() function to
apply the parse_date() function to
the iterable source of data, data.

We can use this as follows:

for row in map(parse_date, data):
  print(row[0], row[3])

    evince -p 373 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 402]

The map() function creates an
iterable object that applies the
parse_date() function to each item
in the data iterable. It yields
each individual item. It saves us
from having to write a generator
expression or a generator function.


How it works...

The map() function replaces some
common boilerplate code. We can
imagine that the definition looks
something like this:

def map(f, iterable):
  for item in iterable:
    yield f(item)

Or, we can imagine that it looks like this:

def map(f, iterable):
  return (f(item) for item in iterable)

Both of these definitions summarize
the core feature of the map()
function. It's handy shorthand that
eliminates some boilerplate code
for applying a function to an
iterable source of data.


There's more...

In this example, we've used the
map() function to apply a function
that takes a single parameter to
each individual item of a single
iterable. It turns out that the
map() function can do a bit more
than this.

Consider this function:

>>> def mul(a, b):
...   return a*b

And these two sources of data:

>>> list_1 = [2, 3, 5, 7]
>>> list_2 = [11, 13, 17, 23]

    evince -p 374 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 403]

We can apply the mul() function to
pairs drawn from each source of
data:

>>> list(map(mul, list_1, list_2))
[22, 39, 85, 161]

This allows us to merge two
sequences of values using different
kinds of operators. We can, for
example, build a mapping that
behaves like the built-in zip()
function.

Here's a mapping:

>>> def bundle(*args):
...   return args
>>> list(map(bundle, list_1, list_2))
[(2, 11), (3, 13), (5, 17), (7, 23)]

We needed to define a small helper
function, bundle(), that takes any
number of arguments, and creates a
tuple out of them.

Here's the zip function for
comparison:

>>> list(zip(list_1, list_2))
[(2, 11), (3, 13), (5, 17), (7, 23)]


See also...

- In the Using stacked generator
  expressions recipe, we looked at
  stacked generators. We built a
  composite function from a number
  of individual mapping operations
  written as generator functions.
  We also included a single filter
  in the stack.


Picking a subset - three ways to filter

In the Using stacked generator
expressions recipe, we wrote a
generator function that excluded
some rows from a set of data. We
defined a function like this:

def skip_header_date(rows):
  for row in rows:
    if row[0] == 'date':
      continue
    yield row

    evince -p 375 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 404]

When the condition is true - row[0]
is date - the continue statement
will skip the rest of the
statements in the body of the for
statement. In this case, there's
only a single statement, yield row.

There are two conditions:

- row[0] == 'date': The yield
  statement is skipped; the row is
  rejected from further processing
- row[0] != 'date': The yield
  statement means that the row will
  be passed on to the function or
  statement that's consuming the
  data

At four lines of code, this seems
long-winded. The for...if...yield
pattern is clearly boilerplate, and
only the condition is really
material in this kind of construct.

Can we express this more
succinctly?


Getting ready...

We have a spreadsheet that is used
to record fuel consumption on a
large sailboat. It has rows which
look like this:

___Insert_Figure

date
engine on
fuel height
engine off
fuel height
Other notes
10/25/2013 08:24
29
13:15
27
calm seas - anchor solomon's island
10/26/2013 09:12
27
18:25
22
choppy - anchor in jackson's creek

For more background on this data,
see the Slicing and dicing a list
recipe.

    evince -p 376 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 405]

In the Using stacked generator
expressions recipe, we defined two
functions to reorganize this data.
The first combined each three-row
group into a single row with a
total of eight columns of data:

def row_merge(source_iter):
  group = []
  for row in source_iter:
    if len(row[0]) != 0:
      if group:
        yield group
      group = row.copy()
    else:
      group.extend(row)
  if group:
    yield group

This is a variation on the
head-tail algorithm. When
len(row[0]) != 0 , this is the
header row for a new group - any
previously complete group is
yielded, and then the working value
of the group variable is reset to a
fresh, new list based on this
header row. A copy() is made so
that we can avoid mutating the list
object later on. When len(row[0])
== 0, this is the tail of the
group; the row is appended to the
working value of the group
variable. At the end of the source
of data, there's generally a
complete group that needs to be
processed. There's an edge case
where there's no data at all; in
which case, there's no final group
to yield, either.

We can use this function to
transform the data from many
confusing rows to single rows of
useful information:

>>> from ch08_r02 import row_merge, log_rows
>>> pprint(list(row_merge(log_rows)))

[['date',
 'engine on',
 'fuel height',
 '',
 'engine off',
 'fuel height',
 '',
 'Other notes',
 ''],
['10/25/13',
 '08:24:00 AM',
 '29',
 '',
 '01:15:00 PM',
 '27',
 '',
 "calm seas -- anchor solomon's island",
 ''],
['10/26/13',
 '09:12:00 AM',
 '27',
 '',
 '06:25:00 PM',
 '22',
 '',
 "choppy -- anchor in jackson's creek",
 '']]

We see that the first row is just
the spreadsheet headers. We'd like
to skip this row. We'll create a
generator expression to handle the
filtering and reject this extra
row.

    evince -p 377 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 406]

How to do it...

- 1. Write the predicate function
  that tests an item to see whether
  it should be passed through the
  filter for further processing. In
  some cases, we'll have to start
  with a reject rule and then write
  the inverse to make it into a
  pass rule:

def pass_non_date(row):
  return row[0] != 'date'

This can be used in three ways:
statements, expressions, and the
filter() function. Here is an
example of an explicit
for...if...yield pattern of
statements for passing rows:

for item in collection:
  if pass_non_date(item):
    yield item

This uses a for statement to
process each item in the collection
using the filter function. Selected
items are yielded. Other items are
rejected.

The second way to use this function
is in a generator expression like
this:

(item for item in data if pass_non_date(item))

This generator expressions applies
the filter function,
pass_non_date(), to each item. The
third choice is the filter()
function.

    evince -p 378 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 407]

- 2. Use the filter() function to
  apply the function to the source
  data:

filter(pass_non_date, data)

We've provided the name of the
function, pass_non_date. We don't
use () characters after the
function name because this
expression doesn't evaluate the
function. The filter() function
will apply the given function to
the iterable source of data, data.
In this case, data is a collection,
but it can be any iterable,
including the results of a previous
generator expression. Each item for
which the pass_non_date() function
is true will be passed by the
filter; all other values are
rejected.

We can use this as follows:

for row in filter(pass_non_date, row_merge(data)):
  print(row[0], row[1], row[4])

The filter() function creates an
iterable object that applies the
pass_non_date() function as a rule
to pass or reject each item in the
row_merge(data) iterable. It yields
the rows that don't have date in
column zero.


How it works...

The filter() function replaces some
common boilerplate code. We can
imagine that the definition looks
something like this:

def filter(f, iterable):
  for item in iterable:
    if f(item):
      yield f(item)

Or, we can imagine that it looks
like this:

def filter(f, iterable):
  return (item for item in iterable if f(item))

Both of these definitions summarize
the core feature of the filter()
function: some data is passed and
some data is rejected. This is a
handy shorthand that eliminates
some boilerplate code for applying
a function to an iterable source of
data.

    evince -p 379 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 408]

There's more...

Sometimes it's difficult to write a
simple rule to pass data. It may be
clearer if we write a rule to
reject data. For example, this
might make more sense:

def reject_date(row):
  return row[0] == 'date'

We can use a reject rule in a
number of ways. Here's a
for...if...continue...yield pattern
of statements. This will use
continue to skip the rejected rows,
and yield the remaining rows:

for item in collection:
  if reject_date(item):
    continue
  yield item

We can also use this variation. For
some programmers, the not reject
concept can become confusing. It
might seem like a double negative:

for item in collection:
  if not reject_date(item):
    yield item

We can also use a generator
expression like this:

(item for item in data if not reject_date(item))

We can't, however, easily use the
filter() function with a rule
that's designed to reject data. The
filter() function is designed to
work with pass rules only.

We have two essential choices for
dealing with this kind of logic. We
can wrap the logic in another
expression, or we use a function
from the itertools module. When it
comes to wrapping, we have two
further choices. We can wrap a
reject function to create a pass
function from it. We can use
something like this:

def pass_date(row):
  return not reject_date(row)

This makes it possible to create a
simple reject rule and use it in
the filter() function. Another way
to wrap the logic is to create a
lambda object:

filter(lambda item: not reject_date(item), data)

    evince -p 380 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 409]

The lambda function is a small,
anonymous function. It's a function
that's been reduced to just two
elements: the parameter list and a
single expression. We've wrapped
the reject_date() function to
create a kind of not_reject_date
function via the lambda object.

In the itertools module, we use the
filterfalse() function. We can
import filterfalse() and use this
instead of the built-in filter()
function.


See also...

- In the Using stacked generator
  expressions recipe, we placed a
  function like this in a stack of
  generators. We built a composite
  function from a number of
  individual mapping and filtering
  operations written as generator
  functions.


Summarizing a collection - how to reduce

In the introduction to this
chapter, we noted that there are
three common processing patterns:
map, filter, and reduce. We've seen
examples of mapping in the Applying
transformations to a collection
recipe, and examples of filtering
in the Picking a subset - three
ways to filter recipe. It's
relatively easy to see how these
become very generic operations.

Mapping applies a simple function
to all elements of a collection.
{M(x): x ∈ C} applies a function,
M, to each item, x, of a larger
collection, C. In Python, it can
look like this:

(M(x) for x in C)

Or, we can use the built-in map()
function to remove the boilerplate
and simplify it to this:

map(M, c)

Similarly, filtering uses a
function to select elements from a
collection. {x: x ∈ C if F(x)} uses
a function, F, to determine whether
to pass or reject an item, x, from
a larger collection, C. We can
express this in a variety of ways
in Python, one of which is like
this:

filter(F, c)

    evince -p 381 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 410]

This applies a predicate function,
F(), to a collection, c.

The third common pattern is
reduction. In the Designing classes
with lots of processing and
Extending a collection: a list that
does statistics recipes, we looked
at class definitions that computed
a number of statistical values.
These definitions relied - almost
exclusively - on the built-in sum()
function. This is one of the more
common reductions.

Can we generalize summation in a
way that allows us to write a
number of different kinds of
reductions? How can we define the
concept of reduction in a more
general way?


Getting ready

One of the most common reductions
is the sum. Other reductions
include a product, minimum,
maximum, average, variance, and
even a simple count of values.

Here's a way to think of the
mathematical definition of the sum
function, +, applied to values in a
collection, C:

Sigma ...

We've expanded the definition of
sum by inserting the + operator
into the sequence of values, C =
c0, c1, c2, ..., cn. This idea of
folding in the + operator captures
the meaning of the built-in sum()
function.

Similarly, the definition of
product looks like this:

PI (for product)

Here, too, we've performed a
different fold on a sequence of
values. Expanding a reduction by
folding involves two items: a
binary operator and a base value.
For sum, the operator was + and the
base value is zero. For product,
the operator is × and the base
value is one.

    evince -p 382 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 411]

We could define a generic
higher-level function, F(⋄, ⊥),
that captures the ideal of a fold.
The fold function definition
includes a placeholder for an
operator, ⋄, and a placeholder for
a base value, ⊥. The function's
value for a given collection, C,
can be defined with this recursive
rule:

___insert_figure

If the collection, C, is empty, the
value is the base value, ⊥. When
defining sum(), the base value
would be zero. If C is not empty,
then we'll first compute the fold
of everything but the last value in
the collection, F◊, ⊥(C0..n-1).
Then we'll apply the operator - for
example, addition - between the
previous fold result and the final
value in the collection, Cn-1. For
sum(), the operator is +.

We've used the notation C0..n in
the Pythonic sense of an open-ended
range. The values at indices 0 to
n-1 are included, but the value of
index n is not included. This means
that C0..0= ∅: there are no
elements in this range C0..0.

This definition is called a fold
left operation because the net
effect of this definition is to
perform the underlying operations
from left to right in the
collection. This could be changed
to also define a fold right
operation. Since Python's reduce()
function is a fold left, we'll
stick with that.

We'll define a prod() function that
can be used to compute factorial
values:


The value of n factorial is the
product of all of the numbers
between 1 and n inclusive. Since
Python uses half-open ranges, it's
a little more Pythonic to use or
define a range using 1 ≤ x < n + 1.
This definition fits the built-in
range() function better.

Using the fold operator that we
defined earlier, we have this.
We've defined a fold (or reduce)
using an operator of
multiplication, *, and a base value
of one:


    evince -p 383 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 412]

The idea of folding is the generic
concept that underlies Python's
concept of reduce(). We can apply
this to many algorithms,
potentially simplifying the
definition.


How to do it...

- 1. Import the reduce() function
  from the functools module:

>>> from functools import reduce

- 2. Pick the operator. For sum,
  it's +. For product, it's *.
  These can be defined in a variety
  of ways. Here's the long version.
  Other ways to define the
  necessary binary operator will be
  shown later:

>>> def mul(a, b):
...   return a * b

- 3. Pick the base value required.
  For sum, it's zero. For product,
  it's one. This allows us to
  define a prod() function that
  computes a generic product:

>>> def prod(values):
...   return reduce(mul, values, 1)

- 4. For factorial, we need to
  define the sequence of values
  that will be reduced:

range(1, n+1)

Here's how this works with the
prod() function:

>>> prod(range(1, 5+1))
120

Here's the whole factorial
function:

>>> def factorial(n):
...   return prod(range(1, n+1))

Here's the number of ways that a
52-card deck can be arranged. This
is the value 52!:

>>> factorial(52)
80658175170943878571660636856403766975289505440883277824000000000000

    evince -p 384 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 413]

There are a lot of a ways a deck
can be shuffled.

How many 5-card hands are possible?
The binomial calculation uses
factorial:

(52)      52!
    = -----------
(5 )   5!(52-5)!

>>> factorial(52)//(factorial(5)*factorial(52-5))
2598960

For any given shuffle, there are
about 2.6 million different
possible poker hands. (And yes,
this is a terribly inefficient way
to compute the binomial.)


How it works...

The reduce() function behaves as
though it had this definition:

def reduce(function, iterable, base):
  result = base
  for item in iterable:
    result = function(result, item)
  return result

This will iterate through the
values from left to right. It will
apply the given binary function
between the previous set of values
and the next item from the iterable
collection.

When we look at the Recursive
functions and Python's stack limits
recipe, we can see that the
recursive definition of fold can be
optimized to this for statement.


There's more...

When designing a reduce() function,
we need to provide a binary
operator. There are three ways to
define the necessary binary
operator. We used a complete
function definition like this:

def mul(a, b):
  return a * b

    evince -p 385 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 414]

There are two other choices. We can
use a lambda object instead of a
complete function:

>>> add = lambda a, b: a + b
>>> mul = lambda a, b: a * b

A lambda function is an anonymous
function boiled down to just two
essential elements: the parameters
and the return expression. There
are no statements inside a lambda,
only a single expression. In this
case, the expression simply uses
the desired operator.

We can use it like this:

>>> def prod2(values):
...   return reduce(lambda a, b: a*b, values, 1)

This provides the multiplication
function as a lambda object without
the overhead of a separate function
definition.

We can also import the definition
from the operator module:

from operator import add, mul

This works nicely for all of the
built-in arithmetic operators.

Note that logical reductions using
the logic operators AND and OR are
a little different from other
arithmetic reductions. These
operators short-circuit: once the
value is false, an and-reduce can
stop processing. Similarly, once
the value is True, an or-reduce can
stop processing. The built-in
functions any() and all() embody
this nicely. The short-circuit
feature is difficult to capture
using the built-in reduce().


Maxima and minima

How can we use reduce() to compute
a maximum or minimum? This is a
little more complex because there's
no trivial base value that can be
used. We cannot start with zero or
one because these values might be
outside the range of values being
minimized or maximized.

Also, the built-in max() and min()
must raise an exception for an
empty sequence. These functions
can't fit perfectly with the way
the sum() function and reduce()
functions work.

We have to use something like this
to provide the expected feature
set:

def mymax(sequence):
  try:
    base = sequence[0]
    max_rule = lambda a, b: a if a > b else b
    reduce(max_rule, sequence, base)
  except IndexError:
    raise ValueError

    evince -p 386 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 415]

This function will pick the first
value from the sequence as a base
value. It creates a lambda object,
named max_rule, which selects the
larger of the two argument values.
We can then use this base value
located in the data, and the lambda
object. The reduce() function will
then locate the largest value in a
non-empty collection. We've
captured the IndexError exception
so that an empty collection will
raise a ValueError exception.

This example shows how we can
invent a more complex or
sophisticated minimum or maximum
function that is still based on the
built-in reduce() function. The
advantage of this is replacing the
boilerplate for statement when
reducing a collection to a single
value.


Potential for abuse

Note that a fold (or reduce() as
it's called in Python) can be
abused, leading to poor
performance. We have to be cautious
about simply using a reduce()
function without thinking carefully
about what the resulting algorithm
might look like. In particular, the
operator being folded into the
collection should be a simple
process such as adding or
multiplying. Using reduce() changes
the complexity of an O(1) operation
into O(n).

Imagine what would happen if the
operator being applied during the
reduction involved a sort over a
collection. A complex operator -
with O(n log n) complexity - being
used in a 2 reduce() would change
the complexity of the overall
reduce() to O(n log n).


Combining map and reduce transformations

In the other recipes in this
chapter, we've been looking at map,
filter, and reduce operations.
We've looked at each of these in
isolation:

- The Applying transformations to a
  collection recipe shows the map()
  function
- The Picking a subset - three ways
  to filter recipe shows the
  filter() function
- The Summarizing a collection -
  how to reduce recipe shows the
  reduce() function

    evince -p 387 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 416]

Many algorithms will involve
combinations of functions. We'll
often use mapping, filtering, and a
reduction to produce a summary of
available data. Additionally, we'll
need to look at a profound
limitation of working with
iterators and generator functions.
Namely this limitation:

  Tip: An iterator can only produce
  values once.

If we create an iterator from a
generator function and a collection
data, the iterator will only
produce the data one time. After
that, it will appear to be an empty
sequence.

Here's an example:

>>> typical_iterator = iter([0, 1, 2, 3, 4])
>>> sum(typical_iterator)
10
>>> sum(typical_iterator)
0

We created an iterator over a
sequence of values by manually
applying the iter() function to a
literal list object. The first time
that the sum() function used the
value of typical_iterator, it
consumed all five values. The next
time we tried to apply any function
to the typical_iterator, there will
be no more values to be consumed -
the iterator appears empty.

This basic one-time-only
restriction drives some of the
design considerations when working
with multiple kinds of generator
functions in conjunction with map,
filter, and reduce. We'll often
need to cache intermediate results
so that we can perform multiple
reductions on the data.


Getting ready

In the Using stacked generator
expressions recipe, we looked at
data that required a number of
processing steps. We merged rows
with a generator function. We
filtered some rows to remove them
from the resulting data.
Additionally, we applied a number
of mappings to the data to convert
dates and times to more useful
information.

    evince -p 388 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 417]

We'd like to supplement this with
two more reductions to get some
average and variance information.
These statistics will help us
understand the data more fully.

We have a spreadsheet that is used
to record fuel consumption on a
large sailboat. It has rows which
look like this:

date
engine on
fuel height
engine off
fuel height
Other notes
10/25/2013 08:24
29
13:15
27
calm seas - anchor solomon's island
10/26/2013 09:12
27
18:25
22
choppy - anchor in jackson's creek

The initial processing was a
sequence of operations to change
the organization of the data,
filter out the headings, and
compute some useful values.


How to do it...

- 1. Start with the goal. In this
  case, we'd like a function we can
  use like this:

>>> round(sum_fuel(clean_data(row_merge(log_rows))), 3)
7.0

This shows a three-step pattern for
this kind of processing. These
three steps will define our
approach to creating the various
parts of this reduction:

  - 1. First, transform the data
    organization. This is sometimes
    called normalizing the data. In
    this case, we'll use a function
    called row_merge(). See the
    Using stacked generator
    expressions recipe for more
    information on this.
  - 2. Second, use mapping and
    filtering to clean and enrich
    the data. This is defined as a
    single function, clean_data().

    evince -p 389 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 418]

  - 3. Finally, reduce the data to
    a sum with sum_fuel(). There
    are a variety of other
    reductions that make sense. We
    might compute averages, or sums
    of other values. There are a
    lot of reductions we might want
    to apply.

- 2. If needed, define the data
  structure normalization function.
  This almost always has to be a
  generator function. A structural
  change can't be applied via
  map():

from ch08_r02 import row_merge

As shown in the Using stacked
generator expressions recipe, this
generator function will restructure
the data from three rows per each
leg of the voyage to one row per
leg. When all of the columns are in
a single row, the data is much
easier to work with.

- 3. Define the overall data
  cleansing and enrichment data
  function. This is a generator
  function that's built from
  simpler functions. It's a stack
  of map() and filter() operations
  that will derive data from the
  source fields:

def clean_data(source):
  namespace_iter = map(make_namespace, source)
  fitered_source = filter(remove_date, namespace_iter)
  start_iter = map(start_datetime, fitered_source)
  end_iter = map(end_datetime, start_iter)
  delta_iter = map(duration, end_iter)
  fuel_iter = map(fuel_use, delta_iter)
  per_hour_iter = map(fuel_per_hour, fuel_iter)
  return per_hour_iter

Each of the map() and filter()
operations involves a small
function to do a single conversion
or computation on the data.

- 4. Define the individual
  functions that are used for
  cleansing and deriving additional
  data.
- 5. Convert the merged rows of
  data into a SimpleNamespace. This
  will allow us to use names such
  as start_time instead of row[1]:

from types import SimpleNamespace
  def make_namespace(row):
    ns = SimpleNamespace(
      date = row[0],
      start_time = row[1],
      start_fuel_height = row[2],
      end_time = row[4],
      end_fuel_height = row[5],
      other_notes = row[7]
    )
    return ns

    evince -p 390 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 419]

This function builds a
SimpleNamspace from selected
columns of the source data. Columns
three and six were omitted because
they were always zero-length
strings, ''.

- 6. Here's the function used by
  the filter() to remove the
  heading rows. If needed, this can
  be expanded to remove blank lines
  or other bad data from the
  source. The idea is to remove bad
  data as soon as possible in the
  processing:

def remove_date(row_ns):
  return not(row_ns.date == 'date')

- 7. Convert data to a usable form.
  First, we'll convert strings to
  dates. The next two functions
  depend on this timestamp()
  function that converts a date
  string from one column plus a
  time string from another column
  into a proper datetime instance:

import datetime
def timestamp(date_text, time_text):
  date = datetime.datetime.strptime(date_text, "%m/%d/%y").date()
  time = datetime.datetime.strptime(time_text, "%I:%M:%S %p").time()
  timestamp = datetime.datetime.combine(date, time)
  return timestamp

This allows us to do simple date
calculations based on the datetime
library. In particular, subtracting
two timestamps will create a
timedelta object that has the exact
number of seconds between any two
dates.

Here's how we'll use this function
to create a proper timestamp for
the start of the leg and the end of
the leg:

def start_datetime(row_ns):
  row_ns.start_timestamp = timestamp(row_ns.date, row_ns.start_time)
  return row_ns

def end_datetime(row_ns):
  row_ns.end_timestamp = timestamp(row_ns.date, row_ns.end_time)
  return row_ns

    evince -p 391 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 420]

Both of these functions will add a
new attribute to the
SimpleNamespace and also return the
namespace object. This allows these
functions to be used in a stack of
map() operations. We can also
rewrite these functions to replace
the mutable SimpleNamespace with an
immutable namedtuple() and still
preserve the stack of map()
operations.

- 8. Compute derived time data. In
  this case, we can compute a
  duration too. Here's a function
  which must be performed after the
  previous two:

def duration(row_ns):
  travel_time = row_ns.end_timestamp - row_ns.start_timestamp
  row_ns.travel_hours = round(travel_time.total_seconds()/60/60, 1)
  return row_ns

This will convert the difference in
seconds into a value in hours. It
will also round to the nearest
tenth of an hour. Any more accuracy
than this is largely noise. The
departure and arrival times are
(generally) off by at least a
minute; they depend on when the
skipper remembered to look at her
watch. In some cases, she may have
estimated the time.

- 9. Compute other metrics that are
  needed for the analyses. This
  includes creating the height
  values that are converted to
  float numbers. The final
  calculation is based on two other
  calculated results:

def fuel_use(row_ns):
  end_height = float(row_ns.end_fuel_height)
  start_height = float(row_ns.start_fuel_height)
  row_ns.fuel_change = start_height - end_height
  return row_ns

def fuel_per_hour(row_ns):
  row_ns.fuel_per_hour = row_ns.fuel_change/row_ns.travel_hours
  return row_ns

The fuel per hour calculation
depends on the entire preceding
stack of calculations. The travel
hours comes from the start and end
timestamps which are computed
separately.

    evince -p 392 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 421]

How it works...

The idea is to create a composite
operation that follows a common
template:

- 1. Normalize the structure: This
  often requires a generator
  function to read data in one
  structure and yield data in a
  different structure.
- 2. Filter and cleanse: This may
  involve a simple filter as shown
  in this example. We'll look at
  more complex filters later.
- 3. Derive data via mappings or
  via lazy properties of class
  definitions: A class with lazy
  properties is a reactive object.
  Any change to the source property
  should cause changes to the
  computed properties.

In some cases, we may want to
combine the basic facts with other
dimensional descriptions. For
example, we might need to look up
reference data, or decode coded
fields.

Once we've done the preliminary
steps, we have data which is usable
for a variety of analyses. Many
times, this is a reduce operation.
The initial example computes a sum
of fuel use. Here are two other
examples:

from statistics import *
def avg_fuel_per_hour(iterable):
  return mean(row.fuel_per_hour for row in iterable)
def stdev_fuel_per_hour(iterable):
  return stdev(row.fuel_per_hour for row in iterable)

These functions apply the mean()
and stdev() functions to the
fuel_per_hour attribute of each row
of the enriched data.

We might use this as follows:

>>> round(avg_fuel_per_hour(
...   clean_data(row_merge(log_rows))), 3)
0.48

We've used the
clean_data(row_merge(log_rows))
mapping pipeline to cleanse and
enrich the raw data. Then we
applied a reduction to this data to
get the value we're interested in.

We now know that our 30″ tall tank
is good for about 60 hours of
motoring. At 6 knots, we can go
about 360 nautical miles on a full
tank of fuel.

    evince -p 393 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 422]

There's more...

As we noted, we can only perform
one reduction on an iterable source
of data. If we want to compute
several averages, or the average
and the variance, we'll need to use
a slightly different pattern.

In order to compute multiple
summaries of the data, we'll need
to create a sequence object of some
kind that can be summarized
repeatedly:

data = tuple(clean_data(row_merge(log_rows)))
m = avg_fuel_per_hour(data)
s = 2*stdev_fuel_per_hour(data)
print("Fuel use {m:.2f} ±{s:.2f}".format(m=m, s=s))

Here, we've created a tuple from
the cleaned and enriched data. This
tuple will produce an iterable, but
unlike a generator function, it can
produce this iterable many times.
We can compute two summaries by
using the tuple object.

This design involves a large number
of transformations of source data.
We've built it using a stack of
map, filter, and reduce operations.
This provides a great deal of
flexibility.

The alternative approach is to
create a class definition. A class
can be designed with lazy
properties. This would create a
kind of reactive design embodied in
a single block of code.

See the Using properties for lazy
attributes recipe for examples of
this.

We can also use the tee() function
in the itertools module for this
kind of processing:

from itertools import tee
data1, data2 = tee(clean_data(row_merge(log_rows)), 2)
m = avg_fuel_per_hour(data1)
s = 2*stdev_fuel_per_hour(data2)

We've used tee() to create two
clones of the iterable output from
clean_data(row_merge(log_rows)). We
can use these two clones to compute
a mean and a standard deviation.


See also

- We looked at how to combine
  mapping and filtering in the
  Using stacked generator
  expressions recipe.
- We looked at lazy properties in
  the Using properties for lazy
  attributes recipe. Also, this
  recipe looks at some important
  variations on map-reduce
  processing.

    evince -p 394 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 423]

Implementing "there exists" processing

The processing patterns we've been
looking at can all be summarized
with the quantifier for all. It's
been an implicit part of all of the
processing definitions:

- Map: For all items in the source,
  apply the map function. We can
  use the quantifier to make this
  explicit: {M(x) ∀ x: x ∈ C}
- Filter: For all items in the
  source, pass those for which the
  filter function is true. Here
  also, we've used the quantifier
  to make this explicit. We want
  all values, x, from a set, C, if
  some function, F(x), is true: {x
  ∀ x: x ∈ C if F(x)}
- Reduce: For all items in the
  source, use the given operator
  and base value to compute a
  summary. The rule for this is a
  recursion that clearly works for
  all values of the source
  collection or iterable:
[+ Formula]
.

We've used the notation C0..n in
the Pythonic sense of an open-ended
range. Values with index positions
of 0 and n-1 are included, but the
value at index position n is not
included. This means that there are
no elements in this range.

What's more important is that
C0..n-1 ∪ Cn-1= C. That is, when we
take the last item from the range,
no items are lost - we're always
processing all the items in the
collection. Also, we aren't
processing item Cn-1 twice. It's
not part of the C0..n-1 range, but
it is a standalone item
Cn-1.

How can we write a process using
generator functions that stops when
the first value matches some
predicate? How do we avoid for all
and quantify our logic with there
exists?


Getting ready

There's another quantifier that we
might need - there exists, ∃. Let's
look at an example of an existence
test.

We might want to know whether a
number is prime or composite. We
don't need all of the factors of a
number to know it's not prime. It's
sufficient to show that a factor
exists to know that a number is not
prime.

    evince -p 395 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 424]

We can define a prime predicate,
P(n), like this:

P(n) = ¬∃i: 2 ≤ i < n if n mod i = 0

A number, n, is prime if there does
not exist a value of i (between 2
and the number) that divides the
number evenly. We can move the
negation around and rephrase this
as follows:

¬P(n) = ∃i: 2 ≤ i < n if n mod i = 0

A number, n, is composite
(non-prime) if there exists a
value, i, between 2 and the number
itself, that divides the number
evenly. We don't need to know all
such values. The existence of one
value that satisfies the predicate
is sufficient.

Once we've found such a number, we
can break early from any iteration.
This requires the break statement
inside for and if statements.
Because we're not processing all
values, we can't easily use a
higher-order function such as
map(), filter(), or reduce().


How to do it...

- 1. Define a generator function
  template that will skip items
  until the required one is found.
  This will yield only one value
  that passes the predicate test:

def find_first(predicate, iterable):
  for item in iterable:
    if predicate(item):
      yield item
      break

- 2. Define a predicate function.
  For our purposes, a simple lambda
  object will do. Also, a lambda
  allows us to work with a variable
  bound to the iteration and a
  variable that's free from the
  iteration. Here's the expression:

lambda i: n % i == 0

We're relying on a non-local value,
n, in this lambda. This will be
global to the lambda, but still
local to the overall function. If n
% i is 0, then i is a factor of n,
and n is not prime.

    evince -p 396 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 425]

- 3. Apply the function with the
  given range and predicate:

import math
def prime(n):
  factors = find_first(
    lambda i: n % i == 0,
    range(2, int(math.sqrt(n)+1)) )
  return len(list(factors)) == 0

If the factors iterable has an
item, then n is composite.
Otherwise, there are no values in
the factors iterable, which means n
is a prime number.

As a practical matter, we don't
need to test every single number
between two and n to see whether n
is prime. It's only necessary to
test values, i, such that 2 ≤ i <
√n.


How it works...

In the find_first() function, we
introduce a break statement to stop
processing the source iterable.
When the for statement stops, the
generator will reach the end of the
function, and return normally.

The process which is consuming
values from this generator will be
given the StopIteration exception.
This exception means the generator
will produce no more values. The
find_first() function raises as an
exception, but it's not an error.
It's the normal way to signal that
an iterable has finished processing
the input values.

In this case, the signal means one
of two things:

- If a value has been yielded, the
  value is a factor of n
- If no value was yielded, then n
  is prime

This small change of breaking early
from the for statement makes a
dramatic difference in the meaning
of the generator function. Instead
of processing all values from the
source, the find_first() generator
will stop processing as soon as the
predicate is true.

This is different from a filter,
where all of the source values will
be consumed. When using the break
statement to leave the for
statement early, some of the source
values may not be processed.

    evince -p 397 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 426]

There's more...

In the itertools module, there is
an alternative to this find_first()
function. The takewhile() function
uses a predicate function to keep
taking values from the input. When
the predicate becomes false, then
the function stops processing
values.

We can easily change the lambda
from lambda i: n % i == 0 to lambda
i: n % i != 0. This will allow the
function to take values while they
are not factors. Any value that is
a factor will stop the processing
by ending the takewhile() process.

Let's look at two examples. We'll
test 13 for being prime. We need to
check numbers in the range. We'll
also test 15 for being prime:

>>> from itertools import takewhile
>>> n = 13
>>> list(takewhile(lambda i: n % i != 0, range(2, 4)))
[2, 3]
>>> n = 15
>>> list(takewhile(lambda i: n % i != 0, range(2, 4)))
[2]

For a prime number, all of the test
values pass the takewhile()
predicate. The result is a list of
non-factors of the given number, n.
If the set of non-factors is the
same as the set of values being
tested, then n is prime. In the
case of 13, both collections of
values are [2, 3].

For a composite number, some values
pass the takewhile() predicate. In
this example, 2 is not a factor of
15. However, 3 is a factor; this
does not pass the predicate. The
collection of non-factors, [2], is
not the same as the set of values
collection that was tested, [2, 3].

We wind up with a function that looks like this:

def prime_t(n):
  tests = set(range(2, int(math.sqrt(n)+1)))
  non_factors = set(
    takewhile(
      lambda i: n % i != 0,
      tests
    )
  )
  return tests == non_factors

    evince -p 398 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 427]

This creates two intermediate set
objects, tests and non_factors. If
all of the tested values are not
factors, the number is prime. The
function shown previously, based on
find_first() only creates one
intermediate list object. That list
will have at most one member,
making the data structure much
smaller.


The itertools module

There are a number of additional
functions in the itertools module
that we can use to simplify complex
map-reduce applications:

- filterfalse(): It is the
  companion to the built-in
  filter() function. It inverts the
  predicate logic of the filter()
  function; it rejects items for
  which the predicate is true.
- zip_longest(): It is the
  companion to the built-in zip()
  function. The built-in zip()
  function stops merging items when
  the shortest iterable is
  exhausted. The zip_longest()
  function will supply a given fill
  value to pad short iterables to
  match the longest.
- starmap(): It is a modification
  to the essential map() algorithm.
  When we perform map(function,
  iter1, iter2), then an item from
  each iterable is provided as two
  positional arguments to the given
  function. The starmap() expects
  an iterable to provide a tuple
  that contains the argument
  values. In effect:

map = starmap(function, zip(iter1, iter2))

There are still others that we
might use, too:

- accumulate(): This function is a
  variation on the built-in sum()
  function. This will yield each
  partial total that's produced
  before reaching the final sum.
- chain(): This function will
  combine iterables in order.
- compress(): This function uses
  one iterable as a source of data
  and the other as a source of
  selectors. When the item from the
  selector is true, the
  corresponding data item is
  passed. Otherwise, the data item
  is rejected. This is an
  item-by-item filter based on
  true-false values.
- dropwhile(): While the predicate
  to this function is true, it will
  reject values. Once the predicate
  becomes false, it will pass all
  remaining values. See
  takewhile().
- groupby(): This function uses a
  key function to control the
  definition of groups. Items with
  the same key value are grouped
  into separate iterators. For the
  results to be useful, the
  original data should be sorted
  into order by the keys.

    evince -p 399 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 428]

- islice(): This function is like a
  slice expression, except it
  applies to an iterable, not a
  list. Where we use list[1:] to
  discard the first row of a list,
  we can use islice(iterable, 1) to
  discard the first item from an
  iterable.
- takewhile(): While the predicate
  is true, this function will pass
  values. Once the predicate
  becomes false, stop processing
  any remaining values. See
  dropwhile().
- tee(): This splits a single
  iterable into a number of clones.
  Each clone can then be consumed
  separately. This is a way to
  perform multiple reductions on a
  single iterable source of data.


Creating a partial function

When we look at functions such as
reduce(), sorted(), min(), and
max(), we see that we'll often have
some permanent argument values. For
example, we might find a need to
write something like this in
several places:

reduce(operator.mul, ..., 1)

Of the three parameters to
reduce(), only one - the iterable
to process - actually changes. The
operator and the base value
arguments are essentially fixed at
operator.mul and 1.

Clearly, we can define a whole new
function for this:

def prod(iterable):
  return reduce(operator.mul, iterable, 1)

However, Python has a few ways to
simplify this pattern so that we
don't have to repeat the
boilerplate def and return
statements.

How can we define a function that
has some parameters provided in
advance?

Note that the goal here is
different from providing default
values. A partial function doesn't
provide a way to override the
defaults. Instead, we want to
create as many partial functions as
we need, each with specific
parameters bound in advance.

    evince -p 400 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 429]

Getting ready

Some statistical modeling is done
with standard scores, sometimes
called z-scores. The idea is to
standardize a raw measurement onto
a value that can be easily compared
to a normal distribution, and
easily compared to related numbers
that are measured in different
units.

The calculation is this:

z = (x - μ)/σ

Here, x is the raw value, μ is the
population mean, and σ is the
population standard deviation. The
value z will have a mean of 0 and a
standard deviation of 1. This makes
it particularly easy to work with.

We can use this value to spot
outliers - values which are
suspiciously far from the mean. We
expect that (about) 99.7% of our z
values will be between -3 and +3.

We could define a function like this:

def standarize(mean, stdev, x):
  return (x-mean)/stdev

This standardize() function will
compute a z-score from a raw score,
x. This function has two kinds of
parameters:

- The values for mean and stdev are
  essentially fixed. Once we've
  computed the population values,
  we'll have to provide them to the
  standardize() function over and
  over again.
- The value for x is more variable.

Let's say we've got a collection of
data samples in big blocks of text:

text_1 = '''10  8.04
8     6.95
13    7.58
...
5     5.68
'''
    evince -p 401 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 430]

We've defined two small functions
to convert this data to pairs of
numbers. The first simply breaks
each block of text to a sequence of
lines, and then breaks each line
into a pair of text items:

text_parse = lambda text: (r.split() for r in text.splitlines())

We've used the splitlines() method
of the text block to create a
sequence of lines. We put this into
a generator function so that each
individual line could be assigned
to r. Using r.split() separates the
two blocks of text in each row.

If we use list(text_parse(text_1)),
we'll see data like this:

[['10', '8.04'],
 ['8', '6.95'],
 ['13', '7.58'],
 ...
 ['5', '5.68']]

We need to further enrich this data
to make it more usable. We need to
convert the strings to proper float
values. While doing that, we'll
create SimpleNamespace instances
from each item:

from types import SimpleNamespace
row_build = lambda rows: (SimpleNamespace(x=float(x), y=float(y)) for x,y in rows)

The lambda object creates a
SimpleNamespace instance by
applying the float() function to
each string item in each row. This
gives us data we can work with.

We can apply these two lambda
objects to the data to create some
usable datasets. Earlier, we showed
text_1. We'll assume that we have a
second, similar set of data
assigned to text_2:

data_1 = list(row_build(text_parse(text_1)))
data_2 = list(row_build(text_parse(text_2)))

This creates data from two blocks
of similar text. Each has pairs of
data points. The SimpleNamespace
object has two attributes, x and y,
assigned to each row of the data.

Note that this process creates
instances of types.SimpleNamespace.
When we print them, they will be
displayed using the class
namespace. These are mutable
objects, so that we can update each
one with the standardized z-score.

    evince -p 402 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 431]

Printing data_1 looks like this:

[namespace(x=10.0, y=8.04), namespace(x=8.0, y=6.95), namespace(x=13.0, y=7.58),
...,
namespace(x=5.0, y=5.68)]

As an example, we'll compute a
standardized value for the x
attribute. This means getting mean
and standard deviation. Then we'll
need to apply these values to
standardize data in both of our
collections. It looks like this:

import statistics
mean_x = statistics.mean(item.x for item in data_1)
stdev_x = statistics.stdev(item.x for item in data_1)

for row in data_1:
  z_x = standardize(mean_x, stdev_x, row.x)
  print(row, z_x)

for row in data_2:
  z_x = standardize(mean_x, stdev_x, row.x)
  print(row, z_x)

Providing the mean_v1, stdev_v1
values each time we evaluate
standardize() can clutter an
algorithm with details that aren't
deeply important. In some rather
complex algorithms, the clutter can
lead to more confusion than
clarity.


How to do it...

In addition to simply using the def
statement to create a function that
has a partial set of argument
values, we have two other ways to
create a partial function:

- Using the partial() function from
  the functools module
- Creating a lambda object

    evince -p 403 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 432]

Using functools.partial()

- 1. Import the partial function
  from functools:

from functools import partial

- 2. Create an object using
  partial(). We provide the base
  function, plus the positional
  arguments that need to be
  included. Any parameters which
  are not supplied when the partial
  is defined must be supplied when
  the partial is evaluated:

z = partial(standardize, mean_x, stdev_x)

- 3. We've provided values for the
  first two positional parameters,
  mean and stdev. The third
  positional parameter, x, must be
  supplied in order to compute a
  value.


Creating a lambda object

- 1. Define a lambda object that
  binds the fixed parameters:

lambda x: standardize(mean_v1, stdev_v1, x)

- 2. Create an object using lambda:

z = lambda x: standardize(mean_v1, stdev_v1, x)


How it works...

Both techniques create a callable
object - a function - named z()
that has the values for mean_v1 and
stdev_v1 already bound to the first
two positional parameters. With
either approach, we have processing
that can look like this:

for row in data_1:
  print(row, z(row.x))

for row in data_2:
  print(row, z(row.x))

We've applied the z() function to
each set of data. Because the
function has some parameters
already applied, its use here looks
very simple.

    evince -p 404 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 433]

We can also do the following
because each row is a mutable
object:

for row in data_1:
  row.z = z(row.v1)

for row in data_2:
  row.z = z(row.v1)

We've updated the row to include a
new attribute, z, with the value of
the z() function. In a complex
algorithm, tweaking the row objects
like this may be a helpful
simplification.

There's a significant difference
between the two techniques for
creating the z() function:

- The partial() function binds the
  actual values of the parameters.
  Any subsequent change to the
  variables that were used doesn't
  change the definition of the
  partial function that's created.
  After creating z =
  partial(standardize(mean_v1,
  stdev_v1)), changing the value of
  mean_v1 or stdev_v1 doesn't have
  an impact on the partial
  function, z().
- The lambda object binds the
  variable name, not the value. Any
  subsequent change to the
  variable's value will change the
  way the lambda behaves. After
  creating z = lambda x:
  standardize(mean_v1, stdev_v1,
  x), changing the value of mean_v1
  or stdev_v1 changes the values
  used by the lambda object, z().

We can modify the lambda slightly
to bind values instead of names:

z = lambda x, m=mean_v1, s=stdev_v1: standardize(m, s, x)

This extracts the values of mean_v1
and stdev_v1 to create default
values for the lambda object. The
values of mean_v1 and stdev_v1 are
now irrelevant to proper operation
of the lambda object, z().


There's more...

We can provide keyword argument
values as well as positional
argument values when creating a
partial function. In many cases,
this works nicely. There are a few
cases where it doesn't work.

    evince -p 405 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 434]

The reduce() function,
specifically, can't be trivially
turned into a partial function. The
parameters aren't in the ideal
order for creating a partial. The
reduce() function has the following
notional definition. This is not
how it's defined - this is how it
appears to be defined:

def reduce(function, iterable, initializer=None)

If this was the actual definition,
we could do this:

prod = partial(reduce(mul, initializer=1))

Practically, we can't do this
because the definition of reduce()
is a bit more complex than it might
appear. The reduce() function
doesn't permit named argument
values. This means that we're
forced to use the lambda technique:

>>> from operator import mul
>>> from functools import reduce
>>> prod = lambda x: reduce(mul, x, 1)

We've used a lambda object to
define a function, prod(), with
only one parameter. This function
uses reduce() with two fixed
parameters, and one variable
parameter.

Given this definition for prod(),
we can define other functions that
rely on computing products. Here's
a definition of the factorial
function:

>>> factorial = lambda x: prod(range(2,x+1))
>>> factorial(5)
120

The definition of factorial()
depends on prod(). The definition
of prod() is a kind of partial
function that uses reduce() with
two fixed parameter values. We've
managed to use a few definitions to
create a fairly sophisticated
function.

In Python, a function is an object.
We've seen numerous ways that
functions can be an argument to a
function. A function that accepts
another function as an argument is
sometimes called a higher-order
function.

Similarly, functions can also
return a function object as a
result. This means that we can
create a function like this:

def prepare_z(data):
  mean_x = statistics.mean(item.x for item in data_1)
  stdev_x = statistics.stdev(item.x for item in data_1)
  return partial(standardize, mean_x, stdev_x)

    evince -p 406 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 435]

We've defined a function over a set
of (x,y) samples. We've computed
the mean and standard deviation of
the x attribute of each sample.
We've then created a partial
function which can standardize
scores based on the computed
statistics. The result of this
function is a function we can use
for data analysis:

z = prepare_z(data_1)
for row in data_2:
  print(row, z(row.x))

When we evaluated the prepare_z()
function, it returned a function.
We assigned this function to a
variable, z. This variable is a
callable object; it's the function
z() that will standardize a score
based on the sample mean and
standard deviation.


Simplifying complex algorithms with immutable data structures

The concept of a stateful object is
a common feature of object-oriented
programming. We looked at a number
of techniques related to objects
and state in Chapter 6, Basics of
Classes and Objects, and Chapter 7,
More Advanced Class Design. A great
deal of the emphasis of
object-oriented design is creating
methods that mutate an object's
state.

We've also looked at some stateful
functional programming techniques
in the Using stacked generator
expressions, Combining map and
reduce transformations, and
Creating a partial function
recipes. We've used
types.SimpleNamespace because it
creates a simple, stateful object
with easy to use attribute names.

In most of these cases, we've been
working with objects that have a
Python dict object that defines the
attributes. The one exception is
the Optimizing small objects with
__slots__ recipe, where the
attributes are fixed by the
__slots__ attribute definition.

Using a dict object to store an
object's attributes has several
consequences:

- We can trivially add and remove
  attributes. We're not limited to
  simply setting and getting
  defined attributes; we can create
  new attributes too.
- Each object uses a somewhat
  larger amount of memory than is
  minimally necessary. This is
  because dictionaries use a
  hashing algorithm to locate keys
  and values. The hash processing
  generally requires more memory
  than other structures such as a
  list or a tuple. For very large
  amounts of data, this can become
  a problem.

    evince -p 407 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 436]

The most significant issue with
stateful object-oriented
programming is that it can
sometimes be challenging to write
clear assertions about state change
of an object. Rather than defining
assertions about state change, it's
much easier to create entirely new
objects with a state that can be
simply mapped to the object's type.
This, coupled with Python type
hints, can sometimes create more
reliable, and easier to test,
software.

When we create new objects, the
relationships between data items
and computations can be captured
explicitly. The mypy project
provides tools that can analyze
those type hints to provide some
confirmation that the objects used
in a complex algorithm are used
properly.

In some cases, we can also reduce
the amount of memory by avoiding
stateful objects in the first
place. We have two techniques for
doing this:

- Using class definitions with
  __slots__: See the Optimizing
  small objects with __slots__
  recipe for this. These objects
  are mutable, so we can update
  attributes with new values.
- Using immutable tuples or
  namedtuples: See the Designing
  classes with little unique
  processing recipe for some
  background on this. These objects
  are immutable. We can create new
  objects, but we can't change the
  state of an object. The cost
  savings from less memory overall
  have to be balanced against the
  additional costs of creating new
  objects.

Immutable objects can be somewhat
faster that mutable objects. The
more important benefit is to
algorithm design. In some cases,
writing functions that create new
immutable objects from old
immutable objects can be simpler,
and easier to test and debug, than
algorithms that work with stateful
objects. Writing type hints can
help this process.


Getting ready

As we noted in the Using stacked
generator expressions and
Implementing "there exists"
processing recipes, we can only
process a generator once. If we
need to process it more than one
time, the iterable sequence of
objects must be transformed into a
complete collection like a list or
tuple.

    evince -p 408 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 437]

This often leads to a multi-phase
process:

- Initial extract of the data: This
  might involve a database query,
  or reading a .csv file. This
  phase can be implemented as a
  function that yields rows or even
  returns a generator function.
- Cleansing and filtering the data:
  This may involve a stack of
  generator expressions that can
  process the source just once.
  This phase is often implemented
  as a function that includes
  several map and filter
  operations.
- Enriching the data: This, too,
  may involve a stack of generator
  expressions that can process the
  data one row at a time. This is
  typically a series of map
  operations to create new, derived
  data from existing data.
- Reducing or summarizing the data:
  This may involve multiple
  summaries. In order for this to
  work, the output from the
  enrichment phase needs to be a
  collection object that can be
  processed more than one time.

In some cases, the enrichment and
summary processes may be
interleaved. As we saw in the
Creating a partial function recipe,
we might do some summarization
followed by more enrichment.

There are two common strategies for
handling the enriching phase:

- Mutable objects: This means that
  enrichment processing adds or
  sets values of attributes. This
  can be done with eager
  calculations as attributes are
  set. See the Using settable
  properties to update eager
  attributes recipe. It can also be
  done with lazy properties. See
  the Using properties for lazy
  attributes recipe. We've shown
  examples using
  types.SimpleNamespace where the
  computation is done in functions
  separate from the class
  definition.
- Immutable objects: This means
  that the enrichment process
  creates new objects from old
  objects. Immutable objects are
  derived from tuple or are a type
  created by namedtuple(). These
  objects have the advantage of
  being very small and very fast.
  Also, the lack of any internal
  state change can make them very
  simple.

Let's say we've got a collection of
data samples in big blocks of text:

text_1 = '''10   8.04
8   6.95
13  7.58
...
5   5.68
'''

    evince -p 409 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 438]

Our goal is a three-step process
that includes the get, cleanse, and
enrich operations:

data = list(enrich(cleanse(get(text))))

The get() function acquires the
data from a source; in this case,
it would parse the big block of
text. The cleanse() function would
remove blank lines and other
unusable data. The enrich()
function would do the final
calculation on the cleaned data.
We'll look at each phase of this
pipeline separately.

The get() function is limited to
pure text processing, doing as
little filtering as possible:

from typing import *

def get(text: str) -> Iterator[List[str]]:
  for line in text.splitlines():
    if len(line) == 0:
      continue
    yield line.split()

In order to write type hints, we've
imported the typing module. This
allows us to make an explicit
declaration about the inputs and
outputs of this function. The get()
function accepts a string, str. It
yields a List[str] structure. Each
line of input is decomposed to a
sequence of values.

This function will generate all
non-empty lines of data. There is a
small filtering feature to this,
but it's related to a small
technical issue around data
serialization, not an
applicationspecific filtering rule.

The cleanse() function will
generate named tuples of data. This
will apply a number of rules to
assure that the data is valid:

from collections import namedtuple

DataPair = namedtuple('DataPair', ['x', 'y'])

def cleanse(iterable: Iterable[List[str]]) -> Iterator[DataPair]:
  for text_items in iterable:
    try:
      x_amount = float(text_items[0])
      y_amount = float(text_items[1])
    yield DataPair(x_amount, y_amount)
      except Exception as ex:
      print(ex, repr(text_items))

    evince -p 410 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 439]

We've defined a namedtuple with the
uninformative name of DataPair.
This item has two attributes, x,
and y. If the two text values can
be properly converted to float,
then this generator will yield a
useful DataPair instance. If the
two text values cannot be
converted, this will display an
error for the offending pair.

Note the technical subtlety that's
part of the mypy project's type
hints. A function with a yield
statement is an iterator. We can
use it as if it's an iterable
object because of a formal
relationship that says iterators
are a kind of iterable.

Additional cleansing rules could be
applied here. For example, assert
statements could be added inside
the try statement. Any exception
raised by unexpected or invalid
data will stop processing the given
row of input.

Here's the result of this initial
cleanse() and get() processing:

list(cleanse(get(text)))
The output looks like this:
[DataPair(x=10.0, y=8.04),
 DataPair(x=8.0, y=6.95),
 DataPair(x=13.0, y=7.58),
 ...,
 DataPair(x=5.0, y=5.68)]

In this example, we'll rank order
by the y value of each pair. This
requires sorting the data first,
and then yielding the sorted values
with an additional attribute value,
the y rank order.


How to do it...

- 1. Define the enriched
  namedtuple:

RankYDataPair = namedtuple('RankYDataPair', ['y_rank', 'pair'])

Note that we've specifically
included the original pair as a
data item in this new data
structure. We don't want to copy
the individual fields; instead,
we've incorporated the original
object as a whole.

- 2. Define the enrichment function:

PairIter = Iterable[DataPair]
RankPairIter = Iterator[RankYDataPair]

def rank_by_y(iterable:PairIter) -> RankPairIter:

    evince -p 411 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 440]

We've included type hints on this
function to make it clear precisely
what types are expected and
returned by this enrichment
function. We defined the type hints
separately so that they're shorter
and so that they can be reused in
other functions.

- 3. Write the body of the
  enrichment. In this case, we're
  going to be rank ordering, so
  we'll need sorted data, using the
  original y attribute. We're
  creating new objects from the old
  objects, so the function yields
  instances of RankYDataPair:

all_data = sorted(iterable, key=lambda pair:pair.y)
for y_rank, pair in enumerate(all_data, start=1):
  yield RankYDataPair(y_rank, pair)

We've used enumerate() to create
the rank order numbers to each
value. The starting value of 1 is
sometimes handy for some
statistical processing. In other
cases, the default starting value
of 0 will work out well.

Here's the whole function:

def rank_by_y(iterable: PairIter) -> RankPairIter:
  all_data = sorted(iterable, key=lambda pair:pair.y)
  for y_rank, pair in enumerate(all_data, start=1):
    yield RankYDataPair(y_rank, pair)

We can use this in a longer
expression to get, cleanse, and
then rank. The use of type hints
can make this clearer than an
alternative involving stateful
objects. In some cases, there can
be a very helpful improvement in
the clarity of the code.


How it works...

The result of the rank_by_y()
function is a new object which
contains the original object, plus
the result of the enrichment.
Here's how we'd use this stacked
sequence of generators:

rank_by_y(), cleanse(), and get():
>>> data = rank_by_y(cleanse(get(text_1)))
>>> pprint(list(data))
[RankYDataPair(y_rank=1, pair=DataPair(x=4.0, y=4.26)),
 RankYDataPair(y_rank=2, pair=DataPair(x=7.0, y=4.82)),
 RankYDataPair(y_rank=3, pair=DataPair(x=5.0, y=5.68)),
 ...,
 RankYDataPair(y_rank=11, pair=DataPair(x=12.0, y=10.84))]

    evince -p 412 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 441]

The data is in ascending order by
the y value. We can now use these
enriched data values for further
analysis and calculation.

Creating new objects can - in many
cases - be more expressive of the
algorithm than changing the state
of objects. This is often a
subjective judgement.

The Python type hints work best
with the creation of new objects.
Consequently, this technique can
provide strong evidence that a
complex algorithm is correct. Using
mypy makes immutable objects more
appealing.

Finally, we may sometimes see a
small speed-up when we use
immutable objects. This relies on a
balance between three features of
Python to be effective:

- Tuples are small data structures.
  Using these can improve
  performance.
- Any relationship between objects
  in Python involves creating an
  object reference, a data
  structure that's also very small.
  A number of related immutable
  objects might be smaller than a
  mutable object.
- Object creation can be costly.
  Creating too many immutable
  objects outweighs the benefits.

The memory savings from the first
two features must be balanced
against the processing cost from
the third feature. Memory savings
can lead to better performance when
there's a huge volume of data that
constrains processing speeds.

For small examples like this one,
the volume of data is so tiny that
the object creation cost is large
compared with any cost savings from
reducing the volume of memory in
use. For larger sets of data, the
object creation cost may be less
than the cost of running low on
memory.


There's more...

The get() and cleanse() functions
in this recipe both refer to a
similar data structure:
Iterable[List[str]] and
Iterator[List[str]]. In the
collections.abc module, we see that
Iterable is the generic definition,
and Iterator is a special case of
Iterable.

The mypy release used for this book
- mypy 0.2.0-dev - is very
particular about functions with the
yield statement being defined as an
Iterator. A future release may
relax this strict check of the
subclass relationship, allowing us
to use one definition for both
cases.

    evince -p 413 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 442]

The typing module includes an
alternative to the namedtuple()
function: NamedTuple(). This allows
specification of a data type for
the various items within the tuple.

It looks like this:

DataPair = NamedTuple('DataPair', [
    ('x', float),
    ('y', float)
  ]
)

We use typing.NamedTuple() almost
exactly the same way we use
collection.namedtuple(). The
definition of the attributes uses a
list of two-tuples instead of a
list of names. The two-tuples have
a name and a type definition.

This supplemental type definition
is used by mypy to determine
whether the NamedTuple objects are
being populated correctly. It can
also be used by other people to
understand the code and make proper
modifications or extensions.

In Python, we can replace some
stateful objects with immutable
objects. There are a number of
limitations, though. The
collections such as list, set, and
dict, must remain as mutable
objects. Replacing these
collections with immutable monads
can work out well in other
programming languages, but it's not
a part of Python.


Writing recursive generator functions with the yield from statement

There are a number of algorithms
that can be expressed neatly as
recursions. In the Designing
recursive functions around Python's
stack limits recipe, we looked at
some recursive functions that could
be optimized to reduce the number
of function calls.

When we look at some data
structures, we see that they
involve recursion. In particular,
JSON documents (as well as XML and
HTML documents) can have a
recursive structure. A JSON
document might include a complex
object that contains other complex
objects within it.

In many cases, there are advantages
to using generators for processing
these kinds of structures. How can
we write generators that work with
recursion? How does the yield from
statement save us from writing an
extra loop?

    evince -p 414 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 443]

Getting ready

We'll look at a way to search an
ordered collection for all matching
values in a complex data structure.
When working with complex JSON
documents, we'll often model them
as dict-ofdict, and dict-of-list
structures. Of course, a JSON
document is not a two-level thing;
dict-ofdict really means
dict-of-dict-of.... Similarly,
dict-of-list really means
dict-of-list-of... These are
recursive structures, which means a
search must descend through the
entire structure looking for a
particular key or value.

A document with this complex
structure might look like this:

document = {
  "field": "value1",
  "field2": "value",
  "array": [
    {"array_item_key1": "value"},
    {"array_item_key2": "array_item_value2"}
  ],
  "object": {
    "attribute1": "value",
    "attribute2": "value2"
  }
}

This shows a document that has four
keys, field, field2, array, and
object. Each of these keys has a
distinct data structure as its
associated value. Some of the
values are unique, and some are
duplicated. This duplication is the
reason why our search must find all
instances inside the overall
document.

The core algorithm is a depth-first
search. The output from this
function will be a list of paths
that identify the target value.
Each path will be a sequence of
field names or field names mixed
with index positions.

In the previous example, the value
value can be found in three places:

- ["array", 0, "array_item_key1"]:
  This path starts with the
  top-level field named array, then
  visits item zero of a list, then
  a field named array_item_key1
- ["field2"]: This path has just a
  single field name where the value
  is found
- ["object", "attribute1"]: This
  path starts with the top-level
  field named object, then the
  child attribute1 of that field

    evince -p 415 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 444]

The find_value() function yield
both of these paths when it
searches the overall document for
the target value. Here's the
conceptual overview of this search
function:

def find_path(value, node, path=[]):
  if isinstance(node, dict):
    for key in node.keys():
      # find_value(value, node[key], path+[key])
      # This must yield multiple values
  elif isinstance(node, list):
    for index in range(len(node)):
      # find_value(value, node[index], path+[index])
      # This will yield multiple values
  else:
    # a primitive type
    if node == value:
      yield path

There are three alternatives in the
find_path() process:

- When the node is a dictionary,
  the value of each key must be
  examined. The values may be any
  kind of data, so we'll use the
  find_path() function recursively
  on each value. This will yield a
  sequence of matches.
- If node is a list, the items for
  each index position must be
  examined. The items may be any
  kind of data, so we'll use the
  find_path() function recursively
  on each value. This will yield a
  sequence of matches.
- The other choice is for the node
  to be a primitive value. The JSON
  specification lists a number of
  primitives that may be present in
  a valid document. If the node
  value is the target value, we've
  found one instance, and can yield
  this single match.

There are two ways to handle the
recursion. One is like this:

for match in find_value(value, node[key], path+[key]):
  yield match

This seems to have too much
boilerplate for such a simple idea.
The other way is simpler and a bit
clearer.

    evince -p 416 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 445]

How to do it...

- 1. Write out the complete for
  statement:

for match in find_value(value, node[key], path+[key]):
  yield match

For debugging purposes, we might
insert a print() function inside
the body of the for statement.

- 2. Replace this with a yield from
  statement once we're sure things
  work:

yield from find_value(value, node[key], path+[key])

The complete depth-first
find_value() search function will
look like this:

def find_path(value, node, path=[]):
  if isinstance(node, dict):
    for key in node.keys():
      yield from find_path(value, node[key], path+[key])
  elif isinstance(node, list):
    for index in range(len(node)):
      yield from find_path(value, node[index], path+[index])
  else:
    if node == value:
      yield path

When we use the find_path()
function, it looks like this:

>>> list(find_path('array_item_value2', document))
[['array', 1, 'array_item_key2']]

The find_path() function is
iterable. It can yield a number of
values. We consumed all of the
results to create a list. In this
example, the list had one item,
['array', 1, 'array_item_key2'].
This item has the path to the
matching item.

We can then evaluate
document['array'][1]['array_item_key2']
to find the referenced value.

    evince -p 417 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 446]

When we look for a non-unique
value, we might see a list like
this:

>>> list(find_value('value', document))
[['array', 0, 'array_item_key1'],
 ['field2'],
 ['object', 'attribute1']]

The resulting list has three items.
Each of these provides the path to
an item with the target value of
value.


How it works...

The yield from X statement is
shorthand for:

for item in X:
  yield item

This lets us write a succinct
recursive algorithm that will
behave as an iterator and properly
yield multiple values.

This can also be used in contexts
that don't involve a recursive
function. It's entirely sensible to
use a yield from statement anywhere
that an iterable result is
involved. It's a big simplification
for recursive functions, however,
because it preserves a clearly
recursive structure.


There's more...

Another common style of definition
assembles a list using append
operations. We can rewrite this
into an iterator and avoid the
overhead of building a list object.

When factoring a number, we can
define the set of prime factors
like this:

___Insert_Figure

If the value, x, is prime, it has
only itself in the set of prime
factors. Otherwise, there must be
some prime number, n, which is the
least factor of x. We can assemble
a set of factors starting with n
and including all factors of x/n.
In order to be sure that only prime
factors are found, then n must be
prime. If we search in ascending
order, we'll find prime factors
before finding composite factors.

    evince -p 418 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 447]

We have two ways to implement this
in Python: one builds a list, the
other generates factors. Here's a
list-building function:

import math
def factor_list(x):
  limit = int(math.sqrt(x)+1)
  for n in range(2, limit):
    q, r = divmod(x, n)
    if r == 0:
      return [n] + factor_list(q)
  return [x]

This factor_list() function will
search all numbers, n, such that 2
≤ n < √x. The first number that's a
factor of x will be the least
factor. It will also be prime.
We'll - of course - search a number
of composite values, wasting time.
For example, after testing two and
three, we'll also test values for
line four and six, even though
they're composite and all of their
factors have already been tested.

This function builds a list object.
If a factor, n, is found, it will
start a list with that factor. It
will append factors from x // n. If
there are no factors of x, then the
value is prime, and we return a
list with just that value.

We can rewrite this to be an
iterator by replacing the recursive
calls with yield from. The function
will look like this:

def factor_iter(x):
  limit = int(math.sqrt(x)+1)
  for n in range(2, limit):
    q, r = divmod(x, n)
    if r == 0:
      yield n
      yield from factor_iter(q)
      return
  yield x

As with the list-building version,
this will search numbers, n, such
that . When a factor is found, then
the function will yield the factor,
followed by any other factors found
by a recursive call to
factor_iter(). If no factors are
found, the function will yield just
the prime number and nothing more.

    evince -p 419 ~/Empire/Doks/Comp/lang/py/mpycb.pdf &
      [-p 448]

Using an iterator allows us to
build any kind of collection from
the factors. Instead of being
limited to always creating a list,
we can create a multiset using the
collection.Counter class. It would
look like this:

>>> from collections import Counter
>>> Counter(factor_iter(384))
Counter({2: 7, 3: 1})

This shows us that:

384 = 27 × 3

In some cases, this kind of
multiset is easier to work with
than the list of factors.


See also

- In the Designing recursive
  functions around Python's stack
  limits recipe, we cover the core
  design patterns for recursive
  functions. This recipe provides
  an alternative way to create the
  results.


___FluPy
    evince -p 363 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Part III. Functions as Objects

    evince -p 364 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter 7 - Functions as First-Class Objects

  A Note For Early Release Readers

  With Early Release ebooks, you
  get books in their earliest form
  - the author's raw and unedited
  content as they write - so you
  can take advantage of these
  technologies long before the
  official release of these titles.

  This will be the 7th chapter of
  the final book. Please note that
  the GitHub repo will be made
  active later on.

  If you have comments about how we
  might improve the content and/or
  examples in this book, or if you
  notice missing material within
  this chapter, please reach out to
  the author at
  fluentpython2e@ramalho.org.

  "I have never considered Python
  to be heavily influenced by
  functional languages, no matter
  what people say or think. I was
  much more familiar with
  imperative languages such as C
  and Algol 68 and although I had
  made functions first-class
  objects, I didn't view Python as
  a functional programming
  language<1>.

  - Guido van Rossum, Python BDFL

Functions in Python are first-class
objects. Programming language
researchers define a "first-class
object" as a program entity that
can be:

- created at runtime;
- assigned to a variable or element
  in a data structure;
- passed as an argument to a
  function;
- returned as the result of a
  function.

    evince -p 365 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Integers, strings, and dictionaries
are other examples of first-class
objects in Python - nothing fancy
here. Having functions as
first-class objects is an essential
feature of functional languages,
such as Clojure, Elixir, and
Haskell. However, first-class
functions are so useful that
they've been adopted by popular
languages like JavaScript, Go, and
Java (since JDK 8), none of which
claim to be "functional languages."

This chapter and most of Part III
explore the practical applications
of treating functions as objects.

  Tip: The term "first-class
  functions" is widely used as
  shorthand for "functions as
  first-class objects." It's not
  ideal because it implies an
  "elite" among functions. In
  Python, all functions are
  first-class.


What's new in this chapter

Section "The Nine Flavors of
Callable Objects" was titled "The
Seven Flavors of Callable Objects"
in the First Edition. The new
callables are native coroutines and
asynchronous generators, introduced
in Python 3.5 and 3.6,
respectively. Both are covered in
Chapter 21, but they are mentioned
here along with the other callables
for completeness.

"Positional-only parameters" is a
new section, covering a feature
added in Python 3.8.

I moved coverage of runtime access
to function annotations to "Reading
Type Hints at Runtime". When I
wrote the First Edition, PEP 484 -
Type Hints - was still under
consideration, and people used
annotations in different ways.
Since Python 3.5, annotations
should conform to PEP 484.
Therefore, the best place to cover
them is when discussing type hints.

    evince -p 366 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Note: The First Edition had
  sections about the introspection
  of function objects that were too
  low-level and distracted from the
  main subject of this chapter. I
  merged those sections into a post
  titled Introspection of Function
  Parameters at fluentpython.com.

Now let's see why Python functions
are full-fledged objects.


Treating a Function Like an Object

The console session in Example 7-1
shows that Python functions are
objects. Here we create a function,
call it, read its __doc__
attribute, and check that the
function object itself is an
instance of the function class.

Example 7-1. Create and test a function, then read its __doc__ and check its type

# 1
>>> def factorial(n):
...   """returns n!"""
...   return 1 if n < 2 else n * factorial(n - 1)
...
>>> factorial(42)
1405006117752879898543142606244511569936384000000000
# 2
>>> factorial.__doc__
'returns n!'
# 3
>>> type(factorial)
<class 'function'>

- 1. This is a console session, so
  we're creating a function at
  "runtime."
- 2. __doc__ is one of several
  attributes of function objects.
- 3. factorial is an instance of
  the function class.

The __doc__ attribute is used to
generate the help text of an
object. In the Python console, the
command help(factorial) will
display a screen like Figure 7-1.

    evince -p 367 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &


Figure 7-1. Help screen for factorial; the text is built from the __doc__ attribute of the function.

Example 7-2 shows the "first class"
nature of a function object. We can
assign it a variable fact and call
it through that name. We can also
pass factorial as an argument to
the map function. Calling
map(function, iterable) returns an
iterable where each item is the
result of calling the first
argument (a function) to successive
elements of the second argument (an
iterable), range(10) in this
example.

Example 7-2. Use function through a different name, and pass function as argument

>>> fact = factorial
>>> fact
<function factorial at 0x...>
>>> fact(5)
120
>>> map(factorial, range(11))
<map object at 0x...>
>>> list(map(factorial, range(11)))
[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]

Having first-class functions
enables programming in a functional
style. One of the hallmarks of
functional programming is the use
of higher-order functions, our next
topic.


Higher-Order Functions

A function that takes a function as
argument or returns a function as
the result is a higher-order
function. One example is map, shown
in Example 7-2.

    evince -p 368 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Another is the built-in function
sorted: the optional key argument
lets you provide a function to be
applied to each item for sorting,
as we saw in "list.sort versus the
sorted Built-In". For example, to
sort a list of words by length,
pass the len function as the key,
as in Example 7-3.

Example 7-3. Sorting a list of words by length

>>> fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']
>>> sorted(fruits, key=len)
['fig', 'apple', 'cherry', 'banana', 'raspberry', 'strawberry']
>>>

Any one-argument function can be
used as the key. For example, to
create a rhyme dictionary it might
be useful to sort each word spelled
backward. In Example 7-4, note that
the words in the list are not
changed at all; only their reversed
spelling is used as the sort
criterion, so that the berries
appear together.

Example 7-4. Sorting a list of words by their reversed spelling

>>> def reverse(word):
...   return word[::-1]
>>> reverse('testing')
'gnitset'
>>> sorted(fruits, key=reverse)
['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']
>>>

In the functional programming
paradigm, some of the best known
higher-order functions are map,
filter, reduce, and apply. The
apply function was deprecated in
Python 2.3 and removed in Python 3
because it's no longer necessary.
If you need to call a function with
a dynamic set of arguments, you can
write fn(*args, **kwargs) instead
of apply(fn, args, kwargs).

The map, filter, and reduce
higher-order functions are still
around, but better alternatives are
available for most of their use
cases, as the next section shows.

    evince -p 369 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Modern Replacements for map, filter, and reduce

Functional languages commonly offer
the map, filter, and reduce
higher-order functions (sometimes
with different names). The map and
filter functions are still
built-ins in Python 3, but since
the introduction of list
comprehensions and generator
expressions, they are not as
important. A listcomp or a genexp
does the job of map and filter
combined, but is more readable.
Consider Example 7-5.

Example 7-5. Lists of factorials produced with map and filter compared to alternatives coded as list comprehensions

# 1
>>> list(map(factorial, range(6)))
[1, 1, 2, 6, 24, 120]
# 2
>>> [factorial(n) for n in range(6)]
[1, 1, 2, 6, 24, 120]
# 3
>>> list(map(factorial, filter(lambda n: n % 2, range(6))))
[1, 6, 120]
# 4
>>> [factorial(n) for n in range(6) if n % 2]
[1, 6, 120]
>>>

- 1. Build a list of factorials
  from 0! to 5!.
- 2. Same operation, with a list
  comprehension.
- 3. List of factorials of odd
  numbers up to 5!, using both map
  and filter.
- 4. List comprehension does the
  same job, replacing map and
  filter, and making lambda
  unnecessary.

In Python 3, map and filter return
generators - a form of iterator -
so their direct substitute is now a
generator expression (in Python 2,
these functions returned lists,
therefore their closest alternative
is a listcomp).

The reduce function was demoted
from a built-in in Python 2 to the
functools module in Python 3. Its
most common use case, summation, is
better served by the sum built-in
available since Python 2.3 was
released in 2003. This is a big win
in terms of readability and
performance (see Example 7-6).

    evince -p 370 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 7-6. Sum of integers up to 99 performed with reduce and sum

# 1
>>> from functools import reduce
# 2
>>> from operator import add
# 3
>>> reduce(add, range(100))
4950
# 4
>>> sum(range(100))
4950
>>>

- 1. Starting with Python 3.0,
  reduce is no longer a built-in.
- 2. Import add to avoid creating a
  function just to add two numbers.
- 3. Sum integers up to 99.
- 4. Same task with sum - no need
  to import and call reduce and
  add.

Note: The common idea of sum and
reduce is to apply some operation
to successive items in a
sequence, accumulating previous
results, thus reducing a sequence
of values to a single value.

Other reducing built-ins are all
and any:

all(iterable)

Returns True if there are no
falsy elements in the iterable;
all([]) returns True.

any(iterable)

Returns True if any element of
the iterable is truthy; any([])
returns False.

I give a fuller explanation of
reduce in "Vector Take #4: Hashing
and a Faster ==" where an ongoing
example provides a meaningful
context for the use of this
function. The reducing functions
are summarized later in the book
when iterables are in focus, in
"Iterable Reducing Functions".

    evince -p 371 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

To use a higher-order function,
sometimes it is convenient to
create a small, one-off function.
That is why anonymous functions
exist. We'll cover them next.


Anonymous Functions

The lambda keyword creates an
anonymous function within a Python
expression.

However, the simple syntax of
Python limits the body of lambda
functions to be pure expressions.
In other words, the body cannot
contain other Python statements
such as while, try, etc. Assignment
with = is also a statement, so it
cannot occur in a lambda. The new
assignment expression syntax using
:= can be used - but if you need
it, your lambda is probably too
complicated and hard to read, and
it should be refactored into a
regular function using def.

The best use of anonymous functions
is in the context of an argument
list for a higher-order function.
For example, Example 7-7 is the
rhyme index example from Example
7-4 rewritten with lambda, without
defining a reverse function.

Example 7-7. Sorting a list of words by their reversed spelling using lambda

>>> fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']
>>> sorted(fruits, key=lambda word: word[::-1])
['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']
>>>

Outside the limited context of
arguments to higher-order
functions, anonymous functions are
rarely useful in Python. The
syntactic restrictions tend to make
nontrivial lambdas either
unreadable or unworkable. If a
lambda is hard to read, I strongly
advise you follow Fredrik Lundh's
refactoring advice.

    evince -p 372 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Fredrik Lundh's Lambda Refactoring Recipe

If you find a piece of code hard
to understand because of a
lambda, Fredrik Lundh suggests
this refactoring procedure:

- 1. Write a comment explaining
  what the heck that lambda does.
- 2. Study the comment for a while,
  and think of a name that captures
  the essence of the comment.
- 3. Convert the lambda to a def
  statement, using that name.
- 4. Remove the comment.

These steps are quoted from the
Functional Programming HOWTO, a
must read.

The lambda syntax is just syntactic
sugar: a lambda expression creates
a function object just like the def
statement. That is just one of
several kinds of callable objects
in Python. The following section
reviews all of them.


The Nine Flavors of Callable Objects

The call operator () may be applied
to other objects beyond
user-defined functions and lambdas.
To determine whether an object is
callable, use the callable()
built-in function. As of Python
3.9, the Data Model documentation
lists nine callable types:

User-defined functions

  Created with def statements or
  lambda expressions.

Built-in functions

  A function implemented in C (for
  CPython), like len or
  time.strftime.

Built-in methods

  Methods implemented in C, like
  dict.get.

    evince -p 373 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Methods

  Functions defined in the body of
  a class.

Classes

  When invoked, a class runs its
  __new__ method to create an
  instance, then __init__ to
  initialize it, and finally the
  instance is returned to the
  caller. Because there is no new
  operator in Python, calling a
  class is like calling a
  function<2>.

Class instances

  If a class defines a __call__
  method, then its instances may be
  invoked as functions - that's the
  subject of the next section.

Generator functions

  Functions or methods that use the
  yield keyword in their body. When
  called, they return a generator
  object.

Native coroutine functions

  Functions or methods defined with
  async def. When called, they
  return a coroutine object. Added
  in Python 3.5.

Asynchronous generator functions

  Functions or methods defined with
  async def that have yield in
  their body. When called, they
  return an asynchronous generator
  for use with async for. Added in
  Python 3.6.

Generators, native coroutines, and
asynchronous generator functions
are unlike other callables in that
their return values are never
application data, but objects that
require further processing to yield
application data or perform useful
work. Generator functions return
iterators.

    evince -p 374 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Both are covered in Chapter 17.
Native coroutine functions and
asynchronous generator functions
return objects that only work with
the help of an asynchronous
programming framework, such as
asyncio. They are the subject of
Chapter 21.

  Tip: Given the variety of
  existing callable types in
  Python, the safest way to
  determine whether an object is
  callable is to use the callable()
  built-in:

  >>> abs, str, 'Ni!'
  (<built-in function abs>, <class 'str'>, 'Ni!')
  >>> [callable(obj) for obj in (abs, str, 'Ni!')]
  [True, True, False]

We now move on to building class
instances that work as callable
objects.


User-Defined Callable Types

Not only are Python functions real
objects, but arbitrary Python
objects may also be made to behave
like functions. Implementing a
__call__ instance method is all it
takes.

Example 7-8 implements a BingoCage
class. An instance is built from
any iterable, and stores an
internal list of items, in random
order. Calling the instance pops an
item<3>.

Example 7-8. bingocall.py: A BingoCage does one thing: picks items from a shuffled list

import random

class BingoCage:

  def __init__(self, items):
    # 1
    self._items = list(items)
    # 2
    random.shuffle(self._items)

  # 3
  def pick(self):
    try:
      return self._items.pop()
    except IndexError:
      # 4
      raise LookupError('pick from empty BingoCage')

  # 5
  def __call__(self):
    return self.pick()

    evince -p 375 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

- 1. __init__ accepts any iterable;
  building a local copy prevents
  unexpected side effects on any
  list passed as an argument.
- 2. shuffle is guaranteed to work
  because self._items is a list.
- 3. The main method.
- 4. Raise exception with custom
  message if self._items is empty.
- 5. Shortcut to bingo.pick():
  bingo().

Here is a simple demo of Example
7-8. Note how a bingo instance can
be invoked as a function, and the
callable() built-in recognizes it
as a callable object:

>>> bingo = BingoCage(range(3))
>>> bingo.pick()
1
>>> bingo()
0
>>> callable(bingo)
True

A class implementing __call__ is an
easy way to create function-like
objects that have some internal
state that must be kept across
invocations, like the remaining
items in the BingoCage. Another
good use case for __call__ is
implementing decorators. Decorators
must be callable, and it is
sometimes convenient to "remember"
something between calls of the
decorator (e.g., for memoization -
caching the results of expensive
computations for later use) or to
split a complex implementation into
separate methods.

    evince -p 376 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

The functional approach to creating
functions with internal state is to
use closures. Closures, as well as
decorators, are the subject of
Chapter 9.

Now let's explore the powerful
syntax Python offers to declare
function parameters and pass
arguments into them.


From Positional to Keyword-Only Parameters

One of the best features of Python
functions is the extremely flexible
parameter handling mechanism.
Closely related are the use of *
and ** to unpack iterables and
mappings into separate arguments
when we call a function. To see
these features in action, see the
code for Example 7-9 and tests
showing its use in Example 7-10.

Example 7-9. tag generates HTML elements; a keyword-only argument class_ is used to pass "class" attributes as a workaround because class is a keyword in Python

def tag(name, *content, class_=None, **attrs):
  """Generate one or more HTML tags"""
  if class_ is not None:
    attrs['class'] = class_
  attr_pairs = (f' {attr}="{value}"' for attr, value
                in sorted(attrs.items()))
  attr_str = ''.join(attr_pairs)
  if content:
    elements = (f'<{name}{attr_str}>{c}</{name}>'
                for c in content)
    return '\n'.join(elements)
  else:
    return f'<{name}{attr_str} />'

The tag function can be invoked in
many ways, as Example 7-10 shows.

Example 7-10. Some of the many ways
of calling the tag function from
Example 7-9

    evince -p 377 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

# 1
>>> tag('br')
'<br />'
# 2
>>> tag('p', 'hello')
'<p>hello</p>'
>>> print(tag('p', 'hello', 'world'))
<p>hello</p>
<p>world</p>
# 3
>>> tag('p', 'hello', id=33)
'<p id="33">hello</p>'
# 4
>>> print(tag('p', 'hello', 'world', class_='sidebar'))
<p class="sidebar">hello</p>
<p class="sidebar">world</p>
# 5
>>> tag(content='testing', name="img")
'<img content="testing" />'
>>> my_tag = {'name': 'img', 'title': 'Sunset Boulevard',
...           'src': 'sunset.jpg', 'class': 'framed'}
# 6
>>> tag(**my_tag)
'<img class="framed" src="sunset.jpg" title="Sunset Boulevard" />'

- 1. A single positional argument
  produces an empty tag with that
  name.
- 2. Any number of arguments after
  the first are captured by
  *content as a tuple.
- 3. Keyword arguments not
  explicitly named in the tag
  signature are captured by **attrs
  as a dict.
- 4. The class_ parameter can only
  be passed as a keyword argument.
- 5. The first positional argument
  can also be passed as a keyword.
- 6. Prefixing the my_tag dict with
  ** passes all its items as
  separate arguments, which are
  then bound to the named
  parameters, with the remaining
  caught by **attrs. In this case
  we can have a 'class' key in the
  arguments dict, because it is a
  string, and does not clash with
  the class reserved word.

Keyword-only arguments are a
feature of Python 3. In Example
7-9, the class_ parameter can only
be given as a keyword argument - it
will never capture unnamed
positional arguments. To specify
keyword-only arguments when
defining a function, name them
after the argument prefixed with *.
If you don't want to support
variable positional arguments but
still want keyword-only arguments,
put a * by itself in the signature,
like this:

    evince -p 378 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> def f(a, *, b):
...   return a, b
...
>>> f(1, b=2)
(1, 2)
>>> f(1, 2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: f() takes 1 positional argument but 2 were given

Note that keyword-only arguments do
not need to have a default value:
they can be mandatory, like b in
the preceding example.


Positional-only parameters

Since Python 3.8, user-defined
function signatures may specify
positionalonly parameters. This
feature always existed for built-in
functions, such as divmod(a, b),
which can only be called with
positional parameters, and not as
divmod(a=10, b=4).

To define a function requiring
positional-only parameters, use /
in the parameter list.

This example from What's New In
Python 3.8 shows how to emulate the
divmod built-in function:

def divmod(a, b, /):
  return (a // b, a % b)

All arguments to the left of the /
are positional-only. After the /,
you may specify other arguments,
which work as usual.

    evince -p 379 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Warning: The / in the parameter
  list is a syntax error in Python
  3.7 or earlier.

For example, consider the tag
function from Example 7-9. If we
want the name parameter to be
positional only, we can add a /
after it in the function signature,
like this:

def tag(name, /, *content, class_=None, **attrs):
  ...

You can find other examples of
positional-only parameters in
What's New In Python 3.8 and in PEP
570.

After diving into Python's flexible
argument declaration features, the
remainder of this chapter covers
the most useful packages in the
standard library for programming in
a functional style.


Packages for Functional Programming

Although Guido makes it clear that
he did not design Python to be a
functional programming language, a
functional coding style can be used
to good extent, thanks to
first-class functions, pattern
matching, and the support of
packages like operator and
functools, which we cover in the
next two sections.


The operator Module

Often in functional programming it
is convenient to use an arithmetic
operator as a function. For
example, suppose you want to
multiply a sequence of numbers to
calculate factorials without using
recursion. To perform summation,
you can use sum, but there is no
equivalent function for
multiplication. You could use
reduce - as we saw in "Modern
Replacements for map, filter, and
reduce" - but this requires a
function to multiply two items of
the sequence. Example 7-11 shows
how to solve this using lambda.

    evince -p 380 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 7-11. Factorial implemented with reduce and an anonymous function

from functools import reduce

def factorial(n):
  return reduce(lambda a, b: a*b, range(1, n+1))

The operator module provides
function equivalents for dozens of
operators so you don't have to code
trivial functions like lambda a, b:
a*b. With it, we can rewrite
Example 7-11 as Example 7-12.

Example 7-12. Factorial implemented with reduce and operator.mul

from functools import reduce
from operator import mul

def factorial(n):
  return reduce(mul, range(1, n+1))

Another group of one-trick lambdas
that operator replaces are
functions to pick items from
sequences or read attributes from
objects: itemgetter and attrgetter
are factories that build custom
functions to do that.

Example 7-13 shows a common use of
itemgetter: sorting a list of
tuples by the value of one field.
In the example, the cities are
printed sorted by country code
(field 1). Essentially,
itemgetter(1) creates a function
that, given a collection, returns
the item at index 1. That's easier
to write and read than lambda
fields: fields[1], which does the
same:

Example 7-13. Demo of itemgetter to
sort a list of tuples (data from
Example 2-8)

>>> metro_data = [
...   ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),
...   ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),
...   ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),
...   ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),
...   ('São Paulo', 'BR', 19.649, (-23.547778, -46.635833)),
... ]
>>>
>>> from operator import itemgetter
>>> for city in sorted(metro_data, key=itemgetter(1)):
...   print(city)
...
('São Paulo', 'BR', 19.649, (-23.547778, -46.635833))
('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))
('Tokyo', 'JP', 36.933, (35.689722, 139.691667))
('Mexico City', 'MX', 20.142, (19.433333, -99.133333))
('New York-Newark', 'US', 20.104, (40.808611, -74.020386))

    evince -p 381 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

If you pass multiple index
arguments to itemgetter, the
function it builds will return
tuples with the extracted values,
which is useful for sorting on
multiple keys:

>>> cc_name = itemgetter(1, 0)
>>> for city in metro_data:
...   print(cc_name(city))
...
('JP', 'Tokyo')
('IN', 'Delhi NCR')
('MX', 'Mexico City')
('US', 'New York-Newark')
('BR', 'São Paulo')
>>>

Because itemgetter uses the []
operator, it supports not only
sequences but also mappings and any
class that implements __getitem__.

A sibling of itemgetter is
attrgetter, which creates functions
to extract object attributes by
name. If you pass attrgetter
several attribute names as
arguments, it also returns a tuple
of values. In addition, if any
argument name contains a . (dot),
attrgetter navigates through nested
objects to retrieve the attribute.
These behaviors are shown in
Example 7-14. This is not the
shortest console session because we
need to build a nested structure to
showcase the handling of dotted
attributes by attrgetter.

Example 7-14. Demo of attrgetter to
process a previously defined list
of namedtuple called metro_data
(the same list that appears in
Example 7-13)

    evince -p 382 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> from collections import namedtuple
# 1
>>> LatLon = namedtuple('LatLon', 'lat lon')
# 2
>>> Metropolis = namedtuple('Metropolis', 'name cc pop coord')
# 3
>>> metro_areas = [Metropolis(name, cc, pop, LatLon(lat, lon))
...   for name, cc, pop, (lat, lon) in metro_data]
>>> metro_areas[0]
Metropolis(name='Tokyo', cc='JP', pop=36.933,
coord=LatLon(lat=35.689722,
lon=139.691667))
# 4
>>> metro_areas[0].coord.lat
35.689722
>>> from operator import attrgetter
# 5
>>> name_lat = attrgetter('name', 'coord.lat')
>>>
# 6
>>> for city in sorted(metro_areas, key=attrgetter('coord.lat')):
  # 7
...   print(name_lat(city))
...
('São Paulo', -23.547778)
('Mexico City', 19.433333)
('Delhi NCR', 28.613889)
('Tokyo', 35.689722)
('New York-Newark', 40.808611)

- 1. Use namedtuple to define
  LatLon.
- 2. Also define Metropolis.
- 3. Build metro_areas list with
  Metropolis instances; note the
  nested tuple unpacking to extract
  (lat, lon) and use them to build
  the LatLon for the coord
  attribute of Metropolis.
- 4. Reach into element
  metro_areas[0] to get its
  latitude.
- 5. Define an attrgetter to
  retrieve the name and the
  coord.lat nested attribute.
- 6. Use attrgetter again to sort
  list of cities by latitude.
- 7. Use the attrgetter defined in
  mark 5 to show only city name and
  latitude.

    evince -p 383 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Here is a partial list of functions
defined in operator (names starting
with _ are omitted, because they
are mostly implementation details):

>>> [name for name in dir(operator) if not name.startswith('_')]
['abs', 'add', 'and_', 'attrgetter', 'concat', 'contains',
'countOf', 'delitem', 'eq', 'floordiv', 'ge', 'getitem', 'gt',
'iadd', 'iand', 'iconcat', 'ifloordiv', 'ilshift', 'imatmul',
'imod', 'imul', 'index', 'indexOf', 'inv', 'invert', 'ior',
'ipow', 'irshift', 'is_', 'is_not', 'isub', 'itemgetter',
'itruediv', 'ixor', 'le', 'length_hint', 'lshift', 'lt',
'matmul',
'methodcaller', 'mod', 'mul', 'ne', 'neg', 'not_', 'or_', 'pos',
'pow', 'rshift', 'setitem', 'sub', 'truediv', 'truth', 'xor']

Most of the 54 names listed are
self-evident. The group of names
prefixed with i and the name of
another operator - e.g., iadd,
iand, etc. - correspond to the
augmented assignment operators -
e.g., +=, &=, etc. These change
their first argument in place, if
it is mutable; if not, the function
works like the one without the i
prefix: it simply returns the
result of the operation.

Of the remaining operator
functions, methodcaller is the last
we will cover. It is somewhat
similar to attrgetter and
itemgetter in that it creates a
function on the fly. The function
it creates calls a method by name
on the object given as argument, as
shown in Example 7-15.

Example 7-15. Demo of methodcaller: second test shows the binding of extra arguments

>>> from operator import methodcaller
>>> s = 'The time has come'
>>> upcase = methodcaller('upper')
>>> upcase(s)
'THE TIME HAS COME'
>>> hyphenate = methodcaller('replace', ' ', '-')
>>> hyphenate(s)
'The-time-has-come'

The first test in Example 7-15 is
there just to show methodcaller at
work, but if you need to use the
str.upper as a function, you can
just call it on the str class and
pass a string as argument, like
this:

    evince -p 384 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> str.upper(s)
'THE TIME HAS COME'

The second test in Example 7-15
shows that methodcaller can also do
a partial application to freeze
some arguments, like the
functools.partial function does.
That is our next subject.


Freezing Arguments with functools.partial

The functools module provides
several higher-order functions. We
saw reduce in "Modern Replacements
for map, filter, and reduce".
Another is partial: given a
callable, it produces a new
callable with some of the arguments
of the original callable bound to
pre-determined values. This is
useful to adapt a function that
takes one or more arguments to an
API that requires a callback with
fewer arguments. Example 7-16 is a
trivial demonstration.

Example 7-16. Using partial to use a two-argument function where a oneargument callable is required

>>> from operator import mul
>>> from functools import partial
# 1
>>> triple = partial(mul, 3)
# 2
>>> triple(7)
21
# 3
>>> list(map(triple, range(1, 10)))
[3, 6, 9, 12, 15, 18, 21, 24, 27]

- 1. Create new triple function
  from mul, binding first
  positional argument to 3.
- 2. Test it.
- 3. Use triple with map; mul would
  not work with map in this
  example.

A more useful example involves the
unicode.normalize function that we
saw in "Normalizing Unicode for
Reliable Comparisons". If you work
with text from many languages, you
may want to apply
unicode.normalize('NFC', s) to any
string s before comparing or
storing it.

    evince -p 385 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

If you do that often,
it's handy to have an nfc function
to do so, as in Example 7-17.

Example 7-17. Building a convenient Unicode normalizing function with partial

>>> import unicodedata, functools
>>> nfc = functools.partial(unicodedata.normalize, 'NFC')
>>> s1 = 'café'
>>> s2 = 'cafe\u0301'
>>> s1, s2
('café', 'café')
>>> s1 == s2
False
>>> nfc(s1) == nfc(s2)
True

partial takes a callable as first
argument, followed by an arbitrary
number of positional and keyword
arguments to bind.

Example 7-18 shows the use of
partial with the tag function from

Example 7-9, to freeze one
positional argument and one keyword
argument.

Example 7-18. Demo of partial applied to the function tag from Example 7-9

>>> from tagger import tag
>>> tag
# 1
<function tag at 0x10206d1e0>
>>> from functools import partial
# 2
>>> picture = partial(tag, 'img', class_='pic-frame')
>>> picture(src='wumpus.jpeg')
# 3
'<img class="pic-frame" src="wumpus.jpeg" />'
>>> picture
# 4
functools.partial(<function tag at 0x10206d1e0>, 'img', class_='pic-frame')
# 5
>>> picture.func
<function tag at 0x10206d1e0>
>>> picture.args
('img',)
>>> picture.keywords
{'class_': 'pic-frame'}

    evince -p 386 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

- 1. Import tag from Example 7-9
  and show its ID.
- 2. Create picture function from
  tag by fixing the first
  positional argument with 'img'
  and the class_ keyword argument
  with 'pic-frame'.
- 3. picture works as expected.
- 4. partial() returns a
  functools.partial object<4>.
- 5. A functools.partial object has
  attributes providing access to
  the original function and the
  fixed arguments.

The functools.partialmethod
function does the same job as
partial, but is designed to work
with methods.

The functools module also include
higher-order functions designed to
be used as function decorators,
such as cache and singledispatch,
among others. Those functions are
the covered in Chapter 9, which
also explains how to implement
custom decorators.

    evince -p 387 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter Summary

The goal of this chapter was to
explore the first-class nature of
functions in Python. The main ideas
are that you can assign functions
to variables, pass them to other
functions, store them in data
structures, and access function
attributes, allowing frameworks and
tools to act on that information.

Higher-order functions, a staple of
functional programming, are common
in Python. The sorted, min, and max
built-ins, and functools.partial
are examples of commonly used
higher-order functions in the
language. Using map, filter, and
reduce is not as common as it used
to be - thanks to list
comprehensions (and similar
constructs like generator
expressions) and the addition of
reducing built-ins like sum, all,
and any.

Callables come in nine different
flavors since Python 3.6, from the
simple functions created with
lambda to instances of classes
implementing __call__. Generators
and coroutines are also callable,
although their behavior is very
different from other callables. All
callables can be detected by the
callable() built-in. Callables
offer rich syntax for declaring
formal parameters, including
keyword-only parameters,
positional-only paramenters, and
annotations.

Lastly, we covered some functions
from the operator module and
functools.partial, which facilitate
functional programming by
minimizing the need for the
functionally challenged lambda
syntax.


Further Reading

The next chapters continue our
exploration of programming with
function objects. Chapter 8 is
devoted to type hints in function
parameters and return values.
Chapter 9 dives into function
decorators - a special kind of
higher-order function - and the
closure mechanism that makes them
work. Chapter 10 shows how
first-class functions can simplify
some classic Object-Oriented design
patterns.

    evince -p 388 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

In The Python Language Reference,
"3.2. The standard type hierarchy"
presents the nine callable types,
along with all the other built-in
types.

Chapter 7 of the Python Cookbook,
Third Edition (O'Reilly), by David
Beazley and Brian K. Jones, is an
excellent complement to the current
chapter as well as Chapter 9 of
this book, covering mostly the same
concepts with a different approach.

See PEP 3102 - Keyword-Only
Arguments if you are interested in
the rationale and use cases for
that feature.

A great introduction to functional
programming in Python is A. M.
Kuchling's Python Functional
Programming HOWTO. The main focus
of that text, however, is the use
of iterators and generators, which
are the subject of Chapter 17.

The StackOverflow question "Python:
Why is functools.partial
necessary?" has a highly
informative (and funny) reply by
Alex Martelli, co-author of the
classic Python in a Nutshell.

Reflecting on the question "Is
Python a functional language?", I
created one of my favorite talks:
Beyond Paradigms, which I presented
at PyCaribbean, PyBay and PyConDE.
See the slides and video from the
Berlin presentation
- where I met Miroslav Šedivý and
Jürgen Gmach, two of the technical
reviewers of this book.

    evince -p 389 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Soapbox

  Is Python a Functional Language?
  Sometime in the year 2000 I
  attended a Zope workshop at Zope
  Corporation in the United States
  when Guido van Rossum dropped by
  the classroom (he was not the
  instructor). In the Q&A that
  followed, somebody asked him
  which features of Python were
  borrowed from other languages.
  Guido's answer: "Everything that
  is good in Python was stolen from
  other languages."

  Shriram Krishnamurthi, professor
  of Computer Science at Brown
  University, starts his "Teaching
  Programming Languages in a
  PostLinnaean Age" paper with
  this:

    "Programming language
    "paradigms" are a moribund and
    tedious legacy of a bygone age.
    Modern language designers pay
    them no respect, so why do our
    courses slavishly adhere to
    them?"

  In that paper, Python is
  mentioned by name in this
  passage:

    "What else to make of a
    language like Python, Ruby, or
    Perl? Their designers have no
    patience for the niceties of
    these Linnaean hierarchies;
    they borrow features as they
    wish, creating melanges that
    utterly defy characterization."

  Krishnamurthi argues that instead
  of trying to classify languages
  in some taxonomy, it's more
  useful to consider them as
  aggregations of features. His
  ideas inspired my talk Beyond
  Paradigms, mentioned at the end
  of "Further Reading".

  Even if it was not Guido's goal,
  endowing Python with first-class
  functions opened the door to
  functional programming. In his
  post "Origins of Python's
  Functional Features", he says
  that map, filter, and reduce were
  the motivation for adding lambda
  to Python in the first place. All
  of these features were
  contributed together by Amrit
  Prem for Python 1.0 in 1994,
  according to Misc/HISTORY in the
  CPython source code.

    evince -p 390 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Functions like map, filter, and
  reduce first appeared in Lisp,
  the original functional language.
  However, Lisp does not limit what
  can be done inside a lambda,
  because everything in Lisp is an
  expression. Python uses a
  statement-oriented syntax in
  which expressions cannot contain
  statements, and many language
  constructs are statements -
  including try/catch, which is
  what I miss most often when
  writing lambdas. This is the
  price to pay for Python's highly
  readable syntax<5>. Lisp has many
  strengths, but readability is not
  one of them.

  Ironically, stealing the list
  comprehension syntax from another
  functional language - Haskell -
  significantly diminished the need
  for map and filter, and also for
  lambda.

  Besides the limited anonymous
  function syntax, the biggest
  obstacle to wider adoption of
  functional programming idioms in
  Python is the lack of tail-call
  elimination, an optimization that
  allows memory-efficient
  computation of a function that
  makes a recursive call at the
  "tail" of its body. In another
  blog post, "Tail Recursion
  Elimination", Guido gives several
  reasons why such optimization is
  not a good fit for Python. That
  post is a great read for the
  technical arguments, but even
  more so because the first three
  and most important reasons given
  are usability issues. It is no
  accident that Python is a
  pleasure to use, learn, and
  teach. Guido made it so.

  So there you have it: Python is
  not, by design, a functional
  language - whatever that means.
  Python just borrows a few good
  ideas from functional languages.


  The Problem with Anonymous Functions

  Beyond the Python-specific syntax
  constraints, anonymous functions
  have a serious drawback in any
  language: they have no name.

  I am only half joking here. Stack
  traces are easier to read when
  functions have names. Anonymous
  functions are a handy shortcut,
  people have fun coding with them,
  but sometimes they get carried
  away - especially if the language
  and environment encourage deep
  nesting of anonymous functions,
  like JavaScript on Node.js do.
  Lots of nested anonymous
  functions make debugging and
  error handling hard.

    evince -p 391 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Asynchronous programming in
  Python is more structured,
  perhaps because the limited
  lambda syntax prevents its abuse
  and forces a more explicit
  approach. Promises, futures, and
  deferreds are concepts used in
  modern asynchronous APIs. Along
  with coroutines, they provide an
  escape from the so-called
  "callback hell." I promise to
  write more about asynchronous
  programming in the future, but
  this subject must be deferred to
  Chapter 21.


1: "Origins of Python's Functional
Features", from Guido's The History
of Python blog. 2: Calling a class
usually creates an instance of that
same class, but other behaviors are
possible by overriding __new__.
We'll see an example of this in
"Flexible Object Creation with
__new__". 3: Why build a BingoCage
when we already have random.choice?
The choice function may return the
same item multiple times, because
the picked item is not removed from
the collection given. Calling
BingoCage never returns duplicate
results - as long as the instance
is filled with unique values. 4:
The source code for functools.py
reveals that the functools.partial
class is implemented in C and is
used by default. If that is not
available, a pure-Python
implementation of partial is
available since Python 3.4. 5:
There is also the problem of lost
indentation when pasting code to
Web forums, but I digress.

    evince -p 392 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter 8. Type Hints in Functions

  A NOTE FOR EARLY RELEASE READERS

  With Early Release ebooks, you
  get books in their earliest form
  - the author's raw and unedited
  content as they write - so you
  can take advantage of these
  technologies long before the
  official release of these titles.

  This will be the 8th chapter of
  the final book. Please note that
  the GitHub repo will be made
  active later on.

  If you have comments about how we
  might improve the content and/or
  examples in this book, or if you
  notice missing material within
  this chapter, please reach out to
  the author at
  fluentpython2e@ramalho.org.

  "It should also be emphasized
  that Python will remain a
  dynamically typed language, and
  the authors have no desire to
  ever make type hints mandatory,
  even by convention.1"

  - Guido van Rossum, Jukka Lehtosalo, and Łukasz

  Langa, PEP 484 - Type Hints

Type hints are the biggest change
in the history of Python since the
unification of types and classes in
Python 2.2, released in 2001.
However, type hints do not benefit
all Python users equally. That's
why they should always be optional.

PEP 484 - Type Hints introduced
syntax and semantics for explicit
type declarations in function
arguments, return values, and
variables. The goal is to help
developer tools find bugs in Python
codebases via static analysis, i.e.
without actually running the code
through tests.

    evince -p 393 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

The main beneficiaries are
professional software engineers
using IDEs (Integrated Development
Environments) and CI (Continuous
Integration). The cost-benefit
analysis that makes type hints
attractive to that group does not
apply to all users of Python.

Python's user base is much wider
than that. It includes scientists,
traders, journalists, artists,
makers, analysts and students in
many fields - among others. For
most of them, the cost of learning
type hints is higher - unless they
already know a language with static
types, subtyping, and generics. The
cost is likely to be higher and the
benefits will be lower for many of
those users, given how they
interact with Python, and the
smaller size of their code bases
and teams - often, "teams" of one.
Python's default dynamic typing is
simpler and more expressive when
writing code for exploring data and
ideas, as in data science, creative
computing, and learning.

This chapter focuses on Python's
type hints in function signatures.

Chapter 15 explores type hints in
the context of classes, and other
typing module features.

The major topics in this chapter
are:

- A hands-on introduction to
  gradual typing with Mypy.
- The complementary perspectives of
  duck typing and nominal typing.
- Overview of the main categories
  of types that can appear in
  annotations - this is about 60%
  of the chapter.
- Type hinting variadic parameters
  (*args, **kwargs).
- Limitations and downsides of type
  hints and static typing.


What's new in this chapter

This chapter is completely new.
Type hints appeared in Python 3.5
after I wrapped up the first
edition of Fluent Python.

    evince -p 394 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Given the limitations of a static
type system, the best idea of PEP
484 was to introduce a gratual type
system. Let's begin by defining
what that means.


About gradual typing

PEP 484 introduced a gradual type
system to Python. Other languages
with gradual type systems are
Microsoft's TypeScript, Dart (the
language of the Flutter SDK,
created by Google), and Hack (a
dialect of PHP supported by
Facebook's HHVM virtual machine).
The Mypy type checker itself
started as a language: a gradually
typed dialect of Python with its
own interpreter. Guido van Rossum
convinced the creator of Mypy,
Jukka Lehtosalo, to make it a tool
for checking annotated Python code.

A gradual type system:

Is optional.

  By default, the type checker
  should not emit warnings for code
  that has no type hints. Instead,
  the type checker assumes the Any
  type when it cannot determine the
  type of an object. The Any type
  is considered compatible with all
  other types.

Does not catch type errors at runtime.

  Type hints are used by static
  type checkers, linters, and IDEs
  to raise warnings. They do not
  prevent inconsistent values to be
  passed to functions or assigned
  to variables at runtime.

Does not enhance performance.

  Type annotations provide data
  that could, in theory, allow
  optimizations in the generated
  byte code, but such optimizations
  are not implemented in any Python
  runtime that I am aware in July
  2021.2 The best usability feature
  of gradual typing is that
  annotations are always optional.

    evince -p 395 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

With static type systems, most type
constraints are easy to express,
many are cumbersome, some are hard,
and a few are impossible.3 You may
very well write an excellent piece
of Python code, with good test
coverage and passing tests, but
still be unable to add type hints
that satisfy a type checker. That's
ok, just leave out the problematic
type hints and ship it!

Type hints are optional at all
levels: you can have entire
packages with no type hints, you
can silence the type checker when
you import one of those packages
into a module where you use type
hints, and you can add special
comments to make the type checker
ignore specific lines in your code.

  Tip: Seeking 100% coverage of
  type hints is likely to stimulate
  type hinting without proper
  thought, only to satisfy the
  metric. It will also prevent
  teams from making the most of the
  power and flexibility of Python.
  Code without type hints should
  naturally be accepted when
  annotations would make an API
  less user-friendly, or unduly
  complicate its implementation.


Gradual typing in practice

Let's see how gradual typing works
in practice, starting with a simple
function and gradually adding type
hints to it, guided by Mypy.

  Note: There are several Python
  type checkers compatible with PEP
  484, including Google's pytype,
  Microsoft's Pyright, Facebook's
  Pyre - in addition to type
  checkers embedded in IDEs such as
  PyCharm. I picked Mypy for the
  examples because it's the best
  known. However, one of the others
  may be a better fit for some
  projects or teams. Pytype, for
  example, is designed to handle
  codebases with no type hints and
  still provide useful advice. It
  is more lenient than Mypy, and
  can also generate annotations for
  your code.

We will annotate a show_count
function that returns a string with
a count and a singular or plural
word, depending on the count:

    evince -p 396 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> show_count(99, 'bird')
'99 birds'
>>> show_count(1, 'bird')
'1 bird'
>>> show_count(0, 'bird')
'no birds'

Example 8-1 shows the source code
of show_count, without annotations.

Example 8-1. show_count from messages.py without type hints.

def show_count(count, word):
  if count == 1:
    return f'1 {word}'
  count_str = str(count) if count else 'no'
  return f'{count_str} {word}s'


Starting with Mypy

To begin type checking, I run the
mypy command on the messages.py
module:

…/no_hints/ $ pip install mypy
[lots of messages omitted...]
…/no_hints/ $ mypy messages.py
Success: no issues found in 1 source file

Mypy with default settings finds no
problem with Example 8-1:

  Warning: I am using Mypy 0.910,
  the most recent release as I
  review this in July 2021. The
  Mypy Introduction warns it "is
  officially beta software. There
  will be occasional changes that
  break backward compatibility."
  Mypy is giving me at least one
  report that is not the same I got
  when I wrote this chapter in
  April 2020. By the time you read
  this, you may get different
  results than shown here.

If a function signature has no
annotations, Mypy ignores it by
default - unless configured
otherwise.

    evince -p 397 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

For this example, I also have
pytest unit tests. This is the code
in messages_test.py.

Example 8-2. messages_test.py without type hints.

from pytest import mark

from messages import show_count

@mark.parametrize('qty, expected', [
  (1, '1 part'),
  (2, '2 parts'),
])

def test_show_count(qty, expected):
  got = show_count(qty, 'part')
  assert got == expected

def test_show_count_zero():
  got = show_count(0, 'part')
  assert got == 'no parts'

Now let's add type hints, guided by
Mypy.


Making Mypy More Strict

The command-line option
--disallow-untyped-defs makes Mypy
flag any function definition that
does not have type hints for all
its parameters and for its return
value.

Using --disallow-untyped-defs on
the test file produces three errors
and a note:

…/no_hints/ $ mypy --disallow-untyped-defs messages_test.py
messages.py:14: error: Function is missing a type annotation
messages_test.py:10: error: Function is missing a type annotation
messages_test.py:15: error: Function is missing a return type
annotation
messages_test.py:15: note: Use "-> None" if function does not
return a value
Found 3 errors in 2 files (checked 1 source file)

For the first steps with gradual
typing, I prefer to use another
option: -disallow-incomplete-defs.
Initially, it tells me nothing:

    evince -p 398 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

…/no_hints/ $ mypy --disallow-incomplete-defs messages_test.py
Success: no issues found in 1 source file

Now I can add just the return type
to show_count in messages.py:

def show_count(count, word) -> str:

This is enough to make Mypy look at
it. Using the same command line as
before to check messages_test.py,
will lead Mypy to look at
messages.py again:

…/no_hints/ $ mypy --disallow-incomplete-defs messages_test.py
messages.py:14: error: Function is missing a type annotation for
one or more arguments
Found 1 error in 1 file (checked 1 source file)

Now I can gradually add type hints
function by function, without
getting warnings about functions
that I haven't annotated. This is a
fully annotated signature that
satisfies Mypy:

def show_count(count: int, word: str) -> str:

  Tip: Instead of typing command
  line options like
  --disallow-incomplete-defs, you
  can save your favorite as
  described in the Mypy
  configuration file documentation.
  You can have global settings and
  per-module settings. Here is a
  simple mypy.ini to get started:

  [mypy]
  python_version = 3.9
  warn_unused_configs = True
  disallow_incomplete_defs = True

    evince -p 399 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

A Default Parameter Value

The show_count function in Example
8-1 only works with regular nouns.
If the plural can't be spelled by
appending an 's', we should let the
user provide the plural form, like
this:

>>> show_count(3, 'mouse', 'mice')
'3 mice'

Let's do a little "type driven
development." First we add a test
that uses that third argument.
Don't forget to add the return type
hint to the test function,
otherwise Mypy will not check it.

def test_irregular() -> None:
  got = show_count(2, 'child', 'children')
  assert got == '2 children'

Mypy detects the error:

…/hints_2/ $ mypy messages_test.py
messages_test.py:22: error: Too many arguments for "show_count"
Found 1 error in 1 file (checked 1 source file)

Now I edit show_count, adding the
optional plural parameter:

Example 8-3. showcount from hints_2/messages.py with an optional parameter.

def show_count(count: int, singular: str, plural: str = '') -> str:
  if count == 1:
    return f'1 {singular}'
  count_str = str(count) if count else 'no'
  if not plural:
    plural = singular + 's'
  return f'{count_str} {plural}'

Now Mypy reports "Success."

    evince -p 400 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Warning:

  Here is one typing mistake that
  Python does not catch. Can you
  spot it?

  def hex2rgb(color=str) -> tuple[int, int, int]:

  Mypy's error report is not very
  helpful:

  colors.py:24: error: Function is missing a type
    annotation for one or more arguments

  The type hint for the color
  argument should be color: str. I
  wrote color=str, which is not an
  annotation: it sets the default
  value of color to str.

  In my experience, it's a common
  mistake and easy to overlook,
  especially in complicated type
  hints.

The following details are
considered good style for type
hints:

- There should be no space between
  the parameter name and the :, and
  one space after the :.
- There should be spaces on both
  sides of the = that precedes a
  default parameter value.

On the other hand, PEP 8 says there
should be no spaces around the = if
there is no type hint for that
particular parameter.

    evince -p 401 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Code Style: Use flake8 And blue

  Instead of memorizing such silly
  rules, use tools like flake8 and
  blue. flake8 reports on code
  styling, among many other issues,
  and blue rewrites source code
  according to (most) rules embeded
  in the black code formatting
  tool.

  Given the goal of enforcing a
  "standard" coding style, blue is
  better than black because it
  follows Python's own style of
  using single quotes by default,
  double quotes as an alternative:

  >>> "I prefer single quotes"
  'I prefer single quotes'

  The preference for single quotes
  is embedded in repr(), among
  other places in CPython. The
  doctest module depends on repr()
  using single quotes by default.

  If you must use black, use the
  black -S option. Then it will
  leave your quotes as they are.

    Note:

    One of the authors of blue is
    Barry Warsaw, co-author of PEP
    8, Python core developer since
    1994, and member of Python's
    Steering Council from 2019 to
    present (July, 2021). We are in
    very good company when we
    choose single quotes by
    default.


Using None as a default

In Example 8-3 the parameter plural
is annotated as str, and the
default value is '', so there is no
type conflict.

I like that solution, but in other
contexts None is a better default.
If the optional parameter expects a
mutable type, then None is the only
sensible default - as we saw in
"Mutable Types as Parameter
Defaults: Bad Idea".

    evince -p 402 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

To have None as the default for the
plural parameter, here is how the
signature would look like:

from typing import Optional

def show_count(count: int, singular: str, plural: Optional[str] = None) -> str:

Let's unpack that:

- Optional[str] means plural may be
  a str or None.
- You must explicitly provide the
  default value = None.

If you don't assign a default value
to plural, the Python runtime will
treat it as a required parameter.
Remember: at runtime, type hints
are ignored.

Note that we need to import
Optional from the typing module.
When importing types, it's good
practice to use the syntax from
typing import X, to reduce the
length of the function signatures.

  Warning:

  Optional is not a great name,
  because that annotation does not
  make the parameter optional. What
  makes it optional is assigning a
  default value to the parameter.
  Optional[str] just means: the
  type of this parameter may be str
  or NoneType. In the Haskell and
  Elm languages, a similar type is
  named Maybe.

Now that we've had a first
practical view of gradual typing,
let's consider what the concept of
type means in practice.

    evince -p 403 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Types are defined by supported operations

  "There are many definitions of
  the concept of type in the
  literature. Here we assume that
  type is a set of values and a set
  of functions that one can apply
  to these values."

  - PEP 483: The Theory of Type Hints

In practice, it's more useful to
consider the set of supported
operations as the defining
characteristic of a type.4

For example, from the point of view
of applicable operations, what are
the valid types for x in the
following function?

def double(x):
  return x * 2

The x parameter type may be numeric
(int, complex, Fraction,
numpy.uint32 etc.) but it may also
be a sequence (str, tuple, list,
array), an N-dimensional
numpy.array or any other type that
implements or inherits a __mul__
method that accepts an int
argument.

However, consider this annotated
double. Please ignore the missing
return type for now, let's focus on
the parameter type:

from collections import abc

def double(x: abc.Sequence):
  return x * 2

A type checker will reject that
code. If you tell Mypy that x is of
type abc.Sequence, it will flag x *
2 as an error because the Sequence
ABC does not implement or inherit
the __mul__ method. At runtime,
that code will work with concrete
sequences such as str, tuple, list,
array etc. - as well as numbers,
because at runtime the type hints
are ignored. But the type checker
only cares about what is explicitly
declared, and abc.Sequence has no
__mul__.

That's why the title of this
section is "Types are defined by
supported operations". The Python
runtime accepts any object as the x
argument for both versions of the
double function. The computation x
* 2 may work, or it may raise
TypeError if the operation is not
supported by x.

    evince -p 404 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

In contrast, Mypy will declare x *
2 as wrong while analyzing the
annotated double source code,
because it's an unsupported
operation for the declared type: x:
abc.Sequence.

In a gradual type system, we have
the interplay of two different
views of types:

Duck typing

  The view adopted by Smalltalk -
  the pioneering OO language - as
  well as Python, JavaScript, and
  Ruby. Objects have types, but
  variables (including parameters)
  are untyped. In practice, it
  doesn't matter what is the
  declared type of the object, only
  what operations it actually
  supports. If I can invoke
  birdie.quack(), then birdie is a
  duck in this context. By
  definition, duck typing is only
  enforced at runtime, when
  operations on objects are
  attempted. This is more flexible
  than nominal typing, at the cost
  of allowing more errors at
  runtime.5

Nominal typing

  The view adopted by C++, Java,
  and C#, supported by annotated
  Python. Objects and variables
  have types. But objects only
  exist at runtime, and the type
  checker only cares about the
  source code where variables
  (including parameters) are
  annotated with type hints. If
  Duck is a subclass of Bird, you
  can assign a Duck instance to a
  parameter annotated as birdie:
  Bird. But in the body of the
  function, the type checker
  considers the call birdie.quack()
  illegal, because birdie is
  nominally a Bird, and that class
  does not provide the .quack()
  method. It doesn't matter if the
  actual argument at runtime is a
  Duck, because nominal typing is
  enforced statically. The type
  checker doesn't run any part of
  the program, it only reads the
  source code. This is more rigid
  than duck typing, with the
  advantage of catching some bugs
  earlier in a build pipeline, or
  even as the code is typed in an
  IDE.

    evince -p 405 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Here is a silly example that
contrasts duck typing and nominal
typing, as well as static type
checking and runtime behavior6:

Example 8-4. birds.py

class Bird:
  pass

# 1
class Duck(Bird):
  def quack(self):
    print('Quack!')

# 2
def alert(birdie):
  birdie.quack()

# 3
def alert_duck(birdie: Duck) -> None:
  birdie.quack()

# 4
def alert_bird(birdie: Bird) -> None:
  birdie.quack()

- 1. Duck is a subclass of Bird.
- 2. alert has no type hints, so
  the type checker ignores it.
- 3. alert_duck takes one argument
  of type Duck.
- 4. alert_bird takes one argument
  of type Bird.

Type checking birds.py with Mypy,
we see a problem:

…/birds/ $ mypy birds.py
birds.py:16: error: "Bird" has no attribute "quack"
Found 1 error in 1 file (checked 1 source file)

Just by analyzing the source code,
Mypy sees that alert_bird is
problematic: the type hint declares
the birdie parameter with type
Bird, but the body of the function
calls birdie.quack() - and the Bird
class has no such method.

Now let's try to use the birds
module in daffy.py:

    evince -p 406 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 8-5. daffy.py

from birds import *

daffy = Duck()
# 1
alert(daffy)
# 2
alert_duck(daffy)
# 3
alert_bird(daffy)

- 1. Valid call, because alert has
  no type hints.
- 2. Valid call, because alert_duck
  takes a Duck argument, and daffy
  is a Duck.
- 3. Valid call, because alert_bird
  takes a Bird argument, and daffy
  is a also a Bird - the superclass
  of Duck.

Running Mypy on daffy.py raises the
same error about the quack call in
the alert_bird function defined in
birds.py:

…/birds/ $ mypy daffy.py
birds.py:16: error: "Bird" has no attribute "quack"
Found 1 error in 1 file (checked 1 source file)

But Mypy sees no problem with
daffy.py itself: the three function
calls are OK.

Now, if you run daffy.py, this is
what you get:

…/birds/ $ python3 daffy.py
Quack!
Quack!
Quack!

Everything works! Duck typing FTW!

At runtime, Python doesn't care
about declared types. It uses duck
typing only. Mypy flagged an error
in alert_bird, but calling it with
daffy works fine at runtime. This
may surprise many Pythonistas at
first: a static type checker will
sometimes find errors in programs
that we know will execute.

    evince -p 407 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

However, if months from now you are
tasked with extending the silly
bird example, you may be grateful
for Mypy. Consider this woody.py
module which also uses birds:

Example 8-6. woody.py

from birds import *

woody = Bird()
alert(woody)
alert_duck(woody)
alert_bird(woody)

Mypy finds two errors while
checking woody.py:

…/birds/ $ mypy woody.py
birds.py:16: error: "Bird" has no attribute "quack"
woody.py:5: error: Argument 1 to "alert_duck" has incompatible
type "Bird"; expected "Duck"
Found 2 errors in 2 files (checked 1 source file)

The first error is in birds.py: the
birdie.quack() call in alert_bird,
which we've seen before. The second
error is in woody.py: woody is an
instance of Bird, so the call
alert_duck(woody) is invalid
because that function requires a
Duck. Every Duck is a Bird, but not
every Bird is a Duck.

At runtime, none of the calls in
woody.py succeed. The succession of
failures is best illustrated in a
console session with callouts:

Example 8-7. Runtime errors and how Mypy could have helped.

>>> from birds import *
>>> woody = Bird()
# 1
>>> alert(woody)
Traceback (most recent call last):
  ...
AttributeError: 'Bird' object has no attribute 'quack'
>>>
# 2
>>> alert_duck(woody)
Traceback (most recent call last):
  ...
AttributeError: 'Bird' object has no attribute 'quack'
>>>
# 3
>>> alert_bird(woody)
Traceback (most recent call last):
  ...
AttributeError: 'Bird' object has no attribute 'quack'

    evince -p 408 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

- 1. Mypy could not detect this
  error because there are no type
  hints in alert.
- 2. Mypy reported the problem:
  Argument 1 to "alert_duck" has
  incompatible type "Bird";
  expected "Duck".
- 3. Mypy has been telling us since
  Example 8-4 that the body of the
  alert_bird function is wrong:
  "Bird" has no attribute "quack".

This little experiment shows that
duck typing is easier to get
started and is more flexible, but
allows unsupported operations to
cause errors at runtime. Nominal
typing detects errors before
runtime, but sometimes can reject
code that actually runs - such as
the call alert_bird(daffy) in
Example 8-5. Even if it sometimes
works, the alert_bird function is
misnamed: its body does require an
object that supports the .quack()
method, which Bird doesn't have.

In this silly example, the
functions are one-liners. But in
real code they could be longer,
they could pass the birdie argument
to more functions, and the origin
of the birdie argument could be
many function calls away, making it
hard to pinpoint the cause of a
runtime error. The type checker
prevents many such errors from ever
happening at runtime.

    evince -p 409 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Note: The value of type hints is
  questionable in the tiny examples
  that fit in a book. The benefits
  grow with the size of the
  codebase. That's why companies
  with millions of lines of Python
  code - like Dropbox, Google, and
  Facebook - invested in teams and
  tools to support the company-wide
  adoption of type hints, and have
  significant and increasing
  portions of their Python
  codebases type checked in their
  CI pipelines.

In this section we explored the
relationship of types and
operations in duck typing and
nominal typing, starting with the
simple double() function - which we
left without proper type hints. Now
we will tour the most important
types used for annotating
functions. We'll see a good way to
add type hints to double() when we
reach "Static Protocols". But
before we get to that, there are
more fundamental types to know.


Types usable in annotations

Pretty much any Python type can be
used in type hints, but there are
restrictions and recommendations.
In addition, the typing module
introduced special constructs with
semantics that are sometimes
surprising.

This section covers all the major
types you can use with annotations:

- typing.Any;
- Simple types and classes;
- typing.Optional and typing.Union;
- Generic collections, including
  tuples and mappings;
- Abstract Base Classes;
- Generic iterables;
- Parameterized generics and
  TypeVar;
- typing.Protocols - the key to
  static duck typing;
- typing.Callable;
- typing.NoReturn - a good way to
  end this list.

    evince -p 410 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

We'll cover each of these in turn,
starting with a type that is
strange, apparently useless, but
crucially important.


The Any type

The keystone of any gradual type
system is the Any type, also known
as the dynamic type. When a type
checker sees an untyped function
like this:

def double(x):
  return x * 2

It assumes this:

def double(x: Any) -> Any:
  return x * 2

That means the x argument and the
return value can be of any type,
including different types. Any is
assumed to support every possible
operation.

Contrast Any with object. Consider
this signature:

def double(x: object) -> object:

This function also accepts
arguments of every type, because
every type is a subtype-of object.

However, a type checker will reject
this function:

def double(x: object) -> object:
  return x * 2

The problem is that object does not
support the __mul__ operation. This
is what Mypy reports:

    evince -p 411 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

…/birds/ $ mypy double_object.py
double_object.py:2: error: Unsupported operand types for *
("object" and "int")
Found 1 error in 1 file (checked 1 source file)

More general types have narrower
interfaces, i.e. they support less
operations. The object class
implements fewer operations than
abc.Sequence, which implements
fewer operations than
abc.MutableSequence, which
implements fewer operations than
list.

But Any is a magic type that sits
at the top and the bottom of the
type hierarchy. It's simultaneously
the most general type - so that an
argument n: Any accepts values of
every type - and the most
specialized type, supporting every
possible operation. At least,
that's how the type checker
understands Any.

Of course, no type can support
every possible operation, so using
Any prevents the type checker from
fulfilling its core mission:
detecting potentially illegal
operations before your program
crashes with a runtime exception.


Subtype-of versus Consistent-with

Traditional Object-Oriented nominal
type systems rely on the is
subtype-of relationship. Given a
class T1 and a subclass T2, then T2
is subtype-of T1.

Consider this code:

class T1:
  ...

class T2(T1):
  ...

def f1(p: T1) -> None:
  ...

o2 = T2()

f1(o2)  # OK

    evince -p 412 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

The call f1(o2) is an application
of the Liskov Substitution
Principle - LSP. Barbara Liskov7
actually defined is-sub-type-of in
terms of supported operations: if
an object of type T2 substitutes an
object of type T1 and the program
still behaves correctly, then T2 is
subtype-of T1.

Continuing from the previous code,
this shows a violation of the LSP:

def f2(p: T2) -> None:
  ...

o1 = T1()

f2(o1)
  # type error

From the point of view of supported
operations, this makes perfect
sense: as a subclass, T2 inherits
and must support all operations
that T1 does. So an instance of T2
can be used anywhere a instance of
T1 is expected. But the reverse is
not necessarily true: T2 may
implement additional methods, so an
instance of T1 may not be used
everywhere an instance of T2 is
expected. This focus on supported
operations is reflected in the name
behavioral subtyping, also used to
refer to the LSP.

In a gradual type system, there is
another relationship:
consistent-with, which applies
wherever subtype-of applies, with
special provisions for type Any.

The rules for consistent-with are:

- 1. Given T1 and a subtype T2,
  then T2 is consistent-with T1
  (Liskov substitution).
- 2. Every type is consistent-with
  Any: you can pass objects of
  every type to an argument
  declared of type Any.
- 3. Any is consistent-with every
  type: you can always pass an
  object of type Any where an
  argument of another type is
  expected.

Considering the previous
definitions of the objects o1 and
o2, here are examples of valid
code, illustrating rules #2 and #3:

    evince -p 413 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

def f3(p: Any) -> None:
  ...

o0 = object()
o1 = T1()
o2 = T2()

f3(o0) #
f3(o1) # all OK: rule #2
f3(o2) #

# implicit return type: `Any`
def f4():
  ...

# inferred type: `Any`
o4 = f4()

f1(o4) #
f2(o4) # all OK: rule #3
f3(o4) #

Every gradual type system needs a
wildcard type like Any.

  Tip: The verb "to infer" is a
  fancy synomym for "to guess",
  used in the context of type
  analysis. Modern type checkers in
  Python and other languages don't
  require type annotations
  everywhere because they can infer
  the type of many expressions. For
  example, if I write x = len(s) *
  10, the type checker doesn't need
  an explicit local declaration to
  know that x is an int, as long as
  it can find type hints for the
  len built-in.

Now we can explore the rest of the
types used in annotations.


Simple types and classes

Simple types like int, float, str,
bytes may be used directly in type
hints. Concrete classes from the
standard library, external
packages, or user defined -
FrenchDeck, Vector2d, and Duck -
may also be used in type hints.

    evince -p 414 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Abstract Base Classes are also
useful in type hints. We'll get
back to them as we study collection
types, and in "Abstract Base
Classes".

Among classes, is consistent-with
is defined like is subtype-of: a
subclass is consistent-with all its
superclasses.

However, "practicality beats
purity" so there is an important
exception:

  Int is Consistent-With Complex

  There is no nominal subtype
  relationship between the built-in
  types int, float and complex:
  they are direct subclasses of
  object. But PEP 484 declares that
  int is consistent-with float, and
  float is consistent-with complex.
  It makes sense in practice: int
  implements all operations that
  float does, and int implements
  additional ones as well - bitwise
  operations like &, |, << etc. The
  end result is: int is
  consistent-with complex. For i =
  3, i.real is 3, and i.imag is 0.


Optional and Union types

We saw the Optional special type in
"Using None as a default". It
solves the problem of having None
as a default, as in this example
from that section:

from typing import Optional

def show_count(count: int, singular: str, plural: Optional[str] = None) -> str:

The construct Optional[str] is
actually a shortcut for Union[str,
None] which means the type of
plural may be str or None.

    evince -p 415 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Better Syntax For Optional And Union In Python 3.10

  We can write str | bytes instead
  of Union[str, bytes] since Python
  3.10. Less typing, and no need to
  import Optional or Union from
  typing. Contrast the old and new
  syntax for the type hint of the
  plural parameter of show_count:

  plural: Optional[str] = None # before
  plural: str | None = None # after

  The | operator also works with
  insinstance and issubclass to
  build the second argument:
  isinstance(x, int | str). For
  more, see PEP 604 - Complementary
  syntax for Union[].

The ord built-in function's
signature is a simple example of
Union - it accepts str or bytes,
and returns an int:8

def ord(c: Union[str, bytes]) -> int: ...

Here is an example of a function
that takes a str, but may return a
str or a float:

from typing import Union

def parse_token(token: str) -> Union[str, float]:
  try:
    return float(token)
  except ValueError:
    return token

If possible, avoid creating
functions that return Union types,
as they put an extra burden on the
user - forcing them to check the
type of the returned value at
runtime to know what to do with it.
But the parse_token above is a
reasonable use case in the context
of a simple expression evaluator.

    evince -p 416 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Tip: In "Dual-Mode str and bytes
  APIs" we saw functions that
  accept either str or bytes
  arguments but return str if the
  argument was str or bytes if the
  arguments was bytes. In those
  cases, the return type is
  determined by the input type, so
  Union is not an accurate
  solution. To properly annotate
  such functions, we need a type
  variable - presented in
  "Parameterized generics and
  TypeVar" - or overloading, which
  we'll see in "Overloaded
  signatures".

Union[] requires at least two
types. Nested Union types have the
same effect as a flattened Union.
So this type hint:

Union[A, B, Union[C, D, E]]

is the same as:

Union[A, B, C, D, E]

Union is more useful with types
that are not consistent among
themselves. For example: Union[int,
float] is redundant because int is
consistent-with float. If you just
use float to annotate the
parameter, it will accept int
values as well.


Generic collections

Most Python collections are
heterogeneous. For example, you can
put any mixture of different types
in a list. However, in practice
that's not very useful: if you put
objects in a collection, you are
likely to want to operate on them
later, and usually this means they
must share at least one common
method.9

Generic types can be declared with
type parameters to specify the type
of the items they can handle.

For example, a list can be
parameterized to constrain the type
of the elements in it:

    evince -p 417 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 8-8. tokenize with type hints for Python ≥ 3.9

def tokenize(text: str) -> list[str]:
  return text.upper().split()

In Python ≥ 3.9, that means
tokenize returns a list where every
item is of type str.

The annotations stuff: list and
stuff: list[Any] mean the same
thing: stuff is a list of objects
of any type.

  Tip: If you are using Python 3.8
  or earlier the concept is the
  same, but you need more code to
  make it work - as explained the
  optional box "Legacy Support and
  Deprecated Collection Types".

PEP 585 - Type Hinting Generics In
Standard Collections lists
collections from the standard
library accepting generic type
hints. The following list shows
only those collections that use the
simplest form of generic type hint:
container[item].

list
collections.deque
abc.MutableSequence
set
abc.Container
abc.MutableSet
frozenset
abc.Collection
abc.Sequence
abc.Set

The tuple and mapping types support
more complex type hints, as we'll
see in their respective sections.

As of Python 3.10, there is no good
way to annotate array.array taking
into account the typecode
constructor argument which
determines whether integers or
floats are stored in the array. An
even harder problem is how to
typecheck integer ranges to prevent
OverflowError at runtime when
adding elements to arrays. For
example, an array with typecode='B'
can only hold int values from 0 to
255. Currently, Python's static
type system is not up to this
challenge.

    evince -p 418 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

  Legacy Support And Deprecated Collection Types

  (You may skip this box if you
  only use Python 3.9 or later.)

  For Python 3.7 and 3.8, you need
  a __future__ import to make the
  [] notation work with built-in
  collections such as list:

  Example 8-9. tokenize with type hints for Python ≥ 3.7

  from __future__ import annotations

  def tokenize(text: str) -> list[str]:
    return text.upper().split()

  That __future__ import does not
  work with Python 3.6 or earlier.
  This is how to annotate tokenize
  in a way that works with Python ≥
  3.5:

  Example 8-10. tokenize with type hints for Python ≥ 3.5

  from typing import List

  def tokenize(text: str) -> List[str]:
    return text.upper().split()

  To provide the initial support
  for generic type hints, the
  authors of PEP 484 created dozens
  of generic types in the typing
  module. Table 8-1 shows some of
  them. For the full list, visit
  the typing documentation.

    evince -p 419 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Table 81. Some collection types and the irtypehint equivalents

    evince -p 420 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

collection
  type hint equivalent

list
  typing.List

set
  typing.Set

frozenset
  typing.FrozenSet

collections.deque
  typing.Deque

collections.abc.MutableSequence
  typing.MutableSequence

collections.abc.Sequence
  typing.Sequence

collections.abc.Set
  typing.AbstractSet

collections.abc.MutableSet
  typing.MutableSet

    evince -p 421 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

PEP 585 - Type Hinting Generics In
Standard Collections started a
multi-year process to improve the
usability of generic type hits. We
can summarize that process in 4
steps:

- 1. Introduce from __future__
  import annotations in Python 3.7
  to enable the use of standard
  library classes as generics with
  list[str] notation.
- 2. Make that behavior the default
  in Python 3.9: list[str] now
  works without the future import.
- 3. Deprecate all the redundant
  generic types from the typing
  module.10 Deprecation warnings
  will not be issued by the Python
  interpreter because type checkers
  should flag the deprecated types
  when the checked program targets
  Python 3.9 or newer.
- 4. Remove those redundant generic
  types in the first version of
  Python released 5 years after
  Python 3.9. At the current
  cadence, that could be Python
  3.14, a.k.a as Python Pi.

Now let's see how to annotate
generic tuples.


Tuple types

There are three ways to annotate
tuple types:

- 1. tuples as records;
- 2. tuples as records with named
  fields;
- 3. tuples as immutable sequences.

    evince -p 422 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Tuples as records

If you're using a tuple as a
record, use the tuple built-in and
declare the types of the fields
within [].

For example, the type hint would be
tuple[str, float, str] to accept a
tuple with city name, population
and country: ('Shanghai', 24.28,
'China').

Consider a function that takes a
pair of geographic coordinates and
returns a Geohash, used like this:

>>> shanghai = 31.2304, 121.4737
>>> geohash(shanghai)
'wtw3sjq6q'

This is how geohash is defined,
using the geolib package from PyPI:

Example 8-11. coordinates.py with the geohash function.

# 1
from geolib import geohash as gh
  # type: ignore

PRECISION = 9

# 2
def geohash(lat_lon: tuple[float, float]) -> str:
  return gh.encode(*lat_lon, PRECISION)

- 1. This comment stops Mypy from
  reporting that the geolib package
  doesn't have type hints.
- 2. lat_lon parameter annotated as
  a tuple with two float fields.

  Tip: For Python < 3.9, import and
  use typing.Tuple in type hints.
  It is deprecated but will remain
  in the standard library at least
  until 2024.

    evince -p 423 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Tuples as records with named fields

To annotate a tuple with many
fields, or specific types of tuple
your code uses in many places, I
highly recommend using
typing.NamedTuple - as seen in
Chapter 5. Here is a variation of
Example 8-11 with NamedTuple:

Example 8-12. coordinates_named.py with the NamedTuple Coordinates and the geohash function.

from typing import NamedTuple

from geolib import geohash as gh
  # type: ignore

PRECISION = 9

class Coordinate(NamedTuple):
  lat: float
  lon: float

def geohash(lat_lon: Coordinate) -> str:
  return gh.encode(*lat_lon, PRECISION)

As explained in "Overview of data
class builders", typing.NamedTuple
is a factory for tuple subclasses,
so Coordinate is consistent-with
tuple[float, float] but the reverse
is not true - after all, Coordinate
has extra methods added by
NamedTuple, like ._asdict(), and
could also have user-defined
methods.

In practice, this means that it is
typesafe to pass a Coordinate
instance to the display function
defined below.

def display(lat_lon: tuple[float, float]) -> str:
  lat, lon = lat_lon
  ns = 'N' if lat >= 0 else 'S'
  ew = 'E' if lon >= 0 else 'W'
  return f'{abs(lat):0.1f}°{ns}, {abs(lon):0.1f}°{ew}'


Tuples as immutable sequences

To annotate tuples of unspecified
length that are used as immutable
lists you must specify a single
type, followed by a comma and ...
(that's Python's ellipsis token,
made of three periods, not Unicode
U+2026 - HORIZONTAL ELLIPSIS).

    evince -p 424 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

For example, tuple[int, ...] is a tuple with int items.

The ellipsis indicates that any
number of elements >= 1 is
acceptable. There is no way to
specify fields of different types
for tuples of arbitrary length.

The annotations stuff: tuple[Any,
...] and stuff: tuple mean the same
thing: stuff is a tuple of
unspecified length with objects of
any type.

Here is a columnize function that
transforms a sequence into a table
of rows and cells in the form of
list of tuples with unspecified
lengths. This is useful to display
items in columns, like this:

>>> animals = 'drake fawn heron ibex koala lynx tahr xerus yak zapus'.split()
>>> table = columnize(animals)
>>> table
[('drake', 'koala', 'yak'), ('fawn', 'lynx', 'zapus'), ('heron', 'tahr'),
 ('ibex', 'xerus')]
>>> for row in table:
...   print(''.join(f'{word:10}' for word in row))
...
drake
koala
yak
fawn
lynx
zapus
heron
tahr
ibex
xerus

Example 8-13 shows the
implementation of columnize. Note
the return type:

`list[tuple[str, ...]]`.

Example 8-13. columnize.py returns a list of tuples of strings.

from collections.abc import Sequence

def columnize(
  sequence: Sequence[str], num_columns: int = 0) -> list[tuple[str, ...]]:
  if num_columns == 0:
    num_columns = round(len(sequence) ** 0.5)
  num_rows, reminder = divmod(len(sequence), num_columns)
  num_rows += bool(reminder)
  return [tuple(sequence[i::num_rows]) for i in range(num_rows)]

    evince -p 425 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Generic mappings

Generic mapping types are annotated
as MappingType[KeyType, ValueType].
The built-in dict and the mapping
types in collections and
collections.abc accept that
notation in Python ≥ 3.9. For
earlier versions, you must use
typing.Dict and other mapping types
from the typing module, as
described in "Legacy Support and
Deprecated Collection Types".

Example 8-14 shows a practical use
of a function returning an inverted
index to search Unicode characters
by name - a variation of Example
4-21 more suitable for server-side
code that we'll study in Chapter
21.

Given starting and ending Unicode
character codes, name_index returns
a dict[str, set[str]] which is an
inverted index mapping each word to
a set of characters that have that
word in their names. For example,
after indexing ASCII characters
from 32 to 64, here are the sets of
characters mapped to the words
'SIGN' and 'DIGIT', and how to find
the character named 'DIGIT EIGHT':

>>> index = name_index(32, 65)
>>> index['SIGN']
{'$', '>', '=', '+', '<', '%', '#'}
>>> index['DIGIT']
{'8', '5', '6', '2', '3', '0', '1', '4', '7', '9'}
>>> index['DIGIT'] & index['EIGHT']
{'8'}

Below is the source code for
charindex.py with the name_index
function. Besides a dict[] type
hint, this example has three
features appearing for the first
time in the book.

    evince -p 426 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 8-14. charindex.py

import sys
import re
import unicodedata
from collections.abc import Iterator

RE_WORD = re.compile(r'\w+')
STOP_CODE = sys.maxunicode + 1

# 1
def tokenize(text: str) -> Iterator[str]:
  """return iterable of uppercased words"""
  for match in RE_WORD.finditer(text):
    yield match.group().upper()

def name_index(start: int = 32, end: int = STOP_CODE) -> dict[str, set[str]]:
  # 2
  index: dict[str, set[str]] = {}
  for char in (chr(i) for i in range(start, end)):
    # 3
    if name := unicodedata.name(char, ''):
      for word in tokenize(name):
        index.setdefault(word, set()).add(char)
  return index

- 1. tokenize is a generator
  function. Chapter 17 is about
  generators.
- 2. The local variable index is
  annotated. Without the hint, Mypy
  says: Need type annotation for
  'index' (hint: "index:
  dict[<type>, <type>] = ...").
- 3. I used the walrus operator :=
  in the if condition. It assigns
  the result of the
  unicodedata.name() call to name,
  and the whole expression
  evaluates to that result. When
  the result is '', that's falsy
  and the index is not updated.11

  Note: When using a dict as a
  record, it is common to have all
  keys of the str type, with values
  of different types depending on
  the keys. That is covered in
  "TypedDict", Chapter 15.

    evince -p 427 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Abstract Base Classes

  "Be conservative in what you
  send, be liberal in what you
  accept."

  - Postel's law, a.k.a. the Robustness Principle

Table 8-1 list several abstract
classes from collections.abc.
Ideally, a function should accept
arguments of those abstract types -
or their typing equivalents before
Python 3.9 - and not concrete
types. This gives more flexibility
to the caller.

Consider this function signature:

from collections.abc import Mapping

def name2hex(name: str, color_map: Mapping[str, int]) -> str:

Using abc.Mapping allows the caller
to provide an instance of dict,
defaultdict, ChainMap, a UserDict
subclass or any other type that is
a subtype-of Mapping.

In contrast, consider this
signature:

def name2hex(name: str, color_map: dict[str, int]) -> str:

Now color_map must be a dict or one
of its subtypes such as defaultDict
or OrderedDict. In particular, a
subclass of collections.UserDict
would not pass the type check for
color_map, despite being the
recommended way to create
user-defined mappings, as we saw in
"Subclassing UserDict Instead of
dict". Mypy would reject a UserDict
or an instance of a class derived
from it, because UserDict is not a
subclass of dict; they are
siblings. Both are subclasses of
abc.MutableMapping.12

Therefore, in general it's better
to use abc.Mapping or
abc.MutableMapping in parameter
type hints, instead of dict (or
typing.Dict in legacy code). If the
name2hex function doesn't need to
mutate the given color_map, the
most accurate type hint for
color_map is abc.Mapping. That way,
the caller doesn't need to provide
an object that implements methods
like setdefault, pop and update
which are part of the
MutableMapping interface, but not
of Mapping. This has to do with the
second part of Postel's law: "be
liberal in what you accept."

    evince -p 428 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Postel's law also tells us to be
conservative in what we send. The
return value of a function is
always a concrete object, so the
return type hint should be a
concrete type, as in the example
from "Generic collections" - which
uses list[str].

def tokenize(text: str) -> list[str]:
  return text.upper().split()

Under the entry of typing.List, the
Python documentation says:

  Generic version of list. Useful
  for annotating return types. To
  annotate arguments it is
  preferred to use an abstract
  collection type such as Sequence
  or Iterable.

A similar comment appears in the
entries for typing.Dict and
typing.Set.

Remember that most ABCs from
collections.abc and other concrete
classes from collections, as well
as built-in collections, support
generic type hint notation like
collections.deque[str] starting
with Python 3.9. The corresponding
typing collections are only needed
to support code written in Python
3.8 or earlier. The full list of
classes that became generic appears
in section Implementation of PEP
585 - Type Hinting Generics In
Standard Collections.

To wrap up our discussion of ABCs
in type hints, we need to talk
about the numbers ABCs.


The Fall of the Numeric Tower

The numbers package defines the
so-called numeric tower described
in PEP 3141 - A Type Hierarchy for
Numbers. The tower is linear
hierarchy of ABCs, with Number at
the top:

    evince -p 429 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

- Number
- Complex
- Real
- Rational
- Integral

Those ABCs work perfectly well for
runtime type checking, but they are
not supported for static type
checking. Section The Numeric Tower
of PEP 484 rejects the numbers ABCs
and dictates that the built-in
types complex, float, and int
should be treated as special cases,
as explained in "int is
consistent-with complex".

We'll come back to this issue in
"The numbers ABCs and numeric
protocols", in Chapter 13 which is
devoted to contrasting protocols
and ABCs.

In practice, if you want to
annotate numeric arguments for
static type checking, you have a
few options:

- 1. Use one of the concrete types
  int, float, complex - as
  recommended by PEP 488;
- 2. Declare a union type like
  Union[float, Decimal, Fraction];
- 3. If you want to avoid
  hard-coding concrete types, use
  numeric protocols like
  SupportsFloat covered in "Runtime
  checkable static protocols".

The upcoming section "Static
Protocols" is a pre-requisite for
understanding the numeric
protocols.

Meanwhile, let's get to one of the
most useful ABCs for type hints:
Iterable.

    evince -p 430 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Iterable

The typing.List documentation I
just quoted recommends Sequence and
Iterable for function parameter
type hints.

One example of Iterable argument
appears the math.fsum function from
the standard library:

def fsum(__seq: Iterable[float]) -> float:

  Stub Files And The Typeshed Project.

  As of Python 3.10, the standard
  library has no annotations but
  Mypy, PyCharm etc. can find the
  necessary type hints in the
  Typeshed project, in the form of
  stub files: special source files
  with a .pyi extension that have
  annotated function and method
  signatures, without the
  implementation - much like header
  files in C.

  The signature for math.fsum is in
  /stdlib/2and3/math.pyi. The
  leading underscores in __seq are
  a PEP 484 convention for
  positional-only parameters,
  explained in "Annotating
  positional-only and variadic
  parameters".

Example 8-15 is another example
using an Iterable parameter that
produces items that are tuple[str,
str]. Here is how the function is
used:

>>> l33t = [('a', '4'), ('e', '3'), ('i', '1'), ('o', '0')]
>>> text = 'mad skilled noob powned leet'
>>> from replacer import zip_replace
>>> zip_replace(text, l33t)
'm4d sk1ll3d n00b p0wn3d l33t'

And here is how it's implemented:

Example 8-15. replacer.py

from collections.abc import Iterable

# 1
FromTo = tuple[str, str]

# 2
def zip_replace(text: str, changes: Iterable[FromTo]) -> str:
  for from_, to in changes:
    text = text.replace(from_, to)
  return text

    evince -p 431 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

- 1. FromTo is a type alias: I
  assigned tuple[str, str] to
  FromTo, to make the signature of
  zip_replace more readable.
- 2. changes needs to be an
  Iterable[FromTo]; that's the same
  as Iterable[tuple[str, str]], but
  shorter and easier to read.

  Explicit TypeAlias In Python 3.10

  PEP 613 - Explicit Type Aliases
  introduced a special type,
  TypeAlias, to make the
  assignments that create type
  aliases more visible and easier
  to typecheck. Starting with
  Python 3.10, this is the
  preferred way to create type
  aliases:

  from typing import TypeAlias

  FromTo: TypeAlias = tuple[str, str]


abc.Iterable versus abc.Sequence

Both math.fsum and
replacer.zip_replace must iterate
over the entire Iterable arguments
to return a result. Given an
endless iterable such as the
itertools.cycle generator as input,
these functions would consume all
memory and crash the Python
process. Despite this potential
danger, it is fairly common in
modern Python to offer functions
that accept an Iterable input even
if they must process it completely
to return a result. That gives the
caller the option of providing
input data as a generator instead
of a pre-built sequence,
potentially saving a lot of memory
if the number of input items is
large.

On the other hand, the columnize
function from Example 8-13 needs a
Sequence parameter, and not an
Iterable, because it must get the
len() of the input to compute the
number of rows up front.

    evince -p 432 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Like Sequence, Iterable is best
used as a parameter type. It's too
vague as a return type. A function
should be more precise about the
concrete type it returns.

Closely related to Iterable is the
Iterator type, used as a return
type in Example 8-14. We'll get
back to it in Chapter 17 which is
about generators and classic
iterators.


Parameterized generics and TypeVar

A parameterized generic is a
generic type, written as list[T]
where T is a type variable that
will be bound to a specific type
with each usage. This allows a
parameter type to be reflected on
the result type.

Example 8-16 defines sample, a
function that takes two arguments:
a Sequence of elements of type T,
and an int. It returns a list of
elements of the same type T, picked
at random from the first argument.

This is the implementation:

Example 8-16. sample.py

from collections.abc import Sequence
from random import shuffle
from typing import TypeVar

T = TypeVar('T')

def sample(population: Sequence[T], size: int) -> list[T]:
  if size < 1:
    raise ValueError('size must be >= 1')
  result = list(population)
  shuffle(result)
  return result[:size]

Here are two examples why I used a
type variable in sample:

- 1. If called with a tuple of type
  tuple[int, ...] - which is
  consistent-with Sequence[int] -
  then the type parameter is int,
  so the return type is list[int];

    evince -p 433 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

- 2. If called with a str - which
  is consistent-with Sequence[str]
- then the type parameter is str,
  so the return type is list[str].


  Why Is TypeVar Needed?

  The authors of PEP 484 wanted to
  introduce type hints by adding
  the typing module and not
  changing anything else in the
  language. With clever
  metaprogramming they could make
  the [] operator work on classes
  like Sequence[T]. But the name of
  the T variable inside the
  brackets must be defined
  somewhere - otherwise the Python
  interpreter would need deep
  changes to support generic type
  notation as special use of [].
  That's why the typing.TypeVar
  constructor is needed: to
  introduce the variable name in
  the current namespace. Languages
  such as Java, C#, and TypeScript
  don't require the name of type
  variable to be declared
  beforehand, so they have no
  equivalent of Python's TypeVar
  class.

Another example is the
statistics.mode function from the
standard library, which returns the
most common data point from a
series.

Here is one usage example from the
documentation:

>>> mode([1, 1, 2, 3, 3, 3, 3, 4])
3

Without using a TypeVar, mode could
have this signature:

Example 8-17. mode_float.py: mode that operates on float and subtypes.13

from collections import Counter
from collections.abc import Iterable

def mode(data: Iterable[float]) -> float:
  pairs = Counter(data).most_common(1)
  if len(pairs) == 0:
    raise ValueError('no mode for empty data')
  return pairs[0][0]

Many uses of mode involve int or
float values, but Python has other
numerical types, and it is
desirable that the return type
follows the element type of the
given Iterable. We can improve that
signature using TypeVar. Let's
start with a simple but wrong
parameterized signature:

    evince -p 434 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

from collections.abc import Iterable
from typing import TypeVar

T = TypeVar('T')

def mode(data: Iterable[T]) -> T:

When it first appears in the
signature, the type parameter T can
be any type. The second time it
appears, it will mean the same type
as the first.

Therefore, every iterable is
consistent-with Iterable[T],
including iterables of unhashable
types that collections.Counter
cannot handle. We need to restrict
the possible types assigned to T.
We'll see two ways of doing that in
the next two sections.


Restricted TypeVar

TypeVar accepts extra positional
arguments to restrict the type
parameter. We can improve the
signature of mode to accept
specific number types like this:

from collections.abc import Iterable
from decimal import Decimal
from fractions import Fraction
from typing import TypeVar

NumberT = TypeVar('NumberT', float, Decimal, Fraction)

def mode(data: Iterable[NumberT]) -> NumberT:

That's better than before, and it
was the signature for mode in the
statistics.pyi stub file on
typeshed on May 25, 2020.

However, the statistics.mode
documentation includes this
example:

>>> mode(["red", "blue", "blue", "red", "green", "red", "red"])
'red'

    evince -p 435 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

In a hurry, we could just add str
to the NumberT definition:

NumberT = TypeVar('NumberT', float, Decimal, Fraction, str)

That certainly works, but NumberT
is badly misnamed if it accepts
str. More importantly, we can't
keep listing types forever as we
realize mode can deal with them. We
can do better with another feature
of TypeVar, introduced next.


Bounded TypeVar

Looking at the body of mode in
Example 8-17, we see that the
Counter class is used for ranking.
Counter is based on dict, therefore
the element type of the data
iterable must be hashable.

At first, this signature may seem
to work:

from collections.abc import Iterable, Hashable

def mode(data: Iterable[Hashable]) -> Hashable:

Now the problem is that the type of
the returned item is Hashable: an
ABC that implements only the
__hash__ method. So the type
checker will not let us do anything
with the return value except call
hash() on it. Not very useful.

The solution is another optional
parameter of TypeVar: the bound
keyword parameter. It sets an upper
boundary for the acceptable types.
In Example 8-18, we have
bound=Hashable, which means the
type parameter may be Hashable or
any subtype-of it.14

Example 8-18. mode_hashable.py: same as Example 8-17, with a more flexible signature.

from collections import Counter
from collections.abc import Iterable, Hashable
from typing import TypeVar

HashableT = TypeVar('HashableT', bound=Hashable)

def mode(data: Iterable[HashableT]) -> HashableT:
  pairs = Counter(data).most_common(1)
  if len(pairs) == 0:
    raise ValueError('no mode for empty data')
  return pairs[0][0]

    evince -p 436 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

To summarize:

- A restricted type variable will
  be set to one of the types named
  in the TypeVar declaration.
- A bounded type variable will be
  set to the inferred type of the
  expression - as long as the
  inferred type is consistent-with
  the boundary declared in the
  bound= keyword argument of
  TypeVar.


  Note: It is unfortunate that the
  keyword argument to declare a
  bounded TypeVar is named bound=,
  because the verb "to bind" is
  commonly used to mean setting the
  value of a variable, which in the
  reference semantics of Python is
  best described as binding a name
  to the value. It would have been
  less confusing if the keyword
  argument was named boundary=.

The typing.TypeVar constructor has
other optional parameters -
covariant and contravariant - that
we'll cover in Chapter 15,
"Variance".

Let's conclude this introduction to
TypeVar with AnyStr.


The AnyStr predefined type variable

The typing module includes a predefined TypeVar named AnyStr. It's
defined like this:
AnyStr = TypeVar('AnyStr', bytes, str)

    evince -p 437 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

AnyStr is used in many functions that accept either bytes or str, and
return values of the given type.

Now, on to typing.Protocol, a new feature of Python 3.8 that can
support more Pythonic use of type hints.

Static Protocols
Note:
In Object-Oriented programming, the concept of a "protocol" as an informal interface is
as old as Smalltalk, and is an essential part of Python from the beginning. However, in
the context of type hints, a protocol is a typing.Protocol subclass defining an
interface that a type checker can verify. Both kinds of protocols are covered in
Chapter 13. This is just a brief introduction in the context of function annotations.

The Protocol type as presented in PEP 544 - Protocols: Structural
subtyping (static duck typing) is similar to interfaces in Go: a protocol type
is defined by specifying one or more methods, and the type checker verifies
that those methods are implemented where that protocol type is required.

In Python, a protocol definition is written as a typing.Protocol
subclass. However, classes that implement a protocol don't need to inherit,
register or declare any relationship with the class that defines the protocol.

It's up to the type checker to find the available protocol types and enforce
their usage.

Here is a problem that can be solved with the help of Protocol and
TypeVar. Suppose you want to create a function top(it, n) that
returns the largest n elements of the iterable it:

>>> top([4, 1, 5, 2, 6, 7, 3], 3)
[7, 6, 5]
>>> l = 'mango pear apple kiwi banana'.split()
>>> top(l, 3)
['pear', 'mango', 'kiwi']
>>>
>>> l2 = [(len(s), s) for s in l]

    evince -p 438 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> l2
[(5, 'mango'), (4, 'pear'), (5, 'apple'), (4, 'kiwi'), (6,
'banana')]
>>> top(l2, 3)
[(6, 'banana'), (5, 'mango'), (5, 'apple')]

A parameterized generic top would look like this:

Example 8-19. top function with an undefined T type parameter.

def top(series: Iterable[T], length: int) -> list[T]:
ordered = sorted(series, reverse=True)
return ordered[:length]

The problem is how to constrain T? It cannot be Any or object, because
the series must work with sorted. The sorted built-in actually
accepts Iterable[Any], but that's because the optional parameter key
takes a function that computes an arbitrary sort key from each element.

What happens if you give sorted a list of plain objects but don't provide
a key argument? Let's try that:

>>> l = [object() for _ in range(4)]
>>> l
[<object object at 0x10fc2fca0>, <object object at 0x10fc2fbb0>,
<object object at 0x10fc2fbc0>, <object object at 0x10fc2fbd0>]
>>> sorted(l)
Traceback (most recent call last):

File "<stdin>", line 1, in <module>
TypeError: '<' not supported between instances of 'object' and
'object'

The error message shows that sorted uses the < operator on the elements
of the iterable. Is this all it takes? Let's do another quick experiment:15
>>> class Spam:

...

def __init__(self, n): self.n = n
...

def __lt__(self, other): return self.n < other.n
...

def __repr__(self): return f'Spam({self.n})'
...
>>> l = [Spam(n) for n in range(5, 0, -1)]
>>> l
[Spam(5), Spam(4), Spam(3), Spam(2), Spam(1)]

    evince -p 439 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> sorted(l)
[Spam(1), Spam(2), Spam(3), Spam(4), Spam(5)]

That confirms it: I can sort a list of Spam because Spam implements
__lt__ - the special method that supports the < operator.

So the T type parameter in Example 8-19 should be limited to types that
implement __lt__. In Example 8-18 we needed a type parameter that
implemented __hash__, so we were able to use typing.Hashable as
the upper bound for the type parameter. But now there is no suitable type in
typing or abc to use, so we need to create it.

Here is the new SupportsLessThan type, a Protocol:

Example 8-20. comparable.py: definition of a SupportsLessThan
Protocol type:

from typing import Protocol, Any
class SupportsLessThan(Protocol):

def __lt__(self, other: Any) -> bool: ...

A protocol is a subclass of typing.Protocol.

The body of the protocol has one or more method definitions, with ...
in their bodies.

A type T is consistent-with a protocol P if T implements all the methods
defined in P, with matching type signatures.

Given SupportsLessThan, we can now define this working version of
top:

Example 8-21. top.py: definition of the top function using a TypeVar
with bound=SupportsLessThan:

from collections.abc import Iterable
from typing import TypeVar
from comparable import SupportsLessThan

    evince -p 440 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

LT = TypeVar('LT', bound=SupportsLessThan)

def top(series: Iterable[LT], length: int) -> list[LT]:
ordered = sorted(series, reverse=True)
return ordered[:length]

Let's test-drive top. Example 8-22 shows part of a test suite for use with
pytest. It tries calling top first with a generator expression that yields
tuple[int, str], and then with a list of object. With the list of
object, we expect to get a TypeError exception.

Example 8-22. top_test.py: partial listing of the test suite for top
from collections.abc import Iterator
from typing import TYPE_CHECKING
import pytest
from top import top
# several lines omitted

def test_top_tuples() -> None:
fruit = 'mango pear apple kiwi banana'.split()
series: Iterator[tuple[int, str]] = (
(len(s), s) for s in fruit)
length = 3
expected = [(6, 'banana'), (5, 'mango'), (5, 'apple')]
result = top(series, length)
if TYPE_CHECKING:
reveal_type(series)
reveal_type(expected)
reveal_type(result)
assert result == expected
# intentional type error

def test_top_objects_error() -> None:
series = [object() for _ in range(4)]
if TYPE_CHECKING:
reveal_type(series)
with pytest.raises(TypeError) as excinfo:

top(series, 3)
assert "'<' not supported" in str(excinfo.value)

    evince -p 441 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

The typing.TYPE_CHECKING constant is always False at
runtime, but type checkers pretend it is True when they are type
checking.

Explicit type declaration for the series variable, to make the Mypy
output easier to read.16], as we'll see in "Generic Iterable Types".]
This if prevents the next three lines from executing when the test runs.
reveal_type() cannot be called at runtime, because it is not a
regular function but a Mypy debugging facility - that's why there is no
import for it. Mypy will output one debugging message for each
reveal_type() pseudo function call, showing the inferred type of
the argument.

This line will be flagged as an error by Mypy.

The above tests pass - but they would pass anyway, with or without type
hints in top.py. More to the point, if I check that test file with Mypy, I see
that the TypeVar is working as intended. See the mypy command output
in Example 8-23.

Warning:
As of Mypy 0.910 (July 2021), the output of reveal_type does not show precisely
the types I declared in some cases, but compatible types instead. For example, I did not
use typing.Iterator but abc.Iterator. Please ignore this detail. The Mypy
output is still useful. I will pretend this issue of Mypy is fixed when discussing the
output.

Example 8-23. Output of mypy top_test.py (lines split for
readability)
…/comparable/ $ mypy top_test.py
top_test.py:32: note:

Revealed type is "typing.Iterator[Tuple[builtins.int,

    evince -p 442 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

builtins.str]]"
top_test.py:33: note:

Revealed type is "builtins.list[Tuple[builtins.int,
builtins.str]]"
top_test.py:34: note:

Revealed type is "builtins.list[Tuple[builtins.int,
builtins.str]]"
top_test.py:41: note:

Revealed type is "builtins.list[builtins.object*]"
top_test.py:43: error:

Value of type variable "LT" of "top" cannot be "object"
Found 1 error in 1 file (checked 1 source file)

In test_top_tuples, reveal_type(series) shows it is an
Iterator[tuple[int, str]] - which I explicitly declared.
reveal_type(result) confirms that the type returned by the top
call is what I wanted: given the type of series, the result is
list[tuple[int, str]].

In test_top_objects_error, reveal_type(series) shows
it is list[object*]. Mypy puts a * after any type that was inferred:

I did not annotate the type of series in this test.

Mypy flags the error that this test intentionally triggers: the element
type of the Iterable series cannot be object (it must be of type
SupportsLessThan).

A key advantage of a protocol type over ABCs is that a type doesn't need
any special declaration to consistent-with a protocol type. This allows a
protocol to be created leveraging pre-existing types, or types implemented
in code that we do not control. I don't need to derive or register str,
tuple, float, set, etc. with SupportsLessThan to use them where
a SupportsLessThan parameter is expected. They only need to
implement __lt__. And the type checker will still be able do its job,
because SupportsLessThan is explicitly defined as a Protocol - in

    evince -p 443 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

contrast with the implicit protocols that are common with duck typing,
which are invisible to the type checker.

The special Protocol class was introduced in PEP 544 - Protocols:

Structural subtyping (static duck typing). Example 8-21 demonstrates why
this feature is known as static duck typing: the solution to annotate the
series parameter of top was to say "The nominal type of series
doesn't matter, as long as it implements the __lt__ method". Python's
duck typing always allowed us to say that implicitly, leaving static type
checkers clueless. A type checker can't read CPython's source code in C, or
perform console experiments to find out that sorted only requires that the
elements support <.

Now we can make duck typing explicit for static type checkers. That's why
it makes sense to say that typing.Protocol gives us static duck
typing.17
There's more to see about typing.Protocol. We'll come back to it in
Part IV, where Chapter 13 contrasts structural typing, duck typing, and
ABCs - another approach to formalizing protocols. In addition,
"Overloaded signatures" (Chapter 15) explains how to declare overloaded
function signatures with @typing.overload, and includes an extensive
example using typing.Protocol and a bounded TypeVar.

Note:
typing.Protocol makes it possible to annotate the double function presented in
"Types are defined by supported operations" without losing functionality. The key is to
define a protocol class with the __mul__ method. I invite you to do that as an exercise.

The solution appears in "The typed double function" (Chapter 13).

Callable
To annotate callback parameters or function objects returned by higher-order functions, the typing module provides the Callable type, which
is parameterized like this:

    evince -p 444 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Callable[[ParamType1, ParamType2], ReturnType]

The parameter list - [ParamType1, ParamType2] - can have 0 or
more types.

Here is an example in context:

def repl(input_fn: Callable[[Any], str] = input) -> None:
The repl function is part of a simple interactive interpreter.18
During normal usage, the repl function uses Python's input built-in to
read expressions from the user. However, for automated testing or for
integration with other input sources, repl accepts an optional input_fn
parameter: a Callable with the same parameter and return types as
input.

The built-in input() has this signature on typeshed:

def input(__prompt: Any = ...) -> str: ...

That function is consistent-with this Callable type hint:

Callable[[Any], str]

As another example, in Chapter 10, the Order.__init__ method in
Example 10-3 uses this signature:

class Order:

def __init__(
self,
customer: Customer,
cart: Sequence[LineItem],
promotion: Optional[Callable[['Order'], float]] = None,
) -> None:

self rarely needs a type hint.19.

    evince -p 445 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

promotion may be None, or Callable[[Order], float]: a
function that takes an Order and returns float.
__init__ always returns None, but I recommend adding the return
type hint for it anyway.20
Note that the Order type appears as the string 'Order' in the
Callable type hint, otherwise Python would raise NameError: name
'Order' is not defined - because the Order class is not defined
until Python reads the whole body of the class - an issue we'll discuss in
Chapter 24: Class Metaprogramming.

Tip:
PEP 563 - Postponed Evaluation of Annotations was implemented in Python 3.7 to
support forward references in annotations, avoiding the need to write Order as string in
the previous example. However, that feature is only enabled when from
__future__ import annotations is used at the top of the module, to avoid
breaking code that depend on reading annotations at runtime, like the pydantic and
FastAPI packages - to name just two examples. The PEP 563 behavior was planned to
become default in Python 3.10 but this has been postponed - pun intended - while a
compromise is reached between those who care about using annotations at runtime and
those who don't. See this message from Python's Steering Council for more: PEP 563
and Python 3.10.

There is no syntax to annotate optional or keyword arguments in
Callable[]. The documentation says "such function types are rarely
used as callback types". If you need a type hint to match a function with a
dynamic signature, replace the whole parameter list with ..., like this:

Callable[..., ReturnType].

NoReturn
This is a special type used only to annotate the return type of functions that
never return. Usually, they exist to raise exceptions. There are dozens of

    evince -p 446 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

such functions in the standard library.

For example: sys.exit() raises SystemExit, to terminate the Python
process.

Its signature in typeshed is:

def exit(__status: object = ...) -> NoReturn: ...

The __status parameter is positional-only, and it has a default value.

Stub files don't spell out the default values: they use ... instead. The type
of __status is object which means it may also be None, therefore it
would be redundant to mark it Optional[object].

In Chapter 24, Example 24-6 uses NoReturn in the
__flag_unknown_attrs, a method designed to produce a user friendly
and comprehensive error message, and then raise AttributeError.

The last section in this epic chapter is about positional and variadic
parameters.

Annotating positional-only and variadic
parameters
Recall the tag function from Example 7-9. The last time we saw its
signature was in section "Positional-only parameters":

def tag(name, /, *content, class_=None, **attrs):
Here is tag, fully annotated, written in several lines - a common
convention for long signatures, with line breaks the way the blue formatter
would do it:

from typing import Optional

def tag(
name: str,

    evince -p 447 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

/,
*content: str,
class_: Optional[str] = None,
**attrs: str,
) -> str:

Note the type hint *content: str for the arbitrary positional
parameters: this means all those arguments must be of type str. The type
of the content local variable in the function body will be tuple[str,
...].

The type hint for the arbitrary keyword arguments is **attrs: str in
this example, therefore the type of attrs inside the function will be
dict[str, str]. For a type hint like **attrs: float, the type of
attrs in the functin would be dict[str, float].

If the attrs parameter must accept values of different types, you'll need
to use a Union[] or Any: **attrs: Any.

The / notation for positional-only parameters is only available in Python ≥
3.8. In Python 3.7 or earlier, that's a syntax error. The PEP 484 convention
is to prefix each positional-only parameter name with two underscores.

Here is the tag signature again, now in two lines, using the PEP 484
convention:

from typing import Optional

def tag(__name: str, *content: str, class_: Optional[str] = None,
**attrs: str) -> str:

Mypy understands and enforces both ways of declaring positional-only
parameters.

To close this chapter, let's briefly consider the limits of type hints and the
static type system they support.

Flawed Typing and Strong Testing

    evince -p 448 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Maintainers of large corporate codebases report that many bugs are found
by static type checkers and fixed more cheaply than if the bugs were
discovered only after the code is running in production.

However, it's essential to note that automated testing was standard practice
and widely adopted long before static typing was introduced in the
companies that I know about.

Even in the contexts where they are most beneficial, static typing cannot be
trusted as the ultimate arbiter of correctness. It's not hard to discover:

False positives: tools report type errors on code that is correct.

False negatives: tools don't report type errors on code that is
incorrect.

Also, if we are forced to type check everything, we lose some of the
expressive power of Python:

Some handy features can't be statically checked. For example:

argument unpacking like config(**settings).

Advanced features like properties, descriptors, metaclasses, and
metaprogramming in general are poorly supported or beyond
comprehension for type checkers.

Type checkers lag behind Python releases, rejecting or even
crashing while analysing code with new language features - for
more than a year in some cases.

Common data constraints cannot be expressed in the type system - even
simple ones. For example: type hints are unable to ensure "quantity must be
an integer > 0" or "label must be a string with 6 to 12 ASCII letters." In
general, type hints are not helpful to catch errors in business logic.

Given those caveats, type hints cannot be the mainstay of software quality,
and making them mandatory without exception would amplify its
downsides.

    evince -p 449 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Consider a static type checker as one of the tools in a modern CI pipeline,
along with test runners, linters, etc. The point of a CI pipeline is to reduce
sofware failures, and automated tests catch many bugs that are beyond the
reach of type hints. Any code you can write in Python, you can test in
Python - with or without type hints.

Note:
The title and conclusion of this section were inspired by Bruce Eckel's article Strong
Typing vs. Strong Testing, also published in the anthology The Best Software Writing I
edited by Joel Spolky. Bruce is a fan of Python and author of books about C++, Java,
Scala, and Kotlin. In that post, he tells how he was a static typing advocate until he
learned Python and concluded: "If a Python program has adequate unit tests, it can be as
robust as a C++, Java, or C# program with adequate unit tests (although the tests in
Python will be faster to write)."

This wraps up our coverage of Python's type hints for now. They are also
the main focus of Chapter 15, which covers generic classes, variance,
overloaded signatures, type casting, and more. Meanwhile, type hints will
make guest appearances in several examples throughout the book.

    evince -p 450 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter summary
We started with a brief introduction to the concept of gradual typing and
then switched to a hands-on approach. It's hard to see how gradual typing
works without a tool that actually reads the type hints, so we developed an
annotated function guided by Mypy error reports.

Back to the idea of gradual typing, we explored how it is a hybrid of
Python's traditional duck typing and the nominal typing more familiar to
users of Java, C++ and other statically typed languages.

Most of the chapter was devoted to presenting the major groups of types
used in annotations. Many of the types we covered are related to familiar
Python object types, such as collections, tuples, and callables - extended to
support generic notation like Sequence[float]. Many of those types
are temporary surrogates implemented in the typing module before the
standard types were changed to support generics in Python 3.9.

Some of the types are special entities. Any, Optional, Union, and
NoReturn have nothing to do with actual objects in memory, but exist
only in the abstract domain of the type system.

We studied parameterized generics and type variables, which bring more
flexibility to type hints without sacrificing type safety.

Parameterized generics become even more expressive with the use of
Protocol. Because it appeared only in Python 3.8, Protocol is not
widely used yet - but it is hugely important. Protocol enables static duck
typing: the essential bridge between Python's duck typed core and the
nominal typing that allows static type checkers to catch bugs.

While covering some of these types we experimented with Mypy to see
type checking errors and inferred types with the help of Mypy's magic
reveal_type() function.

The final section covered how to annotate positional-only and variadic
parameters.

    evince -p 451 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Type hints are a complex and evolving topic. Fortunately, they are an
optional feature. Let us keep Python accessible to the widest user base and
stop preaching that all Python code should have type hints - as I've seen in
public sermons by typing evangelists.

Our BDFL emeritus led this push towards type hints in Python, so it's only
fair that this chapter starts and ends with his words:

I wouldn't like a version of Python where I was morally obligated to add
type hints all the time. I really do think that type hints have their place
but there are also plenty of times that it's not worth it, and it's so
wonderful that you can choose to use them.21
- Guido van Rossum

Further Reading
Bernát Gábor wrote in his excellent post The state of type hints in Python:

Type hints should be used whenever unit tests are worth writing.

I am a big fan of testing, but I also do a lot exploratory coding. When I am
exploring, tests and type hints are not helpful. They are a drag.

Gábor's post is one of the best introductions to Python's type hints that I
found, along with Geir Arne Hjelle's Python Type Checking (Guide).

Hypermodern Python Chapter 4: Typing by Claudio Jolowicz is a shorter
introduction that also covers runtime type checking validation.

For deeper coverage, the Mypy documentation is the best source. It is
valuable regardless of the type checker you are using, because it has tutorial
and reference pages about Python typing in general - not just about the
Mypy tool itself. There you will also find a handy cheat sheets and a very
useful page about Common issues and solutions.

The typing module documentation is a good quick reference, but it
doesn't go into much detail. The ultimate references are the PEP documents
related to typing. There are more than 20 of them already. The intended
audience of PEPs are Python core developers and Python's Steering

    evince -p 452 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Council, so they assume a lot of prior knowledge and are certainly not light
reading.

As mentioned, Chapter 15 covers more typing topics, and "Further
Reading" provides additional references, including Table 15-1, listing
typing PEPs approved or under discussion as of March 2021.

Awesome Python Typing is a valuable collection of links to tools and
references.

    evince -p 453 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

SOAPBOX
Just ride
Forget the ultralight, uncomfortable bikes, flashy jerseys, clunky
shoes that clip onto tiny pedals, the grinding out of endless miles.

Instead, ride like you did when you were a kid - just get on your bike
and discover the pure joy of riding it.
- Grant Petersen, Just Ride: A Radically Practical
Guide to Riding Your Bike
If coding is not your whole profession, but a useful tool in you
profession, or something you do to learn, tinker and enjoy, you probably
don't need type hints anymore than most bikers need shoes with stiff
soles and metal cleats.

Just code.

The cognitive effect of typing
I worry about the effect type hints will have on Python coding style.

I agree that users of most APIs benefit from type hints. But Python
attracted me - among other reasons - because it provides functions that
are so powerful that they replace entire APIs, and we can write
similarly powerful functions ourselves. Consider the max() built-in.

It's powerful yet easy to understand. But I will show in "Max
Overload" that it takes 14 lines of type hints to properly annotate it -
not counting a typing.Protocol and a few TypeVar definitions
to support those type hints.

I am concerned that strict enforcement of type hints in libraries will
discourage programmers from even considering writing such functions
in the future.

According to the English Wikipedia, Linguistic Relativity - a.k.a. the
Sapir-Whorf hypothesis- is a "principle claiming that the structure of

    evince -p 454 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

a language affects its speakers' world view or cognition. Wikipedia
further explains:

The strong version says that language determines thought and
that linguistic categories limit and determine cognitive
categories.

The weak version says that linguistic categories and usage only
influence thought and decisions.

Linguists generally agree the strong version is false, but there is
empirical evidence supporting the weak version.

I am not aware of specific studies with programming languages, but in
my experience they've had a big impact on how I approach problems.

The first programming language I used professionally was Applesoft
BASIC in the age of 8-bit computers. Recursion was not directly
supported by BASIC - you had to roll your own call stack to use it. So I
never considered using recursive algorithms or data structures. I knew
at some conceptual level such things existed, but they weren't part of
my problem-solving toolbox.

Decades later when I started with Elixir, I enjoyed solving problems
with recursion and overused it - until I discovered that many of my
solutions would be simpler if I used existing functions from the Elixir
Enum and Stream modules. I learned that idiomatic Elixir
application-level code rarely has explicit recursive calls, but use enums
and streams that implement recursion under the hood.

Linguistic Relativity could explain the widespread idea (also unproven)
that learning different programming languages makes you a better
programmer, particularly when the languages support different
programming paradigms. Practicing Elixir made me more likely to
apply functional patterns when I write Python or Go code.

Now, back to Earth.

    evince -p 455 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

The requests package would probably have a very different API if
Kenneth Reitz was determined (or told by his boss) to annotate all its
functions. His goal was to write an API that was easy to use, flexible,
and powerful. He succeeded, given the amazing popularity of
requests - in May 2020, it's #4 on PyPI Stats, with 2.6 million
downloads a day. #1 is urllib3, a dependency of requests.

In 2017, the requests maintainers decided not to spend their time
writing type hints. One of the maintainers, Cory Benfield, had written
an e-mail stating:

I think that libraries with Pythonic APIs are the least likely to take up
this typing system because it will provide the least value to them.

In that message, Benfield gave this extreme example of a tentative type
definition for the files keyword argument of
requests.request():

Optional[
Union[
Mapping[
basestring,
Union[
Tuple[basestring, Optional[Union[basestring, file]]],
Tuple[basestring, Optional[Union[basestring, file]],
Optional[basestring]],
Tuple[basestring, Optional[Union[basestring, file]],
Optional[basestring], Optional[Headers]]
]
],
Iterable[
Tuple[
basestring,
Union[
Tuple[basestring, Optional[Union[basestring,
file]]],
Tuple[basestring, Optional[Union[basestring, file]],
Optional[basestring]],
Tuple[basestring, Optional[Union[basestring, file]],
Optional[basestring], Optional[Headers]]
]
]

    evince -p 456 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

]
]

And that assumes this definition:

Headers = Union[
Mapping[basestring, basestring],
Iterable[Tuple[basestring, basestring]],
]

Do you think requests would be the way it is if the maintainers
insisted on 100% type hint coverage? SQLAlchemy is another
important package that doesn't play well with type hints.

What makes these libraries great is embracing the dynamic nature of
Python.

While there are benefits to type hints, there is also a price to pay.

First, there is the significant investment of understanding how the type
system works. That's a one-time cost.

But there is also a recurring cost, forever.

We lose some of the expressive power of Python if we insist on type
checking everything. Beautiful features like argument unpacking - e.g.
config(**settings) - are beyond comprehension for type
checkers.

If you want to have a call like config(**settings) type checked,
you must spell every argument out. That brings me memories of Turbo
Pascal code I wrote 35 years ago.

Libraries that use metaprogramming are hard or impossible to annotate.

Surely metaprogramming can be abused, but it's also what makes many
Python packages a joy to use.

If type hints are mandated top down without exceptions in large
companies, I bet soon we'll see people using code generation to reduce

    evince -p 457 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

boilerplate in Python source-code - a common practice with less
dynamic languages.

For some projects and contexts, type hints just don't make sense. Even
in contexts where they mostly make sense, they don't make sense all
the time. Any reasonable policy about the use of type hints must have
exceptions.

Alan Kay - the Turing Award laureate who pioneered Object-Oriented
Programming - once said:

Some people are completely religious about type systems and as a
mathematician I love the idea of type systems, but nobody has ever
come up with one that has enough scope.22
Thank Guido for optional typing. Let's use it as intended, and not aim
to annotate everything into strict conformity to a coding style that looks
like Java 1.5.

Duck typing FTW
Duck typing fits my brain, and static duck typing is a good compromise
allowing static type checking without losing a lot of flexibility that
some nominal type systems only provide with a lot of complexity - if
ever.

Before PEP 544, this whole idea of type hints seemed utterly
unpythonic to me. I was very glad to see typing.Protocol land in
Python. It brings balance to the force.

Generics or specifics?

From a Python perspective, the typing usage of the term "generic" is
backwards. Common meanings of "generic" are "applicable to an entire
class or group" or "without a brand name."

Consider list versus list[str]. The first is generic: it accepts any
object. The second is specific: it only accepts str.

    evince -p 458 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

The term makes sense in Java, though. Before Java 1.5, all Java
collections (except the magic array) were "specific": they could only
hold Object references, so we had to cast the items that came out of a
collection to use them. With Java 1.5, collections got type parameters,
and became "generic."

1 PEP 484 - Type Hints, section Rationale and Goals; bold emphasis retained from the original.
2 A just-in-time compiler like the one in PyPy has much better data than type hints: it monitors
the Python program as it runs, detects the concrete types in use, and generates optimized
machine code for those concrete types.
3 For example, recursive types are not supported as of July 2021 - see typing module issue
#182 Define a JSON type and Mypy issue #731 Support recursive types
4 Python doesn't provide syntax to control the set of possible values for a type - except in
Enum types. For example, using type hints you can't define Quantity as an integer between
1 and 1000, or AirportCode as a 3-letter combination. NumPy offers uint8, int16 and
other machine-oriented numeric types, but in the Python standard library we only have types
with very small sets of values (NoneType, bool) or extremely large sets (float, int,
str, all possible tuples etc.).
5 Duck typing is a weaker form of structural typing, which Python ≥ 3.8 also supports with the
introduction of typing.Protocol. This is covered later in this chapter - in "Static
Protocols" - with more details in Chapter 13.
6 Inheritance is often overused and hard to justify in examples that are realistic yet simple, so
please accept this animal example as a quick illustration of subtyping.
7 MIT Professor, programming language designer, and Turing Award recipient. Wikipedia:

Barbara Liskov.
8 To be more precise, ord only accepts str or bytes with len(s) == 1. But the type
system currently can't express this constraint.
9 In ABC - the language that most influenced the initial design of Python - each list was
constrained to accept values of a single type: the type of the first item you put into it.
10 One of my contributions to the typing module documentation was to add dozens of
deprecation warnings as I reorganized the entries below Module Contents into subsections,
under the supervision of Guido van Rossum.
11 I use := when it makes sense in a few examples, but I don't cover it in the book. Please see
PEP 572 - Assignment Expressions for all the gory details.

    evince -p 459 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

12 Actually, dict is a virtual subclass of abc.MutableMapping. The concept of a virtual
subclass is explained in Chapter 13. For now, know that issubclass(dict,
abc.MutableMapping) is True, despite the fact that dict is implemented in C and does
not inherit anything from abc.MutableMapping, but only from object.
13 The implementation here is simpler than the one in the Python standard library statistics
module.
14 I contributed this solution to typeshed, and that's how mode is annotated on
statistics.pyi as of May 26, 2020.
15 How wonderful it is to open an interactive console and rely on duck typing to explore
language features like I just did. I badly miss this kind of exploration when I use languages that
don't support it.
16 Without this type hint, Mypy would infer the type of series as
Generator[Tuple[builtins.int, builtins.str*], None, None], which is
verbose but consistent-with Iterator[tuple[int, str
17 I don't know who invented the term static duck typing, but it became more popular with the
success of the Go language, which has interface semantics that are more like Python's
protocols than the nominal interfaces of Java.
18 REPL stands for read-eval-print-loop, the common code pattern in interactive interpreters.
19 We'll see cases where self is annotated in Chapter 15, "Implementing a generic class"
20 As special case for __init__, if at least one parameter has a type hint, Mypy does not
complain about the missing return type, by default. But if you forget this rule, and __init__
is completely untyped, then it will not be type checked.
21 From YouTube video of Type Hints by Guido van Rossum (March 2015). Quote starts at
13'40". I did some light editing for clarity.
22 Source: A Conversation with Alan Kay.

    evince -p 460 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter 9. Decorators and Closures
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form - the
author's raw and unedited content as they write - so you can take
advantage of these technologies long before the official release of these
titles.

This will be the 9th chapter of the final book. Please note that the
GitHub repo will be made active later on.

If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at fluentpython2e@ramalho.org.

There's been a number of complaints about the choice of the name
"decorator" for this feature. The major one is that the name is not
consistent with its use in the GoF book.1 The name decorator probably
owes more to its use in the compiler area - a syntax tree is walked and
annotated.
- PEP 318 - Decorators for Functions and Methods
Function decorators let us "mark" functions in the source code to enhance
their behavior in some way. This is powerful stuff, but mastering it requires
understanding closures - which is what happens when functions capture
variables defined outside of their bodies.

The most obscure reserved keyword in Python is nonlocal, introduced in
Python 3.0. You can have a profitable life as a Python programmer without
ever using it if you adhere to a strict regimen of class-centered object
orientation. However, if you want to implement your own function

    evince -p 461 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

decorators, you must understand closures, and then the need for nonlocal
becomes obvious.

Aside from their application in decorators, closures are also essential for
any type of programming using callbacks, and for coding in a functional
style when it makes sense.

The end goal of this chapter is to explain exactly how function decorators
work, from the simplest registration decorators to the rather more
complicated parameterized ones. However, before we reach that goal we
need to cover:

How Python evaluates decorator syntax
How Python decides whether a variable is local
Why closures exist and how they work
What problem is solved by nonlocal
With this grounding, we can tackle further decorator topics:

Implementing a well-behaved decorator
Powerful decorators in the standard library: @cache,
@lru_cache, and @singledispatch
Implementing a parameterized decorator

What's new in this chapter
The caching decorator functools.cache - new in Python 3.9 - is
simpler than the traditional functools.lru_cache, so I present it first.

The latter is covered in "Using lru_cache", including the simplified form
added in Python 3.8.

Section "Single Dispatch Generic Functions" was expanded and now uses
type hints, the preferred way to use functools.singledispatch
since Python 3.7.

    evince -p 462 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

"Parameterized Decorators" now includes a class-based example,
Example 9-27.

I moved Chapter 10 - Design Patterns with First-Class Functions - to the
end of part III to improve the flow of the book. Section "DecoratorEnhanced Strategy Pattern" is now in that chapter, along with other
variations of the Strategy design pattern using callables.

We start with a very gentle introduction to decorators, and then proceed
with the rest of the items listed in the chapter opening.

Decorators 101
A decorator is a callable that takes another function as argument (the
decorated function).

A decorator may perform some processing with the decorated function, and
returns it or replaces it with another function or callable object.2
In other words, assuming an existing decorator named decorate, this
code:

@decorate
def target():
print('running target()')

Has the same effect as writing this:

def target():
print('running target()')
target = decorate(target)

The end result is the same: at the end of either of these snippets, the
target name is bound to whatever function is returned by
decorate(target) - which may be the function initially named
target, or may be a different function.

    evince -p 463 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

To confirm that the decorated function is replaced, see the console session
in Example 9-1.

Example 9-1. A decorator usually replaces a function with a different one

>>> def deco(func):

...

def inner():
...
print('running inner()')
...
return inner
...
>>> @deco
... def target():

...
print('running target()')
...
>>> target()
running inner()
>>> target
<function deco.<locals>.inner at 0x10063b598>

deco returns its inner function object.
target is decorated by deco.

Invoking the decorated target actually runs inner.

Inspection reveals that target is a now a reference to inner.

Strictly speaking, decorators are just syntactic sugar. As we just saw, you
can always simply call a decorator like any regular callable, passing another
function. Sometimes that is actually convenient, especially when doing
metaprogramming - changing program behavior at runtime.

Three essential facts make a good summary of decorators:

- 1. A decorator is a function or another callable.
- 2. A decorator may replace the decorated function with a different
one.
- 3. Decorators are executed immediately when a module is loaded.

Now let's focus on the third point.

    evince -p 464 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

When Python Executes Decorators
A key feature of decorators is that they run right after the decorated
function is defined. That is usually at import time (i.e., when a module is
loaded by Python). Consider registration.py in Example 9-2.

Example 9-2. The registration.py module
registry = []

def register(func):
print(f'running register({func})')
registry.append(func)
return func
@register
def f1():
print('running f1()')
@register
def f2():
print('running f2()')

def f3():
print('running f3()')

def main():
print('running main()')
print('registry ->', registry)
f1()
f2()
f3()
if __name__ == '__main__':
main()

registry will hold references to functions decorated by
@register.
register takes a function as argument.

Display what function is being decorated, for demonstration.

Include func in registry.

    evince -p 465 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Return func: we must return a function; here we return the same
received as argument.
f1 and f2 are decorated by @register.
f3 is not decorated.
main displays the registry, then calls f1(), f2(), and f3().
main() is only invoked if registration.py runs as a script.

The output of running registration.py as a script looks like this:

$ python3 registration.py
running register(<function f1 at 0x100631bf8>)
running register(<function f2 at 0x100631c80>)
running main()
registry -> [<function f1 at 0x100631bf8>, <function f2 at
0x100631c80>]
running f1()
running f2()
running f3()

Note that register runs (twice) before any other function in the module.

When register is called, it receives the decorated function object as an
argument - for example, <function f1 at 0x100631bf8>.

After the module is loaded, the registry list holds references to the two
decorated functions: f1 and f2. These functions, as well as f3, are only
executed when explicitly called by main.

If registration.py is imported (and not run as a script), the output is this:

>>> import registration
running register(<function f1 at 0x10063b1e0>)
running register(<function f2 at 0x10063b268>)

At this time, if you inspect registry, this is what you see:

    evince -p 466 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> registration.registry
[<function f1 at 0x10063b1e0>, <function f2 at 0x10063b268>]

The main point of Example 9-2 is to emphasize that function decorators are
executed as soon as the module is imported, but the decorated functions
only run when they are explicitly invoked. This highlights the difference
between what Pythonistas call import time and runtime.

Registration decorators
Considering how decorators are commonly employed in real code,
Example 9-2 is unusual in two ways:

The decorator function is defined in the same module as the
decorated functions. A real decorator is usually defined in one
module and applied to functions in other modules.

The register decorator returns the same function passed as
argument. In practice, most decorators define an inner function and
return it.

Even though the register decorator in Example 9-2 returns the
decorated function unchanged, that technique is not useless. Similar
decorators are used in many Python frameworks to add functions to some
central registry - for example, a registry mapping URL patterns to functions
that generate HTTP responses. Such registration decorators may or may not
change the decorated function.

We will see a registration decorator applied in "Decorator-Enhanced
Strategy Pattern" (Chapter 10).

Most decorators do change the decorated function. They usually do it by
defining an inner function and returning it to replace the decorated function.

Code that uses inner functions almost always depends on closures to
operate correctly. To understand closures, we need to take a step back and
review how variable scopes work in Python.

    evince -p 467 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Variable Scope Rules
In Example 9-3, we define and test a function that reads two variables: a
local variable a - defined as function parameter - and variable b that is not
defined anywhere in the function.

Example 9-3. Function reading a local and a global variable

>>> def f1(a):

...
print(a)
...
print(b)
...
>>> f1(3)
3
Traceback (most recent call last):

File "<stdin>", line 1, in <module>
File "<stdin>", line 3, in f1
NameError: global name 'b' is not defined

The error we got is not surprising. Continuing from Example 9-3, if we
assign a value to a global b and then call f1, it works:

>>> b = 6
>>> f1(3)
3
6

Now, let's see an example that may surprise you.

Take a look at the f2 function in Example 9-4. Its first two lines are the
same as f1 in Example 9-3, then it makes an assignment to b. But it fails at
the second print, before the assignment is made.

Example 9-4. Variable b is local, because it is assigned a value in the body
of the function
>>> b = 6
>>> def f2(a):

...
print(a)
...
print(b)
...
b = 9
...
>>> f2(3)
3

    evince -p 468 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Traceback (most recent call last):

File "<stdin>", line 1, in <module>
File "<stdin>", line 3, in f2
UnboundLocalError: local variable 'b' referenced before assignment

Note that the output starts with 3, which proves that the print(a)
statement was executed. But the second one, print(b), never runs. When
I first saw this I was surprised, thinking that 6 should be printed, because
there is a global variable b and the assignment to the local b is made after
print(b).

But the fact is, when Python compiles the body of the function, it decides
that b is a local variable because it is assigned within the function. The
generated bytecode reflects this decision and will try to fetch b from the
local scope. Later, when the call f2(3) is made, the body of f2 fetches
and prints the value of the local variable a, but when trying to fetch the
value of local variable b it discovers that b is unbound.

This is not a bug, but a design choice: Python does not require you to
declare variables, but assumes that a variable assigned in the body of a
function is local. This is much better than the behavior of JavaScript, which
does not require variable declarations either, but if you do forget to declare
that a variable is local (with var), you may clobber a global variable
without knowing.

If we want the interpreter to treat b as a global variable and still assign a
new value to it within the function, we use the global declaration:

>>> b = 6
>>> def f3(a):

...
global b
...
print(a)
...
print(b)
...
b = 9
...
>>> f3(3)
3
6
>>> b
9

    evince -p 469 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

In the examples above we can see two scopes in action:

- 1. A module global scope, made of names assigned to values outside
of any class or function block.
- 2. Function local scopes, made of names assigned to values as
parameters, or directly in the body of the function.

There is one other scope where variables can come from, which we call
nonlocal and is fundamental for closres; we'll see it in a bit.

After this closer look at how variable scopes work in Python, we can tackle
closures in the next section, "Closures". If you are curious about the
bytecode differences between the functions in Examples 9-3 and 9-4, see
the following sidebar.

    evince -p 470 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

COMPARING BYTECODES
The dis module provides an easy way to disassemble the bytecode of
Python functions. Read Examples 9-5 and 9-6 to see the bytecodes for
f1 and f2 from Examples 9-3 and 9-4.

Example 9-5. Disassembly of the f1 function from Example 9-3

>>> from dis import dis
>>> dis(f1)
2
0 LOAD_GLOBAL
3 LOAD_FAST
6 CALL_FUNCTION
keyword pair)
9 POP_TOP
3

10 LOAD_GLOBAL
13 LOAD_GLOBAL
16 CALL_FUNCTION
keyword pair)
19 POP_TOP
20 LOAD_CONST
23 RETURN_VALUE

0 (print)
0 (a)
1 (1 positional, 0

0 (print)
1 (b)
1 (1 positional, 0
0 (None)

Load global name print.

Load local name a.

Load global name b.

Contrast the bytecode for f1 shown in Example 9-5 with the bytecode
for f2 in Example 9-6.

Example 9-6. Disassembly of the f2 function from Example 9-4

>>> dis(f2)
2

0 LOAD_GLOBAL
3 LOAD_FAST
6 CALL_FUNCTION

0 (print)
0 (a)
1 (1 positional, 0

keyword pair)
9 POP_TOP
3

10 LOAD_GLOBAL

0 (print)

    evince -p 471 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

13 LOAD_FAST
16 CALL_FUNCTION

1 (b)
1 (1 positional, 0

keyword pair)
19 POP_TOP
4

20 LOAD_CONST
23 STORE_FAST
26 LOAD_CONST
29 RETURN_VALUE

1 (9)
1 (b)
0 (None)

Load local name b. This shows that the compiler considers b a local
variable, even if the assignment to b occurs later, because the nature
of the variable - whether it is local or not - cannot change in the
body of the function.

The CPython VM that runs the bytecode is a stack machine, so LOAD
and POP operations refer to the stack. It is beyond the scope of this
book to further describe the Python opcodes, but they are documented
along with the dis module in dis - Disassembler for Python
bytecode.

Closures
In the blogosphere, closures are sometimes confused with anonymous
functions. Many confuse them because of the parallel history of those
features: defining functions inside functions is not so common or
convenient, until you have anonymous functions. And closures only matter
when you have nested functions. So a lot of people learn both concepts at
the same time.

Actually, a closure is a function - let's call it f - with an extended scope
that encompasses variables referenced in the body of f that are not global
variables nor local variables of f. Such variables must come from the local
scope of an outer function which encompasses f.

    evince -p 472 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

It does not matter whether the function is anonymous or not; what matters is
that it can access nonglobal variables that are defined outside of its body.

This is a challenging concept to grasp, and is better approached through an
example.

Consider an avg function to compute the mean of an ever-growing series
of values; for example, the average closing price of a commodity over its
entire history. Every day a new price is added, and the average is computed
taking into account all prices so far.

Starting with a clean slate, this is how avg could be used:

>>> avg(10)
10.0
>>> avg(11)
10.5
>>> avg(12)
11.0

Where does avg come from, and where does it keep the history of previous
values?

For starters, Example 9-7 is a class-based implementation.

Example 9-7. average_oo.py: A class to calculate a running average
class Averager():

def __init__(self):
self.series = []

def __call__(self, new_value):
self.series.append(new_value)
total = sum(self.series)
return total / len(self.series)

The Averager class creates instances that are callable:

>>> avg = Averager()
>>> avg(10)
10.0
>>> avg(11)
10.5

    evince -p 473 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> avg(12)
11.0

Now, Example 9-8 is a functional implementation, using the higher-order
function make_averager.

Example 9-8. average.py: A higher-order function to calculate a running
average

def make_averager():
series = []

def averager(new_value):
series.append(new_value)
total = sum(series)
return total / len(series)
return averager

When invoked, make_averager returns an averager function object.

Each time an averager is called, it appends the passed argument to the
series, and computes the current average, as shown in Example 9-9.

Example 9-9. Testing Example 9-8

>>> avg = make_averager()
>>> avg(10)
10.0
>>> avg(11)
10.5
>>> avg(12)
11.0

Note the similarities of the examples: we call Averager() or
make_averager() to get a callable object avg that will update the
historical series and calculate the current mean. In Example 9-7, avg is an
instance of Averager, and in Example 9-8 it is the inner function,
averager. Either way, we just call avg(n) to include n in the series and
get the updated mean.

It's obvious where the avg of the Averager class keeps the history: the
self.series instance attribute. But where does the avg function in the
second example find the series?

    evince -p 474 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Note that series is a local variable of make_averager because the
assignment series = [] happens in the body of that function. But when
avg(10) is called, make_averager has already returned, and its local
scope is long gone.

Within averager, series is a free variable. This is a technical term
meaning a variable that is not bound in the local scope. See Figure 9-1.

Figure 9-1. The closure for averager extends the scope of that function to include the binding for the
free variable series.

Inspecting the returned averager object shows how Python keeps the
names of local and free variables in the __code__ attribute that represents
the compiled body of the function. Example 9-10 demonstrates.

Example 9-10. Inspecting the function created by make_averager in
Example 9-8

    evince -p 475 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

>>> avg.__code__.co_varnames
('new_value', 'total')
>>> avg.__code__.co_freevars
('series',)

The value for series is kept in the __closure__ attribute of the
returned function avg. Each item in avg.__closure__ corresponds to
a name in avg.__code__.co_freevars. These items are cells, and
they have an attribute called cell_contents where the actual value can
be found. Example 9-11 shows these attributes.

Example 9-11. Continuing from Example 9-9

>>> avg.__code__.co_freevars
('series',)
>>> avg.__closure__
(<cell at 0x107a44f78: list object at 0x107a91a48>,)
>>> avg.__closure__[0].cell_contents
[10, 11, 12]

To summarize: a closure is a function that retains the bindings of the free
variables that exist when the function is defined, so that they can be used
later when the function is invoked and the defining scope is no longer
available.

Note that the only situation in which a function may need to deal with
external variables that are nonglobal is when it is nested in another function
and those variables are part of the local scope of the outer function.

The nonlocal Declaration
Our previous implementation of make_averager was not efficient. In
Example 9-8, we stored all the values in the historical series and computed
their sum every time averager was called. A better implementation
would only store the total and the number of items so far, and compute the
mean from these two numbers.

Example 9-12 is a broken implementation, just to make a point. Can you
see where it breaks?

    evince -p 476 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 9-12. A broken higher-order function to calculate a running
average without keeping all history

def make_averager():
count = 0
total = 0

def averager(new_value):
count += 1
total += new_value
return total / count
return averager

If you try Example 9-12, here is what you get:

>>> avg = make_averager()
>>> avg(10)
Traceback (most recent call last):

...

UnboundLocalError: local variable 'count' referenced before
assignment
>>>

The problem is that the statement count += 1 actually means the same
as count = count + 1, when count is a number or any immutable
type. So we are actually assigning to count in the body of averager,
and that makes it a local variable. The same problem affects the total
variable.

We did not have this problem in Example 9-8 because we never assigned to
the series name; we only called series.append and invoked sum
and len on it. So we took advantage of the fact that lists are mutable.

But with immutable types like numbers, strings, tuples, etc., all you can do
is read, never update. If you try to rebind them, as in count = count +
1, then you are implicitly creating a local variable count. It is no longer a
free variable, and therefore it is not saved in the closure.

To work around this, the nonlocal keyword was introduced in Python 3.

It lets you declare a variable as a free variable even when it is assigned

    evince -p 477 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

within the function. If a new value is assigned to a nonlocal variable, the
binding stored in the closure is changed. A correct implementation of our
newest make_averager looks like Example 9-13.

Example 9-13. Calculate a running average without keeping all history
(fixed with the use of nonlocal)

def make_averager():
count = 0
total = 0

def averager(new_value):
nonlocal count, total
count += 1
total += new_value
return total / count
return averager

After studing the use of nonlocal, let's summarize how Python's variable
lookup works.3
The Python bytecode compiler determines when the function is defined
how to fetch a variable x that appears in it, based on these rules:

If there is a global x declaration, x comes from and is assigned
to the x global variable the module.4
If there is a nonlocal x declaration, x comes from and is
assigned to the x local variable of the nearest surrounding function
where x is defined.

If x is a parameter or is assigned a value in the function body, then
x is local variable.

If x is referenced but is not assigned and is not a parameter:

x will be looked up in the local scopes of the surrounding function
bodies (nonlocal scopes);
If not found in sorrounding scopes, it will be read from the module
global scope;

    evince -p 478 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

If not found in the global scope, it will be read from
__builtins__.__dict__.

Now that we have Python closures covered, we can effectively implement
decorators with nested functions.

Implementing a Simple Decorator
Example 9-14 is a decorator that clocks every invocation of the decorated
function and displays the elapsed time, the arguments passed, and the result
of the call.

Example 9-14. clockdeco0.py: simple decorator to show the running
time of functions
import time

def clock(func):
def clocked(*args):
t0 = time.perf_counter()
result = func(*args)
elapsed = time.perf_counter() - t0
name = func.__name__
arg_str = ', '.join(repr(arg) for arg in args)
print(f'[{elapsed:0.8f}s] {name}({arg_str}) -> {result!r}')
return result
return clocked

Define inner function clocked to accept any number of positional
arguments.

This line only works because the closure for clocked encompasses
the func free variable.

Return the inner function to replace the decorated function.

Example 9-15 demonstrates the use of the clock decorator.

Example 9-15. Using the clock decorator

    evince -p 479 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

import time
from clockdeco0 import clock
@clock
def snooze(seconds):
time.sleep(seconds)
@clock
def factorial(n):
return 1 if n < 2 else n*factorial(n-1)
if __name__ == '__main__':
print('*' * 40, 'Calling snooze(.123)')
snooze(.123)
print('*' * 40, 'Calling factorial(6)')
print('6! =', factorial(6))

The output of running Example 9-15 looks like this:

$ python3 clockdeco_demo.py
**************************************** Calling snooze(.123)
[0.12363791s] snooze(0.123) -> None
**************************************** Calling factorial(6)
[0.00000095s] factorial(1) -> 1
[0.00002408s] factorial(2) -> 2
[0.00003934s] factorial(3) -> 6
[0.00005221s] factorial(4) -> 24
[0.00006390s] factorial(5) -> 120
[0.00008297s] factorial(6) -> 720
6! = 720

How It Works
Remember that this code:

@clock
def factorial(n):
return 1 if n < 2 else n*factorial(n-1)

Actually does this:

def factorial(n):
return 1 if n < 2 else n*factorial(n-1)

    evince -p 480 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

factorial = clock(factorial)

So, in both examples, clock gets the factorial function as its func
argument (see Example 9-14). It then creates and returns the clocked
function, which the Python interpreter assigns to factorial (behind the
scenes, in the first example). In fact, if you import the clockdeco_demo
module and check the __name__ of factorial, this is what you get:

>>> import clockdeco_demo
>>> clockdeco_demo.factorial.__name__
'clocked'
>>>

So factorial now actually holds a reference to the clocked function.

From now on, each time factorial(n) is called, clocked(n) gets
executed. In essence, clocked does the following:

- 1. Records the initial time t0.
- 2. Calls the original factorial function, saving the result.
- 3. Computes the elapsed time.
- 4. Formats and displays the collected data.
- 5. Returns the result saved in step 2.

This is the typical behavior of a decorator: it replaces the decorated function
with a new function that accepts the same arguments and (usually) returns
whatever the decorated function was supposed to return, while also doing
some extra processing.

    evince -p 481 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Tip:
In Design Patterns by Gamma et al., the short description of the Decorator pattern starts
with: "Attach additional responsibilities to an object dynamically." Function decorators
fit that description. But at the implementation level, Python decorators bear little
resemblance to the classic Decorator described in the original Design Patterns work.
"Soapbox" has more on this subject.

The clock decorator implemented in Example 9-14 has a few
shortcomings: it does not support keyword arguments, and it masks the
__name__ and __doc__ of the decorated function. Example 9-16 uses
the functools.wraps decorator to copy the relevant attributes from
func to clocked. Also, in this new version, keyword arguments are
correctly handled.

Example 9-16. clockdeco.py: an improved clock decorator
import time
import functools

def clock(func):
@functools.wraps(func)

def clocked(*args, **kwargs):
t0 = time.perf_counter()
result = func(*args, **kwargs)
elapsed = time.perf_counter() - t0
name = func.__name__
arg_lst = [repr(arg) for arg in args]
arg_lst.extend(f'{k}={v!r}' for k, v in kwargs.items())
arg_str = ', '.join(arg_lst)
print(f'[{elapsed:0.8f}s] {name}({arg_str}) -> {result!r}')
return result
return clocked

functools.wraps is just one of the ready-to-use decorators in the
standard library. In the next section, we'll meet the most impressive
decorator that functools provides: cache.

Decorators in the Standard Library

    evince -p 482 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Python has three built-in functions that are designed to decorate methods:

property, classmethod, and staticmethod. We will discuss
property in "Using a Property for Attribute Validation" and the others in
"classmethod Versus staticmethod".

In Example 9-16 we saw another important decorator:

functools.wraps, a helper for building well-behaved decorators. Some
of the most interesting decorators in the standard library are cache,
lru_cache, and singledispatch - all from the functools
module. We'll cover them next.

Memoization with functools.cache
The functools.cache decorator implements memoization:5 an
optimization technique that works by saving the results of previous
invocations of an expensive function, avoiding repeat computations on
previously used arguments.

Tip:
functools.cache was added in Python 3.9. If you need to run these examples in
Python 3.8, replace @cache with @lru_cache. For prior versions of Python, you
must invoke the decorator, writing @lru_cache(), as explained in "Using lru_cache"

A good demonstration is to apply @cache to the painfully slow recursive
function to generate the nth number in the Fibonacci sequence, as shown in
Example 9-17.

Example 9-17. The very costly recursive way to compute the nth number in
the Fibonacci series
from clockdeco import clock
@clock
def fibonacci(n):
if n < 2:
return n

    evince -p 483 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

return fibonacci(n - 2) + fibonacci(n - 1)
if __name__ == '__main__':
print(fibonacci(6))

Here is the result of running fibo_demo.py. Except for the last line, all
output is generated by the clock decorator:

$ python3 fibo_demo.py
[0.00000042s] fibonacci(0) -> 0
[0.00000049s] fibonacci(1) -> 1
[0.00006115s] fibonacci(2) -> 1
[0.00000031s] fibonacci(1) -> 1
[0.00000035s] fibonacci(0) -> 0
[0.00000030s] fibonacci(1) -> 1
[0.00001084s] fibonacci(2) -> 1
[0.00002074s] fibonacci(3) -> 2
[0.00009189s] fibonacci(4) -> 3
[0.00000029s] fibonacci(1) -> 1
[0.00000027s] fibonacci(0) -> 0
[0.00000029s] fibonacci(1) -> 1
[0.00000959s] fibonacci(2) -> 1
[0.00001905s] fibonacci(3) -> 2
[0.00000026s] fibonacci(0) -> 0
[0.00000029s] fibonacci(1) -> 1
[0.00000997s] fibonacci(2) -> 1
[0.00000028s] fibonacci(1) -> 1
[0.00000030s] fibonacci(0) -> 0
[0.00000031s] fibonacci(1) -> 1
[0.00001019s] fibonacci(2) -> 1
[0.00001967s] fibonacci(3) -> 2
[0.00003876s] fibonacci(4) -> 3
[0.00006670s] fibonacci(5) -> 5
[0.00016852s] fibonacci(6) -> 8
8

The waste is obvious: fibonacci(1) is called eight times,
fibonacci(2) five times, etc. But adding just two lines to use cache,
performance is much improved. See Example 9-18.

Example 9-18. Faster implementation using caching
import functools
from clockdeco import clock

    evince -p 484 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

@functools.cache
@clock
def fibonacci(n):
if n < 2:
return n
return fibonacci(n - 2) + fibonacci(n - 1)
if __name__ == '__main__':
print(fibonacci(6))

This line works with Python 3.9 or later. See "Using lru_cache" for
alternatives supporting earlier versions of Python.

This is an example of stacked decorators: @cache is applied on the
function returned by @clock.

STACKED DECORATORS
To make sense of stacked decorators, recall that the @ is syntax sugar for applying the
decorator function to the function below it. If there's more than one decorator, they
behave like nested function calls. This:

@alpha
@beta
def my_fn():
...

Is the same as this:

my_fn = alpha(beta(my_fn))
In other words, the beta decorator is applied first, and the function it returns is then
passed to alpha.

Using cache in Example 9-18, the fibonacci function is called only
once for each value of n:

    evince -p 485 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

$ python3 fibo_demo_lru.py
[0.00000043s] fibonacci(0) -> 0
[0.00000054s] fibonacci(1) -> 1
[0.00006179s] fibonacci(2) -> 1
[0.00000070s] fibonacci(3) -> 2
[0.00007366s] fibonacci(4) -> 3
[0.00000057s] fibonacci(5) -> 5
[0.00008479s] fibonacci(6) -> 8
8

In another test, to compute fibonacci(30), Example 9-18 made the 31
calls needed in 0.00017s - total time-while the uncached Example 9-17
took 12.09s on an Intel Core i7 notebook, because it called
fibonacci(1) 832,040 times, in a total of 2,692,537 calls.

All the arguments taken by the decorated function must be hashable,
because the underlying lru_cache uses a dict to store the results, and
the keys are made from the positional and keyword arguments used in the
calls.

Besides making silly recursive algorithms viable, @cache really shines in
applications that need to fetch information from remote APIs.

Warning:
functools.cache can consume all available memory if there is a very large number
of cache entries. I consider it more suitable for use in short lived command-line scripts.

In long running processes, I recommend using functools.lru_cache with a
suitable maxsize parameter, as explained in the next section.

Using lru_cache
The functools.cache decorator is actually a simple wrapper around
the older functools.lru_cache function, which is more flexible and
compatible with Python 3.8 and earlier versions.

The main advantage of @lru_cache is that its memory usage is bounded
by the maxsize parameter, which has a rather conservative default value

    evince -p 486 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

of 128 - which means the cache will hold at most 128 entries at any time.

The acronym LRU stands for Least Recently Used, meaning that older
entries that have not been read for a while are discarded to make room for
new ones.

Since Python 3.8 lru_cache can be applied in two ways. This is how to
use it in the simplest way:

@lru_cache

def costly_function(a, b):
...

The other way - available since Python 3.2 - is to invoke it as a function -
with ():

@lru_cache()

def costly_function(a, b):
...

In both cases above, the default parameters would be used. They are:

maxsize=128
Sets the maximum number of entries to be stored. After the cache is full,
the least recently used entry is discarded to make room for each new
entry. For optimal performance, maxsize should be a power of 2. If
you pass maxsize=None the LRU logic is disabled, so the cache
works faster but entries are never discarded, which may consume too
much memory. That's what @functools.cache does.
typed=False
Determines whether results of different argument types are stored
separately. For example, in the default setting, float and integer
arguments that are considered equal are stored only once, so there
would be a single entry for the calls f(1) and f(1.0). If
typed=True, those arguments would produce different entries,
possibly storing distinct results.

    evince -p 487 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Here is an example invoking @lru_cache with non-default parameters:

@lru_cache(maxsize=2**20, typed=True)

def costly_function(a, b):
...

Now let's study another powerful decorator:

functools.singledispatch.

Single Dispatch Generic Functions
Imagine we are creating a tool to debug web applications. We want to
generate HTML displays for different types of Python objects.

We could start with a function like this:

import html

def htmlize(obj):
content = html.escape(repr(obj))
return f'<pre>{content}</pre>'

That will work for any Python type, but now we want to extend it to
generate custom displays for some types. Some examples:

str: replace embedded newline characters with '<br/>\n' and
use <p> tags instead of <pre>.
int: show the number in decimal and hexadecimal (with a special
case for bool).
list: output an HTML list, formatting each item according to its
type.
float and Decimal: output the value as usual, but also in the
form of a fraction (why not?).

The behavior we want is shown in Example 9-19.

    evince -p 488 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 9-19. htmlize() generates HTML tailored to different object
types
>>> htmlize({1, 2, 3})
'<pre>{1, 2, 3}</pre>'
>>> htmlize(abs)
'<pre>&lt;built-in function abs&gt;</pre>'
>>> htmlize('Heimlich & Co.\n- a game')
'<p>Heimlich &amp; Co.<br/>\n- a game</p>'
>>> htmlize(42)
'<pre>42 (0x2a)</pre>'
>>> print(htmlize(['alpha', 66, {3, 2, 1}]))
<ul>
<li><p>alpha</p></li>
<li><pre>66 (0x42)</pre></li>
<li><pre>{1, 2, 3}</pre></li>
</ul>
>>> htmlize(True)
'<pre>True</pre>'
>>> htmlize(fractions.Fraction(2, 3))
'<pre>2/3</pre>'
>>> htmlize(2/3)
'<pre>0.6666666666666666 (2/3)</pre>'
>>> htmlize(decimal.Decimal('0.02380952'))
'<pre>0.02380952 (1/42)</pre>'

The original function is registered for object, so it serves as a catchall to handle argument types that don't match the other
implementations.
str objects are also HTML-escaped but wrapped in <p></p> with
<br/> line breaks inserted before each '\n'.

An int is shown in decimal and hexadecimal, inside <pre></pre>.

Each list item is formatted according to its type, and the whole sequence
rendered as an HTML list.

Although bool is an int subtype, it gets special treatment.

Show Fraction as a fraction.

    evince -p 489 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Show float and Decimal with an approximate fractional equivalent.

Function singledispatch
Because we don't have Java-style method overloading in Python, we can't
simply create variations of htmlize with different signatures for each data
type we want to handle differently. A possible solution in Python would be
to turn htmlize into a dispatch function, with a chain of if/elif/… or
match/case/… calling specialized functions like htmlize_str,
htmlize_int, etc. This is not extensible by users of our module, and is
unwieldy: over time, the htmlize dispatcher would become too big, and
the coupling between it and the specialized functions would be very tight.

The functools.singledispatch decorator allows different modules
to contribute to the overall solution, and lets you easily provide a
specialized functions even for types that belong to third party packages that
you can't edit. If you decorate a plain function with @singledispatch,
it becomes the entry point for a generic function: a group of functions to
perform the same operation in different ways, depending on the type of the
first argument. This is what is meant by the term single-dispatch. If more
arguments were used to select the specific functions, we'd have multipledispatch. Example 9-20 shows how.

Warning:
functools.singledispatch exists since Python 3.4, but it only supports type
hints since Python 3.7. The last two functions in Example 9-20 illustrate the syntax that
works in all versions of Python since 3.4.

Example 9-20. @singledispatch creates a custom
@htmlize.register to bundle several functions into a generic
function
from functools import singledispatch
from collections import abc
import fractions

    evince -p 490 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

import decimal
import html
import numbers
@singledispatch
def htmlize(obj: object) -> str:
content = html.escape(repr(obj))
return f'<pre>{content}</pre>'
@htmlize.register

def _(text: str) -> str:
content = html.escape(text).replace('\n', '<br/>\n')
return f'<p>{content}</p>'
@htmlize.register

def _(seq: abc.Sequence) -> str:
inner = '</li>\n<li>'.join(htmlize(item) for item in seq)
return '<ul>\n<li>' + inner + '</li>\n</ul>'
@htmlize.register

def _(n: numbers.Integral) -> str:
return f'<pre>{n} (0x{n:x})</pre>'
@htmlize.register

def _(n: bool) -> str:
return f'<pre>{n}</pre>'
@htmlize.register(fractions.Fraction)

def _(x) -> str:
frac = fractions.Fraction(x)
return f'<pre>{frac.numerator}/{frac.denominator}</pre>'
@htmlize.register(decimal.Decimal)
@htmlize.register(float)

def _(x) -> str:
frac = fractions.Fraction(x).limit_denominator()
return f'<pre>{x} ({frac.numerator}/{frac.denominator})</pre>'

@singledispatch marks the base function that handles the
object type.

Each specialized function is decorated with @«base».register
The type of the first argument given at runtime determines when this
particular function definition will be used. The name of the specialized

    evince -p 491 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

functions is irrelevant; _ is a good choice to make this clear.6
For each additional type to get special treatment, register a new function
with a matching type hint in the first parameter.

The numbers ABCs are useful for use with singledispatch.7
bool is a subtype-of numbers.Integral, but the
singledispatch logic seeks the implementation with the most
specific matching type, regardless of the order they appear in the code.

If you don't want to, or cannot, add type hints to the decorated function,
you can pass a type to the @«base».register decorator. This
syntax works in Python 3.4 or later.

The @«base».register decorator returns the undecorated function,
so it's possible to stack them to register two or more types on the same
implementation.8
When possible, register the specialized functions to handle ABCs (abstract
classes) such as numbers.Integral and abc.MutableSequence
instead of concrete implementations like int and list. This allows your
code to support a greater variety of compatible types. For example, a
Python extension can provide alternatives to the int type with fixed bit
lengths as subclasses of numbers.Integral.9
Tip:
Using ABCs or typing.Protocol with @singledispatch allows your code to
support existing or future classes that are actual or virtual subclasses of those ABCs, or
that implement those protocols. The use of ABCs and the concept of a virtual subclass
are subjects of Chapter 13.

    evince -p 492 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

A notable quality of the singledispatch mechanism is that you can
register specialized functions anywhere in the system, in any module. If you
later add a module with a new user-defined type, you can easily provide a
new custom function to handle that type. And you can write custom
functions for classes that you did not write and can't change.
singledispatch is a well-thought-out addition to the standard library,
and it offers more features than I can describe here. PEP 443 - Singledispatch generic functions is a good reference - but it doesn't mention the
use of type hints, which were added later. The functools module
documentation has improved and more up-to-date coverage with several
examples in its singledispatch entry.

Note:
@singledispatch is not designed to bring Java-style method overloading to
Python. A single class with many overloaded variations of a method is better than a
single function with a lengthy stretch of if/elif/elif/elif blocks. But both
solutions are flawed because they concentrate too much responsibility in a single code
unit - the class or the function. The advantage of @singledispatch is supporting
modular extension: each module can register a specialized function for each type it
supports. In a realistic use case, you would not have all the implementations of generic
function in the same module as in Example 9-20.

We've seen some decorators that take arguments, for example,
@lru_cache() and htmlize.register(float) created by
@singledispatch in Example 9-20. The next section shows how to
build decorators that accept parameters.

Parameterized Decorators
When parsing a decorator in source code, Python takes the decorated
function and passes it as the first argument to the decorator function. So
how do you make a decorator accept other arguments? The answer is: make
a decorator factory that takes those arguments and returns a decorator,

    evince -p 493 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

which is then applied to the function to be decorated. Confusing? Sure.

Let's start with an example based on the simplest decorator we've seen:

register in Example 9-21.

Example 9-21. Abridged registration.py module from Example 9-2, repeated
here for convenience
registry = []

def register(func):
print(f'running register({func})')
registry.append(func)
return func
@register
def f1():
print('running f1()')
print('running main()')
print('registry ->', registry)
f1()

A Parameterized Registration Decorator
In order to make it easy to enable or disable the function registration
performed by register, we'll make it accept an optional active
parameter which, if False, skips registering the decorated function.

Example 9-22 shows how. Conceptually, the new register function is
not a decorator but a decorator factory. When called, it returns the actual
decorator that will be applied to the target function.

Example 9-22. To accept parameters, the new register decorator must be
called as a function
registry = set()

def register(active=True):
def decorate(func):
print('running register'
f'(active={active})->decorate({func})')
if active:
registry.add(func)
else:
registry.discard(func)

    evince -p 494 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

return func
return decorate
@register(active=False)

def f1():
print('running f1()')
@register()

def f2():
print('running f2()')

def f3():
print('running f3()')

registry is now a set, so adding and removing functions is faster.
register takes an optional keyword argument.

The decorate inner function is the actual decorator; note how it takes
a function as argument.

Register func only if the active argument (retrieved from the
closure) is True.

If not active and func in registry, remove it.

Because decorate is a decorator, it must return a function.
register is our decorator factory, so it returns decorate.

The @register factory must be invoked as a function, with the
desired parameters.

If no parameters are passed, register must still be called as a
function - @register() - i.e., to return the actual decorator,
decorate.

    evince -p 495 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

The main point is that register() returns decorate, which is then
applied to the decorated function.

The code in Example 9-22 is in a registration_param.py module. If we
import it, this is what we get:

>>> import registration_param
running register(active=False)->decorate(<function f1 at
0x10063c1e0>)
running register(active=True)->decorate(<function f2 at
0x10063c268>)
>>> registration_param.registry
[<function f2 at 0x10063c268>]

Note how only the f2 function appears in the registry; f1 does not
appear because active=False was passed to the register decorator
factory, so the decorate that was applied to f1 did not add it to the
registry.

If, instead of using the @ syntax, we used register as a regular function,
the syntax needed to decorate a function f would be register()(f) to
add f to the registry, or register(active=False)(f) to not
add it (or remove it). See Example 9-23 for a demo of adding and removing
functions to the registry.

Example 9-23. Using the registration_param module listed in Example 9-22

>>> from registration_param import *
running register(active=False)->decorate(<function f1 at
0x10073c1e0>)
running register(active=True)->decorate(<function f2 at
0x10073c268>)
>>> registry
{<function f2 at 0x10073c268>}
>>> register()(f3)
running register(active=True)->decorate(<function f3 at
0x10073c158>)
<function f3 at 0x10073c158>
>>> registry
{<function f3 at 0x10073c158>, <function f2 at 0x10073c268>}
>>> register(active=False)(f2)
running register(active=False)->decorate(<function f2 at
0x10073c268>)

    evince -p 496 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

<function f2 at 0x10073c268>
>>> registry
{<function f3 at 0x10073c158>}

When the module is imported, f2 is in the registry.

The register() expression returns decorate, which is then
applied to f3.

The previous line added f3 to the registry.

This call removes f2 from the registry.

Confirm that only f3 remains in the registry.

The workings of parameterized decorators are fairly involved, and the one
we've just discussed is simpler than most. Parameterized decorators usually
replace the decorated function, and their construction requires yet another
level of nesting. Now we will explore the architecture of one such function
pyramid.

The Parameterized Clock Decorator
In this section, we'll revisit the clock decorator, adding a feature: users
may pass a format string to control the output of the clocked function
report. See Example 9-24.

Note:
For simplicity, Example 9-24 is based on the initial clock implementation from
Example 9-14, and not the improved one from Example 9-16 that uses
@functools.wraps, adding yet another function layer.

Example 9-24. Module clockdeco_param.py: the parameterized clock
decorator

    evince -p 497 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

import time
DEFAULT_FMT = '[{elapsed:0.8f}s] {name}({args}) -> {result}'

def clock(fmt=DEFAULT_FMT):
def decorate(func):

def clocked(*_args):
t0 = time.perf_counter()
_result = func(*_args)
elapsed = time.perf_counter() - t0
name = func.__name__
args = ', '.join(repr(arg) for arg in _args)
result = repr(_result)
print(fmt.format(**locals()))
return _result
return clocked
return decorate
if __name__ == '__main__':
@clock()

def snooze(seconds):
time.sleep(seconds)
for i in range(3):
snooze(.123)

clock is our parameterized decorator factory.
decorate is the actual decorator.
clocked wraps the decorated function.
_result is the actual result of the decorated function.
_args holds the actual arguments of clocked, while args is str
used for display.
result is the str representation of _result, for display.

Using **locals() here allows any local variable of clocked to be
referenced in the fmt.10

    evince -p 498 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

clocked will replace the decorated function, so it should return
whatever that function returns.
decorate returns clocked.
clock returns decorate.

In this self test, clock() is called without arguments, so the decorator
applied will use the default format str.

If you run Example 9-24 from the shell, this is what you get:

$ python3 clockdeco_param.py
[0.12412500s] snooze(0.123) -> None
[0.12411904s] snooze(0.123) -> None
[0.12410498s] snooze(0.123) -> None

To exercise the new functionality, let's have a look at Examples 9-25 and 926, which are two other modules using clockdeco_param, and the
outputs they generate.

Example 9-25. clockdeco_param_demo1.py
import time
from clockdeco_param import clock
@clock('{name}: {elapsed}s')

def snooze(seconds):
time.sleep(seconds)
for i in range(3):
snooze(.123)

Output of Example 9-25:

$ python3 clockdeco_param_demo1.py
snooze: 0.12414693832397461s
snooze: 0.1241159439086914s
snooze: 0.12412118911743164s

    evince -p 499 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Example 9-26. clockdeco_param_demo2.py
import time
from clockdeco_param import clock
@clock('{name}({args}) dt={elapsed:0.3f}s')

def snooze(seconds):
time.sleep(seconds)
for i in range(3):
snooze(.123)

Output of Example 9-26:

$ python3 clockdeco_param_demo2.py
snooze(0.123) dt=0.124s
snooze(0.123) dt=0.124s
snooze(0.123) dt=0.124s

Note:
Graham Dumpleton and Lennart Regebro - technical reviewer of the First Edition -
argue that decorators are best coded as classes implementing __call__, and not as
functions like the examples in this chapter. I agree that approach is better for non-trivial
decorators, but to explain the basic idea of this language feature, functions are easier to
understand. See "Further Reading", in particular Graham Dumpleton's blog and wrapt
module for industrial-strength techniques when building decorators.

The next section shows an example in the style recommended by
Dumpleton and Regebro.

A class-based clock decorator
As a final example, Example 9-27 lists the implementation of a
parameterized clock decorator implemented as a class with __call__.

Contrast Example 9-24 with Example 9-27. Which one do you prefer?

Example 9-27. Module clockdeco_cls.py: parameterized clock decorator
implemented as class
import time

    evince -p 500 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

DEFAULT_FMT = '[{elapsed:0.8f}s] {name}({args}) -> {result}'
class clock:

def __init__(self, fmt=DEFAULT_FMT):
self.fmt = fmt

def __call__(self, func):
def clocked(*_args):
t0 = time.perf_counter()
_result = func(*_args)
elapsed = time.perf_counter() - t0
name = func.__name__
args = ', '.join(repr(arg) for arg in _args)
result = repr(_result)
print(self.fmt.format(**locals()))
return _result
return clocked

Instead of a clock outer function, the clock class is our
parameterized decorator factory. I named it with a lowercase c to make
clear that this implementation is a drop-in replacement for the one in
Example 9-24.

The argument passed in the clock(my_format) is assigned to the
fmt parameter here. The class constructor returns an instance of
clock, with my_format stored in self.fmt.
__call__ makes the clock instance callable. When invoked, the
instance replaces the decorated function with clocked
clocked wraps the decorated function.

This ends our exploration of function decorators. We'll see class decorators
in Chapter 24.

    evince -p 501 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter Summary
We covered some difficult terrain in this chapter. I tried to make the journey
as smooth as possible, but we definitely entered the realm of
metaprogramming.

We started with a simple @register decorator without an inner function,
and finished with a parameterized @clock() involving two levels of
nested functions.

Registration decorators, though simple in essence, have real applications in
Python frameworks. We will apply the registration idea in one
implementation of the Strategy design pattern in Chapter 10.

Understanding how decorators actually work required covering the
difference between import time and runtime, then diving into variable
scoping, closures, and the new nonlocal declaration. Mastering closures
and nonlocal is valuable not only to build decorators, but also to code
event-oriented programs for GUIs or asynchronous I/O with callbacks, and
to adopt a functional style when it makes sense.

Parameterized decorators almost always involve at least two nested
functions, maybe more if you want to use @functools.wraps to
produce a decorator that provides better support for more advanced
techniques. One such technique is stacked decorators, which we saw in
Example 9-18. For more sophisticated decorators, a class-based
implementation may be easier to read and maintain.

As examples of parametrized decorators in the standard library, we visited
the powerful @cache and @singledispatch from the functools
module.

Further Reading
Item #26 of Brett Slatkin's Effective Python, Second Edition (AddisonWesley, 2019) covers best practices for function decorators and

    evince -p 502 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

recommends always using functools.wraps - which we saw in
Example 9-16.11
Graham Dumpleton has a series of in-depth blog posts about techniques for
implementing well-behaved decorators, starting with "How You
Implemented Your Python Decorator is Wrong". His deep expertise in this
matter is also nicely packaged in the wrapt module he wrote to simplify the
implementation of decorators and dynamic function wrappers, which
support introspection and behave correctly when further decorated, when
applied to methods and when used as attribute descriptors. Chapter 23 in
Part VI is about descriptors.

Chapter 9, Metaprogramming of the Python Cookbook, Third Edition by
David Beazley and Brian K. Jones (O'Reilly), has several recipes from
elementary decorators to very sophisticated ones, including one that can be
called as a regular decorator or as a decorator factory, e.g., @clock or
@clock(). That's "Recipe 9.6. Defining a Decorator That Takes an
Optional Argument" in that cookbook.

Michele Simionato authored a package aiming to "simplify the usage of
decorators for the average programmer, and to popularize decorators by
showing various non-trivial examples," according to the docs. It's available
on PyPI as the decorator package.

Created when decorators were still a new feature in Python, the Python
Decorator Library wiki page has dozens of examples. Because that page
started years ago, some of the techniques shown have been superseded, but
the page is still an excellent source of inspiration.
"Closures in Python" is a short blog post by Fredrik Lundh that explains the
terminology of closures.

PEP 3104 - Access to Names in Outer Scopes describes the introduction
of the nonlocal declaration to allow rebinding of names that are neither
local nor global. It also includes an excellent overview of how this issue is
resolved in other dynamic languages (Perl, Ruby, JavaScript, etc.) and the
pros and cons of the design options available to Python.

    evince -p 503 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

On a more theoretical level, PEP 227 - Statically Nested Scopes
documents the introduction of lexical scoping as an option in Python 2.1
and as a standard in Python 2.2, explaining the rationale and design choices
for the implementation of closures in Python.

PEP 443 provides the rationale and a detailed description of the singledispatch generic functions' facility. An old (March 2005) blog post by
Guido van Rossum, "Five-Minute Multimethods in Python", walks through
an implementation of generic functions (a.k.a. multimethods) using
decorators. His code supports multiple-dispatch (i.e., dispatch based on
more than one positional argument). Guido's multimethods code is
interesting, but it's a didactic example. For a modern, production-ready
implementation of multiple-dispatch generic functions, check out Reg by
Martijn Faassen - author of the model-driven and REST-savvy Morepath
web framework.

    evince -p 504 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

SOAPBOX
The designer of any language with first-class functions faces this issue:

being first-class objects, functions are defined in a certain scope but
may be invoked in other scopes. The question is: how to evaluate the
free variables? The first and simplest answer is "dynamic scope." This
means that free variables are evaluated by looking into the environment
where the function is invoked.

If Python had dynamic scope and no closures, we could improvise avg
- similar to Example 9-8 - like this:

>>> ### this is not a real Python console session! ###
>>> avg = make_averager()
>>> series = []
>>> avg(10)
10.0
>>> avg(11)
10.5
>>> avg(12)
11.0
>>> series = [1]
>>> avg(5)
3.0

Before using avg, we have to define series = [] ourselves, so
we must know that averager (inside make_averager) refers
to a list named series.

Behind the scenes, series accumulates the values to be averaged.

When series = [1] is executed, the previous list is lost. This
could happen by accident, when handling two independent running
averages at the same time.

Functions should be black boxes, with their implementation hidden
from users. But with dynamic scope, if a function uses free variables,

    evince -p 505 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

the programmer has to know its internals to set up an environment
where it works correctly. After years of struggling with the LaTeX
document preparation language, the excellent Practical LaTeX book by
George Grätzer taught me that LaTeX variables use dynamic scope.

That's why they were so confusing to me!

Emacs Lisp also uses dynamic scope, at least by default. See Dynamic
Binding in the Emacs Lisp manual for a short explanation.

Dynamic scope is easier to implement, which is probably why it was
the path taken by John McCarthy when he created Lisp, the first
language to have first-class functions. Paul Graham's article "The Roots
of Lisp" is an accessible explanation of John McCarthy's original paper
about the Lisp language: "Recursive Functions of Symbolic
Expressions and Their Computation by Machine, Part I". McCarthy's
paper is a masterpiece as great as Beethoven's 9th Symphony. Paul
Graham translated it for the rest of us, from mathematics to English and
running code.

Paul Graham's commentary explains how tricky dynamic scoping is.

Quoting from "The Roots of Lisp":

It's an eloquent testimony to the dangers of dynamic scope that even
the very first example of higher-order Lisp functions was broken
because of it. It may be that McCarthy was not fully aware of the
implications of dynamic scope in 1960. Dynamic scope remained in
Lisp implementations for a surprisingly long time - until Sussman
and Steele developed Scheme in 1975. Lexical scope does not
complicate the definition of eval very much, but it may make
compilers harder to write.

Today, lexical scope is the norm: free variables are evaluated
considering the environment where the function is defined. Lexical
scope complicates the implementation of languages with first-class
functions, because it requires the support of closures. On the other
hand, lexical scope makes source code easier to read. Most languages
invented since Algol have lexical scope.

    evince -p 506 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

For many years, Python lambdas did not provide closures,
contributing to the bad name of this feature among functionalprogramming geeks in the blogosphere. This was fixed in Python 2.2
(December 2001), but the blogosphere has a long memory. Since then,
lambda is embarrassing only because of its limited syntax.

Python Decorators and the Decorator Design Pattern
Python function decorators fit the general description of Decorator
given by Gamma et al. in Design Patterns: "Attach additional
responsibilities to an object dynamically. Decorators provide a flexible
alternative to subclassing for extending functionality."

At the implementation level, Python decorators do not resemble the
classic Decorator design pattern, but an analogy can be made.

In the design pattern, Decorator and Component are abstract
classes. An instance of a concrete decorator wraps an instance of a
concrete component in order to add behaviors to it. Quoting from
Design Patterns:

The decorator conforms to the interface of the component it
decorates so that its presence is transparent to the component's
clients. The decorator forwards requests to the component and may
perform additional actions (such as drawing a border) before or after
forwarding. Transparency lets you nest decorators recursively,
thereby allowing an unlimited number of added responsibilities." (p.
175)
In Python, the decorator function plays the role of a concrete
Decorator subclass, and the inner function it returns is a decorator
instance. The returned function wraps the function to be decorated,
which is analogous to the component in the design pattern. The returned
function is transparent because it conforms to the interface of the
component by accepting the same arguments. It forwards calls to the
component and may perform additional actions either before or after it.

Borrowing from the previous citation, we can adapt the last sentence to

    evince -p 507 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

say that "Transparency lets you stack decorators, thereby allowing an
unlimited number of added behaviors."

Note that I am not suggesting that function decorators should be used to
implement the Decorator pattern in Python programs. Although this can
be done in specific situations, in general the Decorator pattern is best
implemented with classes to represent the Decorator and the
components it will wrap.

1 That's the 1995 Design Patterns book by the so-called Gang of Four.
2 If you replace "function" with "class" in the previous sentence, you have a brief description of
what a class decorator does. Class decorators are covered in Chapter 24.
3 Thanks to tech reviewer Leonardo Rochael suggesting this summary.
4 Python does not have a program global scope, only module global scopes.
5 To clarify, this is not a typo: "memoization" is a computer science term vaguely related to
"memorization", but not the same.
6 Unfortunately, Mypy 0.770 complains when it sees multiple functions with the same name…
7 Despite the warning in "The Fall of the Numeric Tower", the number ABC are not
deprecated and you find them in Python 3 code.
8 Maybe one day you'll also be able to express this with single unparameterized
@htmlize.register and type hint using Union, but when I tried, Python raised a
TypeError with a message saying that Union is not a class. So, although PEP 484 syntax is
supported by @singledispatch, the semantics are not there yet.
9 NumPy, for example, implements several machine-oriented integer and floating-point types.
10 Tech reviewer Miroslav Šedivý noted: "It also means that code linters will complain about
unused variables since they tend to ignore uses of locals()." Yes, that's yet another
example of how static checking tools discourage the use of the dynamic features that attracted
me and countless programmers to Python in the first place. To make the linter happy, I could
spell out each local variable twice in the call: .format(elapsed=elapsed,
name=name, args=args, result=result). I'd rather not. If you use static checking
tools, it's very important to know when to ignore them.
11 I wanted to make the code as simple as possible, so I did not follow Slatkin's excellent advice
in all examples.

    evince -p 508 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter 10. Design Patterns with First-Class Functions
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form - the
author's raw and unedited content as they write - so you can take
advantage of these technologies long before the official release of these
titles.

This will be the 10th chapter of the final book. Please note that the
GitHub repo will be made active later on.

If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at fluentpython2e@ramalho.org.

Conformity to patterns is not a measure of goodness.1
- Ralph Johnson, Coauthor of the Design Patterns classic
In software engineering, a design pattern is a general recipe for solving a
common design problem. You don't need to know design patterns to follow
this chapter. I will explain the patterns used in the examples.

The use of design patterns in programming was popularized by the
landmark book Design Patterns: Elements of Reusable Object-Oriented
Software (Addison-Wesley, 1995) by Erich Gamma, Richard Helm, Ralph
Johnson & John Vlissides - a.k.a. "the Gang of Four." The book is a catalog
of 23 patterns consisting of arrangements of classes exemplified with code
in C++, but assumed to be useful in other Object-Oriented languages as
well.

    evince -p 509 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Although design patterns are language-independent, that does not mean
every pattern applies to every language. For example, Chapter 17 will show
that it doesn't make sense to emulate the recipe of the Iterator pattern in
Python, because the pattern is embedded in the language and ready to use in
the form of generators - which don't need classes to work, and require less
code than the classic recipe.

The authors of Design Patterns acknowledge in their Introduction that the
implementation language determines which patterns are relevant:

The choice of programming language is important because it influences
one's point of view. Our patterns assume Smalltalk/C++-level language
features, and that choice determines what can and cannot be
implemented easily. If we assumed procedural languages, we might have
included design patterns called "Inheritance," "Encapsulation," and
"Polymorphism." Similarly, some of our patterns are supported directly
by the less common Object-Oriented languages. CLOS has multimethods, for example, which lessen the need for a pattern such as
Visitor.2
In his 1996 presentation, "Design Patterns in Dynamic Languages", Peter
Norvig states that 16 out of the 23 patterns in the original Design Patterns
book become either "invisible or simpler" in a dynamic language (slide 9).

He was talking about the Lisp and Dylan languages, but many of the
relevant dynamic features are also present in Python. In particular, in the
context of languages with first-class functions, Norvig suggests rethinking
the classic patterns known as Strategy, Command, Template Method, and
Visitor.

The goal of this chapter is to show how - in some cases - functions can do
the same work as classes, with code that more readable and concise. We
will refactor an implementation of Strategy using functions as objects,
removing a lot of boilerplate code. We'll also discuss a similar approach to
simplifying the Command pattern.

What's new in this chapter

    evince -p 510 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

I moved this chapter to the end of Part III so I could apply a registration
decorator in "Decorator-Enhanced Strategy Pattern" and also use type hints
in the examples. Most type hints used in this chapter are not complicated,
and they do help with readability.

Case Study: Refactoring Strategy
Strategy is a good example of a design pattern that can be simpler in Python
if you leverage functions as first-class objects. In the following section, we
describe and implement Strategy using the "classic" structure described in
Design Patterns. If you are familiar with the classic pattern, you can skip to
"Function-Oriented Strategy" where we refactor the code using functions,
significantly reducing the line count.

Classic Strategy
The UML class diagram in Figure 10-1 depicts an arrangement of classes
that exemplifies the Strategy pattern.

    evince -p 511 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

    evince -p 512 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Figure 10-1. UML class diagram for order discount processing implemented with the Strategy design
pattern

The Strategy pattern is summarized like this in Design Patterns:

Define a family of algorithms, encapsulate each one, and make them
interchangeable. Strategy lets the algorithm vary independently from
clients that use it.

A clear example of Strategy applied in the ecommerce domain is computing
discounts to orders according to the attributes of the customer or inspection
of the ordered items.

Consider an online store with these discount rules:

Customers with 1,000 or more fidelity points get a global 5%
discount per order.

A 10% discount is applied to each line item with 20 or more units
in the same order.

Orders with at least 10 distinct items get a 7% global discount.

For brevity, let's assume that only one discount may be applied to an order.

The UML class diagram for the Strategy pattern is depicted in Figure 10-1.

Its participants are:

Context
Provides a service by delegating some computation to interchangeable
components that implement alternative algorithms. In the ecommerce
example, the context is an Order, which is configured to apply a
promotional discount according to one of several algorithms.

Strategy
The interface common to the components that implement the different
algorithms. In our example, this role is played by an abstract class
called Promotion.

    evince -p 513 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Concrete Strategy
One of the concrete subclasses of Strategy. FidelityPromo,
BulkPromo, and LargeOrderPromo are the three concrete
strategies implemented.

The code in Example 10-1 follows the blueprint in Figure 10-1. As
described in Design Patterns, the concrete strategy is chosen by the client
of the context class. In our example, before instantiating an order, the
system would somehow select a promotional discount strategy and pass it to
the Order constructor. The selection of the strategy is outside the scope of
the pattern.

Example 10-1. Implementation of the Order class with pluggable discount
strategies.
from abc import ABC, abstractmethod
from collections.abc import Sequence
from decimal import Decimal
from typing import NamedTuple, Optional
class Customer(NamedTuple):

name: str
fidelity: int
class LineItem(NamedTuple):

product: str
quantity: int
price: Decimal

def total(self) -> Decimal:
return self.price * self.quantity
class Order(NamedTuple): # the Context
customer: Customer
cart: Sequence[LineItem]
promotion: Optional['Promotion'] = None

def total(self) -> Decimal:
totals = (item.total() for item in self.cart)

    evince -p 514 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

return sum(totals, start=Decimal(0))

def due(self) -> Decimal:
if self.promotion is None:
discount = Decimal(0)
else:
discount = self.promotion.discount(self)
return self.total() - discount

def __repr__(self):
return f'<Order total: {self.total():.2f} due:

{self.due():.2f}>'
class Promotion(ABC): # the Strategy: an abstract base class
@abstractmethod
def discount(self, order: Order) -> Decimal:
"""Return discount as a positive dollar amount"""
class FidelityPromo(Promotion): # first Concrete Strategy
"""5% discount for customers with 1000 or more fidelity
points"""

def discount(self, order: Order) -> Decimal:
rate = Decimal('0.05')
if order.customer.fidelity >= 1000:
return order.total() * rate
return Decimal(0)
class BulkItemPromo(Promotion): # second Concrete Strategy
"""10% discount for each LineItem with 20 or more units"""

def discount(self, order: Order) -> Decimal:
discount = Decimal(0)
for item in order.cart:
if item.quantity >= 20:
discount += item.total() * Decimal('0.1')
return discount
class LargeOrderPromo(Promotion): # third Concrete Strategy
"""7% discount for orders with 10 or more distinct items"""

def discount(self, order: Order) -> Decimal:
distinct_items = {item.product for item in order.cart}
if len(distinct_items) >= 10:
    evince -p 515 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

return order.total() * Decimal('0.07')
return Decimal(0)

Note that in Example 10-1, I coded Promotion as an abstract base class
(ABC), to use the @abstractmethod decorator and make the pattern
more explicit.

Example 10-2 shows doctests used to demonstrate and verify the operation
of a module implementing the rules described earlier.

Example 10-2. Sample usage of Order class with different promotions
applied.
>>> joe = Customer('John Doe', 0)
>>> ann = Customer('Ann Smith', 1100)
>>> cart = (LineItem('banana', 4, Decimal('.5')),
...

LineItem('apple', 10, Decimal('1.5')),
...

LineItem('watermelon', 5, Decimal(5)))
>>> Order(joe, cart, FidelityPromo())
<Order total: 42.00 due: 42.00>
>>> Order(ann, cart, FidelityPromo())
<Order total: 42.00 due: 39.90>
>>> banana_cart = (LineItem('banana', 30, Decimal('.5')),
...

LineItem('apple', 10, Decimal('1.5')))
>>> Order(joe, banana_cart, BulkItemPromo())
<Order total: 30.00 due: 28.50>
>>> long_cart = tuple(LineItem(str(sku), 1, Decimal(1))
...
for sku in range(10))
>>> Order(joe, long_cart, LargeOrderPromo())
<Order total: 10.00 due: 9.30>
>>> Order(joe, cart, LargeOrderPromo())
<Order total: 42.00 due: 42.00>

Two customers: joe has 0 fidelity points, ann has 1,100.

One shopping cart with three line items.

The FidelityPromo promotion gives no discount to joe.
ann gets a 5% discount because she has at least 1,000 points.

The banana_cart has 30 units of the "banana" product and 10
apples.

    evince -p 516 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Thanks to the BulkItemPromo, joe gets a $1.50 discount on the
bananas.
long_cart has 10 different items at $1.00 each.
joe gets a 7% discount on the whole order because of
LargerOrderPromo.

Example 10-1 works perfectly well, but the same functionality can be
implemented with less code in Python by using functions as objects. The
next section shows how.

Function-Oriented Strategy
Each concrete strategy in Example 10-1 is a class with a single method,
discount. Furthermore, the strategy instances have no state (no instance
attributes). You could say they look a lot like plain functions, and you
would be right. Example 10-3 is a refactoring of Example 10-1, replacing
the concrete strategies with simple functions and removing the Promo
abstract class. Only small adjustments are needed in the Order class.3
Example 10-3. Order class with discount strategies implemented as
functions.
from collections.abc import Sequence
from dataclasses import dataclass
from decimal import Decimal
from typing import Optional, Callable, NamedTuple
class Customer(NamedTuple):

name: str
fidelity: int
class LineItem(NamedTuple):

product: str
quantity: int
price: Decimal

    evince -p 517 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

def total(self):
return self.price * self.quantity
@dataclass(frozen=True)
class Order: # the Context
customer: Customer
cart: Sequence[LineItem]
promotion: Optional[Callable[['Order'], Decimal]] = None

def total(self) -> Decimal:
totals = (item.total() for item in self.cart)
return sum(totals, start=Decimal(0))

def due(self) -> Decimal:
if self.promotion is None:
discount = Decimal(0)
else:
discount = self.promotion(self)
return self.total() - discount

def __repr__(self):
return f'<Order total: {self.total():.2f} due:

{self.due():.2f}>'

def fidelity_promo(order: Order) -> Decimal:
"""5% discount for customers with 1000 or more fidelity
points"""
if order.customer.fidelity >= 1000:
return order.total() * Decimal('0.05')
return Decimal(0)

def bulk_item_promo(order: Order) -> Decimal:
"""10% discount for each LineItem with 20 or more units"""
discount = Decimal(0)
for item in order.cart:
if item.quantity >= 20:
discount += item.total() * Decimal('0.1')
return discount

def large_order_promo(order: Order) -> Decimal:
"""7% discount for orders with 10 or more distinct items"""
distinct_items = {item.product for item in order.cart}

    evince -p 518 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

if len(distinct_items) >= 10:
return order.total() * Decimal('0.07')
return Decimal(0)

This type hint says: promotion may be None, or it may be a callable
that takes an Order argument and returns a Decimal.

To compute a discount, call the self.promotion callable, passing
self as an argument. See below for the reason.

No abstract class.

Each strategy is a function.

WHY SELF.PROMOTION(SELF)
In the Order class, promotion is not a method. It's an instance attribute that happens
to be callable. So the first part of the expression, self.promotion, retrieves that
callable. To invoke it, we must provide an instance of Order, which in this case is
self. That's why self appears twice in that expression.
"Methods Are Descriptors" will explain the mechanism that binds methods to instances
automatically. It does not apply to promotion because it is not a method.

The code in Example 10-3 is shorter than Example 10-1. Using the new
Order is also a bit simpler, as shown in the Example 10-4 doctests.

Example 10-4. Sample usage of Order class with promotions as functions

>>> joe = Customer('John Doe', 0)
>>> ann = Customer('Ann Smith', 1100)
>>> cart = [LineItem('banana', 4, Decimal('.5')),
...

LineItem('apple', 10, Decimal('1.5')),
...

LineItem('watermelon', 5, Decimal(5))]
>>> Order(joe, cart, fidelity_promo)
<Order total: 42.00 due: 42.00>
>>> Order(ann, cart, fidelity_promo)
<Order total: 42.00 due: 39.90>
>>> banana_cart = [LineItem('banana', 30, Decimal('.5')),
...

LineItem('apple', 10, Decimal('1.5'))]
>>> Order(joe, banana_cart, bulk_item_promo)

    evince -p 519 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

<Order total: 30.00 due: 28.50>
>>> long_cart = [LineItem(str(item_code), 1, Decimal(1))
...
for item_code in range(10)]
>>> Order(joe, long_cart, large_order_promo)
<Order total: 10.00 due: 9.30>
>>> Order(joe, cart, large_order_promo)
<Order total: 42.00 due: 42.00>

Same test fixtures as Example 10-1.

To apply a discount strategy to an Order, just pass the promotion
function as an argument.

A different promotion function is used here and in the next test.

Note the callouts in Example 10-4: there is no need to instantiate a new
promotion object with each new order: the functions are ready to use.

It is interesting to note that in Design Patterns the authors suggest:

"Strategy objects often make good flyweights."4 A definition of the
Flyweight in another part of that work states: "A flyweight is a shared
object that can be used in multiple contexts simultaneously."5 The sharing
is recommended to reduce the cost of creating a new concrete strategy
object when the same strategy is applied over and over again with every
new context - with every new Order instance, in our example. So, to
overcome a drawback of the Strategy pattern - its runtime cost - the authors
recommend applying yet another pattern. Meanwhile, the line count and
maintenance cost of your code are piling up.

A thornier use case, with complex concrete strategies holding internal state,
may require all the pieces of the Strategy and Flyweight design patterns
combined. But often concrete strategies have no internal state; they only
deal with data from the context. If that is the case, then by all means use
plain old functions instead of coding single-method classes implementing a
single-method interface declared in yet another class. A function is more
lightweight than an instance of a user-defined class, and there is no need for
Flyweight because each strategy function is created just once per Python

    evince -p 520 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

process when it loads the module. A plain function is also "a shared object
that can be used in multiple contexts simultaneously."

Now that we have implemented the Strategy pattern with functions, other
possibilities emerge. Suppose you want to create a "meta-strategy" that
selects the best available discount for a given Order. In the following
sections we study additional refactorings that implement this requirement
using a variety of approaches that leverage functions and modules as
objects.

Choosing the Best Strategy: Simple Approach
Given the same customers and shopping carts from the tests in Example 104, we now add three additional tests in Example 10-5.

Example 10-5. The best_promo function applies all discounts and returns
the largest
>>> Order(joe, long_cart, best_promo)
<Order total: 10.00 due: 9.30>
>>> Order(joe, banana_cart, best_promo)
<Order total: 30.00 due: 28.50>
>>> Order(ann, cart, best_promo)
<Order total: 42.00 due: 39.90>

best_promo selected the larger_order_promo for customer
joe.

Here joe got the discount from bulk_item_promo for ordering lots
of bananas.

Checking out with a simple cart, best_promo gave loyal customer
ann the discount for the fidelity_promo.

The implementation of best_promo is very simple. See Example 10-6.

Example 10-6. best_promo finds the maximum discount iterating over a list
of functions

    evince -p 521 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

promos = [fidelity_promo, bulk_item_promo, large_order_promo]

def best_promo(order: Order) -> Decimal:
"""Compute the best discount available"""
return max(promo(order) for promo in promos)

promos: list of the strategies implemented as functions.
best_promo takes an instance of Order as argument, as do the other
*_promo functions.

Using a generator expression, we apply each of the functions from
promos to the order, and return the maximum discount computed.

Example 10-6 is straightforward: promos is a list of functions. Once
you get used to the idea that functions are first-class objects, it naturally
follows that building data structures holding functions often makes sense.

Although Example 10-6 works and is easy to read, there is some duplication
that could lead to a subtle bug: to add a new promotion strategy, we need to
code the function and remember to add it to the promos list, or else the
new promotion will work when explicitly passed as an argument to Order,
but will not be considered by best_promotion.

Read on for a couple of solutions to this issue.

Finding Strategies in a Module
Modules in Python are also first-class objects, and the standard library
provides several functions to handle them. The built-in globals is
described as follows in the Python docs:

globals()
Return a dictionary representing the current global symbol table. This is
always the dictionary of the current module (inside a function or

    evince -p 522 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

method, this is the module where it is defined, not the module from
which it is called).

Example 10-7 is a somewhat hackish way of using globals to help
best_promo automatically find the other available *_promo functions.

Example 10-7. The promos list is built by introspection of the module
global namespace
from decimal import Decimal
from strategy import Order
from strategy import (
fidelity_promo, bulk_item_promo, large_order_promo
)
promos = [promo for name, promo in globals().items()
if name.endswith('_promo') and
name != 'best_promo'
]

def best_promo(order: Order) -> Decimal:
"""Compute the best discount available"""
return max(promo(order) for promo in promos)

Import the promotion functions so they are available in the global
namespace.6
Iterate over each item in the dict returned by globals().

Select only values where the name ends with the _promo suffix and…

Filter out best_promo itself, to avoid an infinite recursion when
best_promo is called.

No changes in best_promo.

Another way of collecting the available promotions would be to create a
module and put all the strategy functions there, except for best_promo.

    evince -p 523 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

In Example 10-8, the only significant change is that the list of strategy
functions is built by introspection of a separate module called
promotions. Note that Example 10-8 depends on importing the
promotions module as well as inspect, which provides high-level
introspection functions.

Example 10-8. The promos list is built by introspection of a new promotions
module
from decimal import Decimal
import inspect
from strategy import Order
import promotions
promos = [func for _, func in inspect.getmembers(promotions,
inspect.isfunction)]

def best_promo(order: Order) -> Decimal:
"""Compute the best discount available"""
return max(promo(order) for promo in promos)

The function inspect.getmembers returns the attributes of an object
- in this case, the promotions module - optionally filtered by a
predicate (a boolean function). We use inspect.isfunction to get
only the functions from the module.

Example 10-8 works regardless of the names given to the functions; all that
matters is that the promotions module contains only functions that
calculate discounts given orders. Of course, this is an implicit assumption of
the code. If someone were to create a function with a different signature in
the promotions module, then best_promo would break while trying
to apply it to an order.

We could add more stringent tests to filter the functions, by inspecting their
arguments for instance. The point of Example 10-8 is not to offer a
complete solution, but to highlight one possible use of module
introspection.

    evince -p 524 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

A more explicit alternative to dynamically collecting promotional discount
functions would be to use a simple decorator. That's next.

Decorator-Enhanced Strategy Pattern
Recall that our main issue with Example 10-6 is the repetition of the
function names in their definitions and then in the promos list used by the
best_promo function to determine the highest discount applicable. The
repetition is problematic because someone may add a new promotional
strategy function and forget to manually add it to the promos list - in
which case, best_promo will silently ignore the new strategy,
introducing a subtle bug in the system. Example 10-9 solves this problem
with the technique covered in "Registration decorators".

Example 10-9. The promos list is filled by the promotion decorator
Promotion = Callable[[Order], Decimal]
promos: list[Promotion] = []

def promotion(promo: Promotion) -> Promotion:
promos.append(promo)
return promo

def best_promo(order: Order) -> Decimal:
"""Compute the best discount available"""
return max(promo(order) for promo in promos)
@promotion
def fidelity(order: Order) -> Decimal:
"""5% discount for customers with 1000 or more fidelity
points"""
if order.customer.fidelity >= 1000:
return order.total() * Decimal('0.05')
return Decimal(0)
@promotion
def bulk_item(order: Order) -> Decimal:
"""10% discount for each LineItem with 20 or more units"""

    evince -p 525 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

discount = Decimal(0)
for item in order.cart:
if item.quantity >= 20:
discount += item.total() * Decimal('0.1')
return discount
@promotion
def large_order(order: Order) -> Decimal:
"""7% discount for orders with 10 or more distinct items"""
distinct_items = {item.product for item in order.cart}
if len(distinct_items) >= 10:
return order.total() * Decimal('0.07')
return Decimal(0)

The promos list is a module global, and starts empty.
promotion is a registration decorator: it returns the promo function
unchanged, after appending it to the promos list.

No changes needed to best_promo, because it relies on the promos
list.

Any function decorated by @promotion will be added to promos.

This solution has several advantages over the others presented before:

The promotion strategy functions don't have to use special names
- no need for the _promo suffix.

The @promotion decorator highlights the purpose of the
decorated function, and also makes it easy to temporarily disable a
promotion: just comment out the decorator.

Promotional discount strategies may be defined in other modules,
anywhere in the system, as long as the @promotion decorator is
applied to them.

In the next section, we discuss Command - another design pattern that is
sometimes implemented via single-method classes when plain functions

    evince -p 526 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

would do.

The Command Pattern
Command is another design pattern that can be simplified by the use of
functions passed as arguments. Figure 10-2 shows the arrangement of
classes in the Command pattern.

    evince -p 527 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

    evince -p 528 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Figure 10-2. UML class diagram for menu-driven text editor implemented with the Command design
pattern. Each command may have a different receiver: the object that implements the action. For
PasteCommand, the receiver is the Document. For OpenCommand, the receiver is the application.

The goal of Command is to decouple an object that invokes an operation
(the Invoker) from the provider object that implements it (the Receiver). In
the example from Design Patterns, each invoker is a menu item in a
graphical application, and the receivers are the document being edited or
the application itself.

The idea is to put a Command object between the two, implementing an
interface with a single method, execute, which calls some method in the
Receiver to perform the desired operation. That way the Invoker does not
need to know the interface of the Receiver, and different receivers can be
adapted through different Command subclasses. The Invoker is configured
with a concrete command and calls its execute method to operate it. Note
in Figure 10-2 that MacroCommand may store a sequence of commands;
its execute() method calls the same method in each command stored.

Quoting from Gamma et al., "Commands are an Object-Oriented
replacement for callbacks." The question is: do we need an Object-Oriented
replacement for callbacks? Sometimes yes, but not always.

Instead of giving the Invoker a Command instance, we can simply give it a
function. Instead of calling command.execute(), the Invoker can just
call command(). The MacroCommand can be implemented with a class
implementing __call__. Instances of MacroCommand would be
callables, each holding a list of functions for future invocation, as
implemented in Example 10-10.

Example 10-10. Each instance of MacroCommand has an internal list of
commands
class MacroCommand:

"""A command that executes a list of commands"""

def __init__(self, commands):
self.commands = list(commands)

def __call__(self):
    evince -p 529 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

for command in self.commands:
command()

Building a list from the commands arguments ensures that it is iterable
and keeps a local copy of the command references in each
MacroCommand instance.

When an instance of MacroCommand is invoked, each command in
self.commands is called in sequence.

More advanced uses of the Command pattern - to support undo, for
example - may require more than a simple callback function. Even then,
Python provides a couple of alternatives that deserve consideration:

A callable instance like MacroCommand in Example 10-10 can
keep whatever state is necessary, and provide extra methods in
addition to __call__.

A closure can be used to hold the internal state of a function
between calls.

This concludes our rethinking of the Command pattern with first-class
functions. At a high level, the approach here was similar to the one we
applied to Strategy: replacing with callables the instances of a participant
class that implemented a single-method interface. After all, every Python
callable implements a single-method interface and that method is named
__call__.

    evince -p 530 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

Chapter Summary
As Peter Norvig pointed out a couple of years after the classic Design
Patterns book appeared, "16 of 23 patterns have qualitatively simpler
implementation in Lisp or Dylan than in C++ for at least some uses of each
pattern" (slide 9 of Norvig's "Design Patterns in Dynamic Languages"
presentation). Python shares some of the dynamic features of the Lisp and
Dylan languages, in particular first-class functions, our focus in this part of
the book.

From the same talk quoted at the start of this chapter, in reflecting on the
20th anniversary of Design Patterns: Elements of Reusable Object-Oriented
Software, Ralph Johnson has stated that one of the failings of the book is
"Too much emphasis on patterns as end-points instead of steps in the design
process."7 In this chapter, we used the Strategy pattern as a starting point: a
working solution that we could simplify using first-class functions.

In many cases, functions or callable objects provide a more natural way of
implementing callbacks in Python than mimicking the Strategy or the
Command patterns as described by Gamma, Helm, Johnson & Vlissides.

The refactoring of Strategy and the discussion of Command in this chapter
are examples of a more general insight: sometimes you may encounter a
design pattern or an API that requires that components implement an
interface with a single method, and that method has a generic-sounding
name such as "execute", "run", or "do_it". Such patterns or APIs often can
be implemented with less boilerplate code in Python using functions as
first-class objects.

Further Reading
"Recipe 8.21. Implementing the Visitor Pattern," in the Python Cookbook,
Third Edition (O'Reilly), by David Beazley and Brian K. Jones, presents an
elegant implementation of the Visitor pattern in which a NodeVisitor
class handles methods as first-class objects.

    evince -p 531 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

On the general topic of design patterns, the choice of readings for the
Python programmer is not as broad as what is available to other language
communities.

Learning Python Design Patterns, by Gennadiy Zlobin (Packt), is the only
book that I have seen entirely devoted to patterns in Python. But Zlobin's
work is quite short (100 pages) and covers eight of the original 23 design
patterns.

Expert Python Programming by Tarek Ziadé (Packt) is one of the best
intermediate-level Python books in the market, and its final chapter, "Useful
Design Patterns," presents several of the classic patterns from a Pythonic
perspective.

Alex Martelli has given several talks about Python Design Patterns. There
is a video of his EuroPython 2011 presentation and a set of slides on his
personal website. I've found different slide decks and videos over the years,
of varying lengths, so it is worthwhile to do a thorough search for his name
with the words "Python Design Patterns." A publisher told me Martelli is
working on a book about this subject. I will certainly get it when it comes
out.

There are many books about design patterns in the context of Java, but
among them the one I like most is Head First Design Patterns, Second
Edition by Eric Freeman & Elisabeth Robson (O'Reilly). It explains 16 of
the 23 classic patterns. If you like the wacky style of the Head First series
and need an introduction to this topic, you will love that work. It is Javacentric, but the Second Edition was uptaded to reflect the addition of firstclass functions in Java, making some of the examples closer to code we'd
write in Python.

For a fresh look at patterns from the point of view of a dynamic language
with duck typing and first-class functions, Design Patterns in Ruby by Russ
Olsen (Addison-Wesley) has many insights that are also applicable to
Python. In spite of their many syntactic differences, at the semantic level
Python and Ruby are closer to each other than to Java or C++.

    evince -p 532 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

In Design Patterns in Dynamic Languages (slides), Peter Norvig shows how
first-class functions (and other dynamic features) make several of the
original design patterns either simpler or unnecessary.

The Introduction of the original Design Patterns book by Gamma et al. is
worth the price of the book - more than the catalog of 23 patterns which
includes recipes ranging from very important to rarely useful. The widely
quoted design principles "Program to an interface, not an implementation"
and "Favor object composition over class inheritance" both come from that
Introduction.

The application of patterns to design originated with the architect
Christopher Alexander, presented in the book A Pattern Language (Oxford
University Press, 1977). Alexander's idea is to create a standard vocabulary
allowing teams to share common design decisions while designing
buildings. M. J. Dominus wrote "Design Patterns" Aren't: an intriguing
slide deck and postscript text arguing that Alexander's original vision of
patterns is more profound, more human, and also applicable to software
engineering.

    evince -p 533 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

SOAPBOX
Python has first-class functions and first-class types, features that
Norvig claims affect 10 of the 23 patterns (slide 10 of Design Patterns
in Dynamic Languages). In the Chapter 9, we saw that Python also has
generic functions ("Single Dispatch Generic Functions"), a limited form
of the CLOS multimethods that Gamma et al. suggest as a simpler way
to implement the classic Visitor pattern. Norvig, on the other hand, says
that multimethods simplify the Builder pattern (slide 10). Matching
design patterns to language features is not an exact science.

In classrooms around the world, design patterns are frequently taught
using Java examples. I've heard more than one student claim that they
were led to believe that the original design patterns are useful in any
implementation language. It turns out that the "classic" 23 patterns from
the Gamma et al. book apply to "classic" Java very well in spite of
being originally presented mostly in the context of C++ - a few have
Smalltalk examples in the book. But that does not mean every one of
those patterns applies equally well in any language. The authors are
explicit right at the beginning of their book that "some of our patterns
are supported directly by the less common Object-Oriented languages"
(recall full quote on first page of this chapter).

The Python bibliography about design patterns is very thin, compared
to that of Java, C++, or Ruby. In "Further Reading" I mentioned
Learning Python Design Patterns by Gennadiy Zlobin, which was
published as recently as November 2013. In contrast, Russ Olsen's
Design Patterns in Ruby was published in 2007 and has 384 pages -
284 more than Zlobin's work.

Now that Python is becoming increasingly popular in academia, let's
hope more will be written about design patterns in the context of this
language. Also, Java 8 introduced method references and anonymous
functions, and those highly anticipated features are likely to prompt
fresh approaches to patterns in Java - recognizing that as languages

    evince -p 534 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

evolve, so must our understanding of how to apply the classic design
patterns.

The __call__ of the wild
As we collaborated to put the final touches to this book, tech reviewer
Leonardo Rochael wondered…

If functions have a __call__ method, and methods are also callable,
do __call__ methods also have a __call__ method?

I don't know if his discovery is useful, but it is a fun fact:

>>> def turtle():

...
return 'eggs'
...
>>> turtle()
'eggs'
>>> turtle.__call__()
'eggs'
>>> turtle.__call__.__call__()
'eggs'
>>> turtle.__call__.__call__.__call__()
'eggs'
>>> turtle.__call__.__call__.__call__.__call__()
'eggs'
>>> turtle.__call__.__call__.__call__.__call__.__call__()
'eggs'
>>>
turtle.__call__.__call__.__call__.__call__.__call__.__call__()
'eggs'
>>>
turtle.__call__.__call__.__call__.__call__.__call__.__call__._
_call__()
'eggs'

Turtles all the way down!

1 From a slide in the talk "Root Cause Analysis of Some Faults in Design Patterns," presented
by Ralph Johnson at IME/CCSL, Universidade de São Paulo, Nov. 15, 2014.
2 Quoted from page 4 of Design Patterns (Addison-Wesley, 1995).

    evince -p 535 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &

3 I had to reimplement Order with @dataclass due to a bug in Mypy. You may ignore this
detail, because this class works with NamedTuple as well, just like in Example 10-1. If
Order is a NamedTuple, Mypy 0.910 crashes when checking the type hint for
promotion. I tried adding # type ignore to that specific line, but Mypy crashed
anyway. Mypy handles the same type hint correctly if Order is built with @dataclass.

Issue #9397 is unresolved as of July 19, 2021. Hopefully it will be fixed by the time you read
this.
4 See page 323 of Design Patterns.
5 idem, p. 196
6 flake8 and VS Code both complain that these names are imported but not used. By definition,
static analysis tools cannot understand the dynamic nature of Python. If we heed every advice
from such tools, we'll soon be writing grim and verbose Java-like code with Python syntax.
7 "Root Cause Analysis of Some Faults in Design Patterns", presented by Johnson at IME-USP,
November 15, 2014.

    evince -p 536 ~/Empire/Doks/Comp/lang/py/flupy-2e_2021.pdf &



__pyrecursion

        evince -p 1 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &

Python Interview Questions ----

Recursion:

Define a base case and continue
reducing the problem until the base
case is reached.

- 1. Divide a problem into
  subproblem. (Subproblems must be
  same as that of original)
- 2. Solve those sub problems.
- 3. Combine the results to solve
  the original.

Eg:

array = [1,7,8,5,4]

def length_of_list(array):
  if array == []:  # base case
    return 0
  return 1 + length_of_list(array[1:])
a=length_of_list(array)  
print(a)

o/p: 5

Errors:
  RecursionError: maximum recursion
  depth exceeded ====> base
  condition never met

To understand recursion, understand STACK.

Stack is a data structure that
holds sequence of data and lets you
interact with topmost data item
only. == LIFO

Python's list follows LIFO

    evince -p 2 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &

Eg-2: Python remembers the last
function call and acts accordingly
i.e order of execution is - recent
fn call -----> first fn call.

def a():
  print("start of a")
  b()
  print("end of a")

def b():
  print("start of b")
  c()
  print("end of b")

def c():
  print("start of c")
  print("end of c")
a()

O/P:

start of a
start of b
start of c
end of c
end of b
end of a


Extra Credit;
https://pymotw.com/3/inspect
https://pymotw.com/3/traceback

    evince -p 3 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &


Frame objects (frame object is a
function call) are stored in Stack,
o/p is popped out each time (2 *
1.3 * 2 ... 5 * 24) (In this case 5
being the last element in Stack)

Fn calls pushes frame objects to stack and return pops it

Video Reference: https://youtu.be/AfBqVVKg4GE


When to use recursion? ----

- Problem that has tree like
  structure.
- When the problem requires
  backtracing.


Limitation:

Python has limitation of 1000
function calls , eg:
factorial(1001), where our script
involves more than 1000 fn calls.
=> gives recursion error.

To overcome this, follow:

- Tail call
  optimization/Elimination: =>
  optimizing the recursive function
  call (which is actually the tail
  of recursive function).

Without TCO:

- The first function is not tail
  recursive because when the
  recursive call is made, the
  function needs to keep track of
  the multiplication it needs to do
  with the result after the call
  returns.

    evince -p 4 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &

Normal Recursive Factorial Can't be TC-Optimized:

def factorial(number):
  if number == 1
    # base case
    return 1
  else:
    # recursive case
    return number * factorial(number - 1)
print(factorial(5))

With TCO: = Cpython doesn't implement TCO 

def factorial(number, accumulator=1):
  if number == 0
    # base case
    return accumulator
  else:
    # recursive case
    return factorial(number - 1, number * accumulator)
print(factorial(5))

TCO => Overcome Stackoverflow error.

  Memoization: reduces To remember the function return values so that the
  next time if the fn takes same parameter, it eliminates the need for
  recalculating

    evince -p 5 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &

___INSERT_FIGURE

O(1)  simple checks and variable assignments.

O(n)->simple for loops.

O(log n)-> eg: binary search
O(n2)->Nested for loops.

O(n log n), O(2^n),O(n!)

OOPS:

___INSERT_FIGURE

    evince -p 6 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &

___INSERT_FIGURE

    evince -p 7 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &

___INSERT_FIGURE

Linear data structure:

Stack: Use stack when you want
quick access to the most recent
item and keep the items in order.

Queue: FIFO, Process items in order
which they were added.

___INSERT_FIGURE

    evince -p 8 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &


___INSERT_FIGURE


___INSERT_FIGURE


Non linear DS: BST,Graphs

    evince -p 9 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &

Sorting:

Insertion

___INSERT_FIGURE

Merge

___INSERT_FIGURE

Quick == Low memory usage.

    evince -p 10 ~/Empire/Doks/Comp/lang/py/funct/pyrecursion.pdf &


